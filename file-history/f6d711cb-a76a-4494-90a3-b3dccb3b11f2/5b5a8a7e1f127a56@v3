#!/usr/bin/env python3
"""
Binance Public Data Collector

Ultra-fast historical data collection using Binance's official public data repository.
10-100x faster than API calls, with complete historical coverage.

Data source: https://data.binance.vision/data/spot/monthly/klines/
"""

import argparse
import csv
import hashlib
import json
import logging
import shutil
import tempfile
import urllib.request
import warnings
import zipfile
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import pandas as pd

from ..gap_filling.universal_gap_filler import UniversalGapFiller


class BinancePublicDataCollector:
    """Ultra-fast cryptocurrency spot data collection from Binance's public data repository.

    This collector provides 10-100x faster data collection compared to API calls by
    downloading pre-generated monthly ZIP files from Binance's official public data repository.
    Supports complete historical coverage with full 11-column microstructure format including
    order flow metrics.

    Features:
        - Ultra-fast bulk data collection from monthly ZIP archives
        - Complete historical coverage from 2017 onwards
        - Full 11-column microstructure format with order flow data
        - Automatic gap detection and filling capabilities
        - Built-in data validation and integrity checks
        - Support for all major timeframes (1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h)
        - DataFrame-first Python API with seamless pandas integration

    Data Format:
        The collector outputs CSV files with 11 columns providing complete market microstructure:
        - OHLCV: Open, High, Low, Close, Volume
        - Timestamps: Open Time, Close Time
        - Order Flow: Quote Asset Volume, Number of Trades
        - Taker Metrics: Taker Buy Base Volume, Taker Buy Quote Volume

    Examples:
        For simple data collection, consider using the function-based API:

        >>> import gapless_crypto_data as gcd
        >>> df = gcd.fetch_data("BTCUSDT", "1h", start="2024-01-01", end="2024-12-31")

        Advanced usage with this class for complex workflows:

        >>> collector = BinancePublicDataCollector()
        >>> result = collector.collect_timeframe_data("1h")
        >>> df = result["dataframe"]
        >>> print(f"Collected {len(df)} bars of {collector.symbol} data")
        Collected 26280 bars of SOLUSDT data

        Custom configuration and multiple timeframes:

        >>> collector = BinancePublicDataCollector(
        ...     symbol="BTCUSDT",
        ...     start_date="2023-01-01",
        ...     end_date="2023-12-31",
        ...     output_dir="./crypto_data"
        ... )
        >>> results = collector.collect_multiple_timeframes(["1h", "4h"])
        >>> for timeframe, result in results.items():
        ...     print(f"{timeframe}: {len(result['dataframe'])} bars")
        1h: 8760 bars
        4h: 2190 bars

    Note:
        This collector only supports USDT spot pairs (BTCUSDT, ETHUSDT, SOLUSDT, etc.).
        It does not support futures, perpetuals, or non-USDT pairs.
    """

    def _validate_symbol(self, symbol: str) -> str:
        """
        Validate and sanitize symbol input for security.

        This method prevents path traversal attacks and ensures symbol format integrity
        by rejecting invalid characters and malformed inputs.

        Args:
            symbol: Trading pair symbol to validate (e.g., "BTCUSDT", "SOLUSDT")

        Returns:
            Validated and normalized symbol string (uppercase, stripped)

        Raises:
            ValueError: If symbol is None, empty, or contains invalid characters

        Security:
            - Prevents path traversal attacks (CWE-22)
            - Blocks directory navigation characters (/, \\, ., ..)
            - Enforces alphanumeric-only input
            - Protects file operations using symbol in paths

        Examples:
            >>> collector._validate_symbol("btcusdt")
            'BTCUSDT'

            >>> collector._validate_symbol("BTC/../etc/passwd")
            ValueError: Symbol contains invalid characters...

            >>> collector._validate_symbol("")
            ValueError: Symbol cannot be empty

            >>> collector._validate_symbol(None)
            ValueError: Symbol cannot be None
        """
        # SEC-03: None value validation
        if symbol is None:
            raise ValueError("Symbol cannot be None")

        # SEC-02: Empty string validation
        if not symbol or not symbol.strip():
            raise ValueError("Symbol cannot be empty")

        # SEC-01: Path traversal prevention
        import re

        if re.search(r"[./\\]", symbol):
            raise ValueError(
                f"Symbol contains invalid characters: {symbol}\n"
                f"Symbol must be alphanumeric (e.g., BTCUSDT, SOLUSDT)"
            )

        # Normalize to uppercase and strip whitespace
        symbol = symbol.upper().strip()

        # Whitelist validation - only alphanumeric characters
        if not re.match(r"^[A-Z0-9]+$", symbol):
            raise ValueError(
                f"Symbol must be alphanumeric: {symbol}\nValid examples: BTCUSDT, ETHUSDT, SOLUSDT"
            )

        return symbol

    def __init__(
        self,
        symbol: str = "SOLUSDT",
        start_date: str = "2020-08-15",
        end_date: str = "2025-03-20",
        output_dir: Optional[Union[str, Path]] = None,
        output_format: str = "csv",
    ) -> None:
        """Initialize the Binance Public Data Collector.

        Args:
            symbol (str, optional): Trading pair symbol in USDT format.
                Must be alphanumeric (A-Z, 0-9) only. Path characters (/, \\, .)
                and special characters are rejected for security.
                Symbol is normalized to uppercase.
                Defaults to "SOLUSDT".
            start_date (str, optional): Start date in YYYY-MM-DD format.
                Data collection begins from this date (inclusive).
                Must be on or before end_date.
                Defaults to "2020-08-15".
            end_date (str, optional): End date in YYYY-MM-DD format.
                Data collection ends on this date (inclusive, 23:59:59).
                Must be on or after start_date.
                Defaults to "2025-03-20".
            output_dir (str or Path, optional): Directory to save files.
                If None, saves to package's sample_data directory.
                Defaults to None.
            output_format (str, optional): Output format ("csv" or "parquet").
                CSV provides universal compatibility, Parquet offers 5-10x compression.
                Defaults to "csv".

        Raises:
            ValueError: If symbol is None, empty, or contains invalid characters
                (path traversal, special characters, non-alphanumeric).
            ValueError: If date format is incorrect (not YYYY-MM-DD).
            ValueError: If end_date is before start_date.
            ValueError: If output_format is not 'csv' or 'parquet'.
            FileNotFoundError: If output_dir path is invalid.

        Security:
            Input validation prevents path traversal attacks (CWE-22) by:
            - Rejecting symbols with path characters (/, \\, ., ..)
            - Enforcing alphanumeric-only symbols
            - Validating date range logic
            - Normalizing inputs to uppercase

        Examples:
            >>> # Default configuration (SOLUSDT, 4+ years of data)
            >>> collector = BinancePublicDataCollector()

            >>> # Custom symbol and shorter timeframe
            >>> collector = BinancePublicDataCollector(
            ...     symbol="BTCUSDT",
            ...     start_date="2024-01-01",
            ...     end_date="2024-12-31"
            ... )

            >>> # Custom output directory with Parquet format
            >>> collector = BinancePublicDataCollector(
            ...     symbol="ETHUSDT",
            ...     output_dir="/path/to/crypto/data",
            ...     output_format="parquet"
            ... )
        """
        # Validate and assign symbol (SEC-01, SEC-02, SEC-03)
        self.symbol = self._validate_symbol(symbol)

        # Parse and assign dates with validation
        try:
            self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
            # Make end_date inclusive of the full day (23:59:59)
            self.end_date = datetime.strptime(end_date, "%Y-%m-%d").replace(
                hour=23, minute=59, second=59
            )
        except ValueError as e:
            raise ValueError(f"Invalid date format. Use YYYY-MM-DD format. Error: {e}") from e

        # SEC-04: Validate date range logic
        if self.end_date < self.start_date:
            raise ValueError(
                f"Invalid date range: end_date ({self.end_date.strftime('%Y-%m-%d')}) "
                f"is before start_date ({self.start_date.strftime('%Y-%m-%d')})"
            )
        self.base_url = "https://data.binance.vision/data/spot/monthly/klines"

        # Validate and store output format
        if output_format not in ["csv", "parquet"]:
            raise ValueError(f"output_format must be 'csv' or 'parquet', got '{output_format}'")
        self.output_format = output_format

        # Configure output directory - use provided path or default to sample_data
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = Path(__file__).parent.parent / "sample_data"

        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize Rich console for progress indicators
        # Simple logging instead of Rich console

        # Available timeframes on Binance public data
        self.available_timeframes = [
            "1s",
            "1m",
            "3m",
            "5m",
            "15m",
            "30m",
            "1h",
            "2h",
            "4h",
            "6h",
            "8h",
            "12h",
            "1d",
            "3d",
            "1w",
            "1mo",
        ]

        # Popular symbols with known availability (for validation)
        self.known_symbols = {
            "BTCUSDT": "2017-08-17",
            "ETHUSDT": "2017-08-17",
            "SOLUSDT": "2020-08-11",
            "ADAUSDT": "2018-04-17",
            "DOTUSDT": "2020-08-19",
            "LINKUSDT": "2019-01-16",
        }

        # Validate date range and symbol
        self._validate_parameters()

        print("Binance Public Data Collector")
        print(f"Symbol: {self.symbol}")
        print(
            f"Date Range: {self.start_date.strftime('%Y-%m-%d')} to {self.end_date.strftime('%Y-%m-%d')}"
        )
        print(f"Data Source: {self.base_url}")

    def _validate_parameters(self):
        """Validate date range and symbol parameters."""
        today = datetime.now().date()
        yesterday = today - timedelta(days=1)

        # Check for future dates
        if self.end_date.date() > yesterday:
            warnings.warn(
                f"‚ö†Ô∏è  Requested end date {self.end_date.strftime('%Y-%m-%d')} is in the future. "
                f"Binance public data is typically available up to {yesterday}. "
                f"Recent data may not be available and requests may fail with 404 errors.",
                UserWarning,
                stacklevel=2,
            )

        # Check symbol availability
        if self.symbol in self.known_symbols:
            symbol_start = datetime.strptime(self.known_symbols[self.symbol], "%Y-%m-%d").date()
            if self.start_date.date() < symbol_start:
                warnings.warn(
                    f"‚ö†Ô∏è  Requested start date {self.start_date.strftime('%Y-%m-%d')} is before "
                    f"{self.symbol} listing date ({symbol_start}). "
                    f"Data before {symbol_start} is not available.",
                    UserWarning,
                    stacklevel=2,
                )
        else:
            # Unknown symbol - provide general guidance
            logging.info(
                f"‚ÑπÔ∏è  Symbol {self.symbol} availability not verified. "
                f"Known symbols: {list(self.known_symbols.keys())}. "
                f"If requests fail with 404 errors, check symbol availability on Binance."
            )

    def generate_monthly_urls(self, trading_timeframe: str) -> List[Tuple[str, str, str]]:
        """Generate list of monthly ZIP file URLs to download."""
        monthly_zip_urls = []
        current_month_date = self.start_date.replace(day=1)  # Start of month

        while current_month_date <= self.end_date:
            year_month_string = current_month_date.strftime("%Y-%m")
            zip_filename = f"{self.symbol}-{trading_timeframe}-{year_month_string}.zip"
            binance_zip_url = f"{self.base_url}/{self.symbol}/{trading_timeframe}/{zip_filename}"
            monthly_zip_urls.append((binance_zip_url, year_month_string, zip_filename))

            # Move to next month
            if current_month_date.month == 12:
                current_month_date = current_month_date.replace(
                    year=current_month_date.year + 1, month=1
                )
            else:
                current_month_date = current_month_date.replace(month=current_month_date.month + 1)

        return monthly_zip_urls

    def download_and_extract_month(self, binance_zip_url, zip_filename):
        """Download and extract a single monthly ZIP file."""
        print(f"  Downloading {zip_filename}...")

        try:
            with tempfile.NamedTemporaryFile() as temporary_zip_file:
                # Download ZIP file
                with urllib.request.urlopen(binance_zip_url, timeout=60) as http_response:
                    if http_response.status == 200:
                        shutil.copyfileobj(http_response, temporary_zip_file)
                        temporary_zip_file.flush()
                    else:
                        print(f"    ‚ö†Ô∏è  HTTP {http_response.status} - {zip_filename} not available")
                        return []

                # Extract CSV data
                with zipfile.ZipFile(temporary_zip_file.name, "r") as zip_file_handle:
                    expected_csv_filename = zip_filename.replace(".zip", ".csv")
                    if expected_csv_filename in zip_file_handle.namelist():
                        with zip_file_handle.open(expected_csv_filename) as extracted_csv_file:
                            csv_file_content = extracted_csv_file.read().decode("utf-8")
                            return list(csv.reader(csv_file_content.strip().split("\n")))
                    else:
                        print(f"    ‚ö†Ô∏è  CSV file not found in {zip_filename}")
                        return []

        except Exception as download_exception:
            print(f"    ‚ùå Error downloading {zip_filename}: {download_exception}")

            # Implement automatic fallback to daily files when monthly fails
            print(f"    üîÑ Attempting daily file fallback for {zip_filename}")
            return self._fallback_to_daily_files(zip_filename)

    def _fallback_to_daily_files(self, failed_monthly_filename):
        """
        Fallback to daily file downloads when monthly file is not available.

        Automatically downloads individual daily files for the failed month
        and combines them into a single dataset for seamless operation.

        Args:
            failed_monthly_filename: The monthly filename that failed (e.g., "BTCUSDT-1d-2025-09.zip")

        Returns:
            List of combined daily data, or empty list if all daily files also fail
        """
        # Extract symbol, timeframe, and year-month from failed filename
        # Format: "BTCUSDT-1d-2025-09.zip"
        parts = failed_monthly_filename.replace(".zip", "").split("-")
        if len(parts) < 4:
            print(f"    ‚ùå Cannot parse monthly filename: {failed_monthly_filename}")
            return []

        symbol = parts[0]
        timeframe = parts[1]
        year = parts[2]
        month = parts[3]

        print(f"    üìÖ Fallback: Downloading daily files for {symbol} {timeframe} {year}-{month}")

        # Generate daily URLs for the entire month
        daily_urls = self._generate_daily_urls_for_month(symbol, timeframe, year, month)

        # Download all daily files for this month
        combined_daily_data = []
        successful_daily_downloads = 0

        for daily_url, daily_filename in daily_urls:
            daily_data = self._download_and_extract_daily_file(daily_url, daily_filename)
            if daily_data:
                combined_daily_data.extend(daily_data)
                successful_daily_downloads += 1

        if successful_daily_downloads > 0:
            print(
                f"    ‚úÖ Daily fallback successful: {successful_daily_downloads}/{len(daily_urls)} daily files retrieved"
            )
            return combined_daily_data
        else:
            print(f"    ‚ùå Daily fallback failed: No daily files available for {year}-{month}")
            return []

    def _generate_daily_urls_for_month(self, symbol, timeframe, year, month):
        """Generate daily URLs for all days in a specific month."""
        from calendar import monthrange

        # Get number of days in the month
        year_int = int(year)
        month_int = int(month)
        _, days_in_month = monthrange(year_int, month_int)

        daily_urls = []

        # Use daily data URL pattern: https://data.binance.vision/data/spot/daily/klines/
        daily_base_url = self.base_url.replace("/monthly/", "/daily/")

        for day in range(1, days_in_month + 1):
            date_str = f"{year}-{month_int:02d}-{day:02d}"
            daily_filename = f"{symbol}-{timeframe}-{date_str}.zip"
            daily_url = f"{daily_base_url}/{symbol}/{timeframe}/{daily_filename}"
            daily_urls.append((daily_url, daily_filename))

        return daily_urls

    def _download_and_extract_daily_file(self, daily_url, daily_filename):
        """Download and extract a single daily ZIP file."""
        try:
            with tempfile.NamedTemporaryFile() as temporary_zip_file:
                # Download daily ZIP file
                with urllib.request.urlopen(daily_url, timeout=30) as http_response:
                    if http_response.status == 200:
                        shutil.copyfileobj(http_response, temporary_zip_file)
                        temporary_zip_file.flush()
                    else:
                        # Daily file not available (normal for future dates or weekends)
                        return []

                # Extract CSV data from daily file
                with zipfile.ZipFile(temporary_zip_file.name, "r") as zip_file_handle:
                    expected_csv_filename = daily_filename.replace(".zip", ".csv")
                    if expected_csv_filename in zip_file_handle.namelist():
                        with zip_file_handle.open(expected_csv_filename) as extracted_csv_file:
                            csv_file_content = extracted_csv_file.read().decode("utf-8")
                            return list(csv.reader(csv_file_content.strip().split("\n")))
                    else:
                        return []

        except Exception:
            # Silent failure for daily files - many days may not have data
            return []

    def _detect_header_intelligent(self, raw_csv_data):
        """Intelligent header detection - determine if first row is data or header."""
        if not raw_csv_data:
            return False

        first_csv_row = raw_csv_data[0]
        if len(first_csv_row) < 6:
            return False

        # Header detection heuristics
        try:
            # Test if first field is numeric timestamp
            first_field_value = int(first_csv_row[0])

            # ‚úÖ BOUNDARY FIX: Support both milliseconds (13-digit) AND microseconds (16-digit) formats
            # Valid timestamp ranges:
            # Milliseconds: 1000000000000 (2001) to 9999999999999 (2286)
            # Microseconds: 1000000000000000 (2001) to 9999999999999999 (2286)
            is_valid_millisecond_timestamp = 1000000000000 <= first_field_value <= 9999999999999
            is_valid_microsecond_timestamp = (
                1000000000000000 <= first_field_value <= 9999999999999999
            )

            if is_valid_millisecond_timestamp or is_valid_microsecond_timestamp:
                # Test if other fields are numeric (prices/volumes)
                for ohlcv_field_index in [1, 2, 3, 4, 5]:  # OHLCV fields
                    float(first_csv_row[ohlcv_field_index])
                return False  # All numeric = data row
            else:
                return True  # Invalid timestamp = likely header

        except (ValueError, IndexError):
            # Non-numeric first field = header
            return True

    def process_raw_data(self, raw_csv_data):
        """Convert raw Binance CSV data with comprehensive timestamp format tracking and transition detection."""
        processed_candle_data = []
        self.corruption_log = getattr(self, "corruption_log", [])

        # Initialize comprehensive format tracking
        self.format_stats = {
            "milliseconds": {
                "count": 0,
                "first_seen": None,
                "last_seen": None,
                "sample_values": [],
            },
            "microseconds": {
                "count": 0,
                "first_seen": None,
                "last_seen": None,
                "sample_values": [],
            },
            "unknown": {"count": 0, "errors": []},
        }
        self.format_transitions = []  # Track format changes
        self.current_format = None

        # Intelligent header detection
        csv_has_header = self._detect_header_intelligent(raw_csv_data)
        data_start_row_index = 1 if csv_has_header else 0

        # Store header detection results for metadata
        self._header_detected = csv_has_header
        self._header_content = raw_csv_data[0][:6] if csv_has_header else None
        self._data_start_row = data_start_row_index

        if csv_has_header:
            print(f"    üìã Header detected: {raw_csv_data[0][:6]}")
        else:
            print("    üìä Pure data format detected (no header)")

        format_transition_logged = False

        for csv_row_index, csv_row_data in enumerate(
            raw_csv_data[data_start_row_index:], start=data_start_row_index
        ):
            if len(csv_row_data) >= 6:  # Binance format has 12 columns but we need first 6
                try:
                    # Binance format: [timestamp, open, high, low, close, volume, close_time, quote_volume, count, taker_buy_volume, taker_buy_quote_volume, ignore]
                    raw_timestamp_value = int(csv_row_data[0])

                    # Comprehensive format detection with transition tracking
                    (
                        detected_timestamp_format,
                        converted_timestamp_seconds,
                        format_validation_result,
                    ) = self._analyze_timestamp_format(raw_timestamp_value, csv_row_index)

                    # Track format transitions
                    if self.current_format is None:
                        self.current_format = detected_timestamp_format
                        print(f"    üéØ Initial timestamp format: {detected_timestamp_format}")
                    elif (
                        self.current_format != detected_timestamp_format
                        and detected_timestamp_format != "unknown"
                    ):
                        self.format_transitions.append(
                            {
                                "row_index": csv_row_index,
                                "from_format": self.current_format,
                                "to_format": detected_timestamp_format,
                                "timestamp_value": raw_timestamp_value,
                            }
                        )
                        self.current_format = detected_timestamp_format
                        if not format_transition_logged:
                            print(
                                f"    üîÑ Format transition detected: {self.format_transitions[-1]['from_format']} ‚Üí {detected_timestamp_format}"
                            )
                            format_transition_logged = True

                    # Update format statistics
                    self.format_stats[detected_timestamp_format]["count"] += 1
                    if self.format_stats[detected_timestamp_format]["first_seen"] is None:
                        self.format_stats[detected_timestamp_format]["first_seen"] = csv_row_index
                    self.format_stats[detected_timestamp_format]["last_seen"] = csv_row_index

                    # Store sample values (first 3 per format)
                    if len(self.format_stats[detected_timestamp_format]["sample_values"]) < 3:
                        self.format_stats[detected_timestamp_format]["sample_values"].append(
                            raw_timestamp_value
                        )

                    # Skip if validation failed
                    if not format_validation_result["valid"]:
                        self.corruption_log.append(format_validation_result["error_details"])
                        continue

                    # ‚úÖ CRITICAL FIX: Use UTC to match Binance's native timezone
                    # Eliminates artificial DST gaps caused by local timezone conversion
                    utc_datetime = datetime.fromtimestamp(converted_timestamp_seconds, timezone.utc)

                    # ‚úÖ BOUNDARY FIX: Don't filter per-monthly-file to preserve month boundaries
                    # Enhanced processing: capture all 11 essential Binance columns for complete microstructure analysis
                    processed_candle_row = [
                        utc_datetime.strftime("%Y-%m-%d %H:%M:%S"),  # date (from open_time)
                        float(csv_row_data[1]),  # open
                        float(csv_row_data[2]),  # high
                        float(csv_row_data[3]),  # low
                        float(csv_row_data[4]),  # close
                        float(csv_row_data[5]),  # volume (base asset volume)
                        # Additional microstructure columns for professional analysis
                        datetime.fromtimestamp(
                            int(csv_row_data[6])
                            / (1000000 if len(str(int(csv_row_data[6]))) >= 16 else 1000),
                            timezone.utc,
                        ).strftime("%Y-%m-%d %H:%M:%S"),  # close_time
                        float(csv_row_data[7]),  # quote_asset_volume
                        int(csv_row_data[8]),  # number_of_trades
                        float(csv_row_data[9]),  # taker_buy_base_asset_volume
                        float(csv_row_data[10]),  # taker_buy_quote_asset_volume
                    ]
                    processed_candle_data.append(processed_candle_row)

                except (ValueError, OSError, OverflowError) as parsing_exception:
                    self.format_stats["unknown"]["count"] += 1
                    error_record = {
                        "row_index": csv_row_index,
                        "error_type": "timestamp_parse_error",
                        "error_message": str(parsing_exception),
                        "raw_row": csv_row_data[:10] if len(csv_row_data) > 10 else csv_row_data,
                    }
                    self.corruption_log.append(error_record)
                    self.format_stats["unknown"]["errors"].append(error_record)
                    continue
            else:
                # Record insufficient columns
                self.corruption_log.append(
                    {
                        "row_index": csv_row_index,
                        "error_type": "insufficient_columns",
                        "column_count": len(csv_row_data),
                        "raw_row": csv_row_data,
                    }
                )

        # Report comprehensive format analysis
        self._report_format_analysis()

        return processed_candle_data

    def _analyze_timestamp_format(self, raw_timestamp_value, csv_row_index):
        """Comprehensive timestamp format analysis with validation."""
        timestamp_digit_count = len(str(raw_timestamp_value))

        # Enhanced format detection logic
        if timestamp_digit_count >= 16:  # Microseconds (16+ digits) - 2025+ format
            detected_format_type = "microseconds"
            converted_seconds = raw_timestamp_value / 1000000
            timestamp_min_bound = 1262304000000000  # 2010-01-01 00:00:00 (microseconds)
            timestamp_max_bound = 1893456000000000  # 2030-01-01 00:00:00 (microseconds)

        elif timestamp_digit_count >= 10:  # Milliseconds (10-15 digits) - Legacy format
            detected_format_type = "milliseconds"
            converted_seconds = raw_timestamp_value / 1000
            timestamp_min_bound = 1262304000000  # 2010-01-01 00:00:00 (milliseconds)
            timestamp_max_bound = 1893456000000  # 2030-01-01 00:00:00 (milliseconds)

        else:  # Unknown format (less than 10 digits)
            detected_format_type = "unknown"
            converted_seconds = None
            timestamp_min_bound = timestamp_max_bound = None

        # Enhanced validation with detailed error reporting
        if detected_format_type == "unknown":
            timestamp_validation_result = {
                "valid": False,
                "error_details": {
                    "row_index": csv_row_index,
                    "error_type": "unknown_timestamp_format",
                    "timestamp_value": raw_timestamp_value,
                    "digit_count": timestamp_digit_count,
                    "expected_formats": "milliseconds (10-15 digits) or microseconds (16+ digits)",
                    "raw_row": f"Timestamp too short: {timestamp_digit_count} digits",
                },
            }
        elif raw_timestamp_value < timestamp_min_bound or raw_timestamp_value > timestamp_max_bound:
            timestamp_validation_result = {
                "valid": False,
                "error_details": {
                    "row_index": csv_row_index,
                    "error_type": "invalid_timestamp_range",
                    "timestamp_value": raw_timestamp_value,
                    "timestamp_format": detected_format_type,
                    "digit_count": timestamp_digit_count,
                    "valid_range": f"{timestamp_min_bound} to {timestamp_max_bound}",
                    "parsed_date": "out_of_range",
                    "raw_row": f"Out of valid {detected_format_type} range (2010-2030)",
                },
            }
        else:
            timestamp_validation_result = {"valid": True}

        return detected_format_type, converted_seconds, timestamp_validation_result

    def _report_format_analysis(self):
        """Report comprehensive format analysis with transition detection."""
        total_rows = sum(stats["count"] for stats in self.format_stats.values())

        print("    üìà COMPREHENSIVE FORMAT ANALYSIS:")

        for format_type, stats in self.format_stats.items():
            if stats["count"] > 0:
                percentage = (stats["count"] / total_rows) * 100 if total_rows > 0 else 0
                print(f"      {format_type.upper()}: {stats['count']:,} rows ({percentage:.1f}%)")

                if format_type != "unknown" and stats["sample_values"]:
                    first_sample = stats["sample_values"][0]
                    print(
                        f"        Sample: {first_sample} (rows {stats['first_seen']}-{stats['last_seen']})"
                    )

        # Report format transitions
        if len(self.format_transitions) > 0:
            print(f"    üîÑ FORMAT TRANSITIONS DETECTED: {len(self.format_transitions)}")
            for i, transition in enumerate(self.format_transitions[:3]):  # Show first 3
                print(
                    f"      #{i + 1}: Row {transition['row_index']} - {transition['from_format']} ‚Üí {transition['to_format']}"
                )
                print(f"          Timestamp: {transition['timestamp_value']}")
            if len(self.format_transitions) > 3:
                print(f"      ... and {len(self.format_transitions) - 3} more transitions")
        else:
            print(
                f"    ‚úÖ SINGLE FORMAT: No transitions detected - consistent {self.current_format}"
            )

        # Store format analysis results for metadata
        self._format_analysis_summary = {
            "total_rows_analyzed": total_rows,
            "formats_detected": {
                fmt: stats["count"]
                for fmt, stats in self.format_stats.items()
                if stats["count"] > 0
            },
            "transitions_detected": len(self.format_transitions),
            "transition_details": self.format_transitions,
            "primary_format": self.current_format,
            "format_consistency": len(self.format_transitions) == 0,
        }

    def collect_timeframe_data(self, trading_timeframe: str) -> Dict[str, Any]:
        """Collect complete historical data for a single timeframe with full 11-column microstructure format.

        Downloads and processes monthly ZIP files from Binance's public data repository
        for the specified timeframe. Automatically handles data processing, validation,
        and saves to CSV while returning a DataFrame for immediate use.

        Args:
            trading_timeframe (str): Timeframe for data collection.
                Must be one of: "1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h".

        Returns:
            dict: Collection results containing:
                - dataframe (pd.DataFrame): Complete OHLCV data with 11 columns:
                    * date: Timestamp (open time)
                    * open, high, low, close: Price data
                    * volume: Base asset volume
                    * close_time: Timestamp (close time)
                    * quote_asset_volume: Quote asset volume
                    * number_of_trades: Trade count
                    * taker_buy_base_asset_volume: Taker buy base volume
                    * taker_buy_quote_asset_volume: Taker buy quote volume
                - filepath (Path): Path to saved CSV file
                - stats (dict): Collection statistics including duration and bar count

        Raises:
            ValueError: If trading_timeframe is not supported.
            ConnectionError: If download from Binance repository fails.
            FileNotFoundError: If output directory is invalid.

        Examples:
            >>> collector = BinancePublicDataCollector(symbol="BTCUSDT")
            >>> result = collector.collect_timeframe_data("1h")
            >>> df = result["dataframe"]
            >>> print(f"Collected {len(df)} hourly bars")
            >>> print(f"Date range: {df['date'].min()} to {df['date'].max()}")
            Collected 26280 hourly bars
            Date range: 2020-08-15 01:00:00 to 2025-03-20 23:00:00

            >>> # Access microstructure data
            >>> print(f"Total trades: {df['number_of_trades'].sum():,}")
            >>> print(f"Average taker buy ratio: {df['taker_buy_base_asset_volume'].sum() / df['volume'].sum():.2%}")
            Total trades: 15,234,567
            Average taker buy ratio: 51.23%

        Note:
            This method processes data chronologically and may take several minutes
            for large date ranges due to monthly ZIP file downloads. Progress is
            displayed during collection.
        """
        print(f"\n{'=' * 60}")
        print(f"COLLECTING {trading_timeframe.upper()} DATA FROM BINANCE PUBLIC REPOSITORY")
        print(f"{'=' * 60}")

        if trading_timeframe not in self.available_timeframes:
            print(f"‚ùå Timeframe '{trading_timeframe}' not available")
            print(f"üìä Available timeframes: {', '.join(self.available_timeframes)}")
            print("üí° Use 'gapless-crypto-data --list-timeframes' for detailed descriptions")
            return None

        # Generate monthly URLs
        monthly_zip_urls = self.generate_monthly_urls(trading_timeframe)
        print(f"Monthly files to download: {len(monthly_zip_urls)}")

        # Collect data from all months
        combined_candle_data = []
        successful_download_count = 0

        for binance_zip_url, year_month_string, zip_filename in monthly_zip_urls:
            raw_monthly_csv_data = self.download_and_extract_month(binance_zip_url, zip_filename)
            if raw_monthly_csv_data:
                processed_monthly_data = self.process_raw_data(raw_monthly_csv_data)
                combined_candle_data.extend(processed_monthly_data)
                successful_download_count += 1
                print(f"    ‚úÖ {len(processed_monthly_data):,} bars from {year_month_string}")
            else:
                print(f"    ‚ö†Ô∏è  No data from {year_month_string}")

        print("\nCollection Summary:")
        print(f"  Successful downloads: {successful_download_count}/{len(monthly_zip_urls)}")
        print(f"  Total bars collected: {len(combined_candle_data):,}")

        if combined_candle_data:
            # Sort by timestamp to ensure chronological order
            combined_candle_data.sort(key=lambda candle_row: candle_row[0])
            print(
                f"  Pre-filtering range: {combined_candle_data[0][0]} to {combined_candle_data[-1][0]}"
            )

            # ‚úÖ BOUNDARY FIX: Apply final date range filtering after combining all monthly data
            # This preserves month boundaries while respecting the requested date range
            date_filtered_data = []
            for candle_row in combined_candle_data:
                candle_datetime = datetime.strptime(candle_row[0], "%Y-%m-%d %H:%M:%S")
                if self.start_date <= candle_datetime <= self.end_date:
                    date_filtered_data.append(candle_row)

            print(f"  Post-filtering: {len(date_filtered_data):,} bars in requested range")
            if date_filtered_data:
                print(f"  Final range: {date_filtered_data[0][0]} to {date_filtered_data[-1][0]}")

            # Save to CSV and return DataFrame for seamless Python integration
            if date_filtered_data:
                # Calculate collection stats for metadata
                collection_stats = {
                    "method": "direct_download",
                    "duration": 0.0,  # Minimal for single timeframe
                    "bars_per_second": 0,
                    "total_bars": len(date_filtered_data),
                }

                # Save to CSV file (addresses the output_dir bug)
                filepath = self.save_data(trading_timeframe, date_filtered_data, collection_stats)

                # Convert to DataFrame for Python API users
                columns = [
                    "date",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "close_time",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                ]
                df = pd.DataFrame(date_filtered_data, columns=columns)

                # Convert numeric columns
                numeric_cols = [
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                ]
                for col in numeric_cols:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

                # Convert date columns to datetime
                df["date"] = pd.to_datetime(df["date"])
                df["close_time"] = pd.to_datetime(df["close_time"])

                return {"dataframe": df, "filepath": filepath, "stats": collection_stats}

            return {"dataframe": pd.DataFrame(), "filepath": None, "stats": {}}

        # Save to CSV and return DataFrame for unfiltered data
        if combined_candle_data:
            # Calculate collection stats for metadata
            collection_stats = {
                "method": "direct_download",
                "duration": 0.0,  # Minimal for single timeframe
                "bars_per_second": 0,
                "total_bars": len(combined_candle_data),
            }

            # Save to CSV file (addresses the output_dir bug)
            filepath = self.save_data(trading_timeframe, combined_candle_data, collection_stats)

            # Convert to DataFrame for Python API users
            columns = [
                "date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "close_time",
                "quote_asset_volume",
                "number_of_trades",
                "taker_buy_base_asset_volume",
                "taker_buy_quote_asset_volume",
            ]
            df = pd.DataFrame(combined_candle_data, columns=columns)

            # Convert numeric columns
            numeric_cols = [
                "open",
                "high",
                "low",
                "close",
                "volume",
                "quote_asset_volume",
                "number_of_trades",
                "taker_buy_base_asset_volume",
                "taker_buy_quote_asset_volume",
            ]
            for col in numeric_cols:
                df[col] = pd.to_numeric(df[col], errors="coerce")

            # Convert date columns to datetime
            df["date"] = pd.to_datetime(df["date"])
            df["close_time"] = pd.to_datetime(df["close_time"])

            return {"dataframe": df, "filepath": filepath, "stats": collection_stats}

        return {"dataframe": pd.DataFrame(), "filepath": None, "stats": {}}

    def generate_metadata(
        self, trading_timeframe, candle_data, collection_performance_stats, gap_analysis_result=None
    ):
        """Generate comprehensive metadata for 11-column microstructure format."""
        if not candle_data:
            return {}

        # Calculate statistics
        price_values = []
        volume_values = []
        for candle_row in candle_data:
            price_values.extend([candle_row[2], candle_row[3]])  # high, low
            volume_values.append(candle_row[5])

        return {
            "version": "v2.10.0",
            "generator": "BinancePublicDataCollector",
            "generation_timestamp": datetime.now(timezone.utc).isoformat() + "Z",
            "data_source": "Binance Public Data Repository",
            "data_source_url": self.base_url,
            "market_type": "spot",
            "symbol": self.symbol,
            "timeframe": trading_timeframe,
            "collection_method": "direct_download",
            "target_period": {
                "start": self.start_date.isoformat(),
                "end": self.end_date.isoformat(),
                "total_days": (self.end_date - self.start_date).days,
            },
            "actual_bars": len(candle_data),
            "date_range": {
                "start": candle_data[0][0] if candle_data else None,
                "end": candle_data[-1][0] if candle_data else None,
            },
            "statistics": {
                "price_min": min(price_values) if price_values else 0,
                "price_max": max(price_values) if price_values else 0,
                "volume_total": sum(volume_values) if volume_values else 0,
                "volume_mean": sum(volume_values) / len(volume_values) if volume_values else 0,
            },
            "collection_performance": collection_performance_stats,
            "data_integrity": {
                "chronological_order": True,
                "data_hash": self._calculate_data_hash(candle_data),
                "corruption_detected": len(getattr(self, "corruption_log", [])) > 0,
                "corrupted_rows_count": len(getattr(self, "corruption_log", [])),
                "corruption_details": getattr(self, "corruption_log", []),
                "header_detection": {
                    "header_found": getattr(self, "_header_detected", False),
                    "header_content": getattr(self, "_header_content", None),
                    "data_start_row": getattr(self, "_data_start_row", 0),
                },
            },
            "timestamp_format_analysis": getattr(
                self,
                "_format_analysis_summary",
                {
                    "total_rows_analyzed": 0,
                    "formats_detected": {},
                    "transitions_detected": 0,
                    "transition_details": [],
                    "primary_format": "unknown",
                    "format_consistency": True,
                    "analysis_note": "Format analysis not available - may be legacy collection",
                },
            ),
            "enhanced_microstructure_format": {
                "format_version": "v2.10.0",
                "total_columns": len(candle_data[0]) if candle_data else 11,
                "enhanced_features": [
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                    "close_time",
                ],
                "analysis_capabilities": [
                    "order_flow_analysis",
                    "liquidity_metrics",
                    "market_microstructure",
                    "trade_weighted_prices",
                    "institutional_data_patterns",
                ],
                "professional_features": True,
                "api_format_compatibility": True,
            },
            "gap_analysis": gap_analysis_result
            or {
                "analysis_performed": False,
                "total_gaps_detected": 0,
                "gaps_filled": 0,
                "gaps_remaining": 0,
                "gap_details": [],
                "gap_filling_method": "authentic_binance_api",
                "data_completeness_score": 1.0,
                "note": "Gap analysis can be performed using UniversalGapFiller.detect_all_gaps()",
            },
            "compliance": {
                "zero_magic_numbers": True,
                "temporal_integrity": True,
                "authentic_spot_data_only": True,
                "official_binance_source": True,
                "binance_format_transition_aware": True,
                "supports_milliseconds_microseconds": True,
                "full_binance_microstructure_format": True,
                "professional_trading_ready": True,
            },
        }

    def _perform_gap_analysis(self, data, timeframe):
        """Perform gap analysis on collected data and return detailed results."""
        if not data or len(data) < 2:
            return {
                "analysis_performed": True,
                "total_gaps_detected": 0,
                "gaps_filled": 0,
                "gaps_remaining": 0,
                "gap_details": [],
                "gap_filling_method": "authentic_binance_api",
                "data_completeness_score": 1.0,
                "note": "Insufficient data for gap analysis (< 2 rows)",
            }

        # Calculate expected interval in minutes
        timeframe_minutes = {
            "1m": 1,
            "3m": 3,
            "5m": 5,
            "15m": 15,
            "30m": 30,
            "1h": 60,
            "2h": 120,
            "4h": 240,
            "1d": 1440,
        }

        interval_minutes = timeframe_minutes.get(timeframe, 60)
        expected_gap_minutes = interval_minutes

        # Analyze timestamp gaps
        gaps_detected = []
        total_bars_expected = 0

        for i in range(1, len(data)):
            current_time = datetime.strptime(data[i][0], "%Y-%m-%d %H:%M:%S")
            previous_time = datetime.strptime(data[i - 1][0], "%Y-%m-%d %H:%M:%S")

            actual_gap_minutes = (current_time - previous_time).total_seconds() / 60

            if actual_gap_minutes > expected_gap_minutes * 1.5:  # Allow 50% tolerance
                missing_bars = int(actual_gap_minutes / expected_gap_minutes) - 1
                if missing_bars > 0:
                    gaps_detected.append(
                        {
                            "gap_start": data[i - 1][0],
                            "gap_end": data[i][0],
                            "missing_bars": missing_bars,
                            "duration_minutes": actual_gap_minutes - expected_gap_minutes,
                        }
                    )
                    total_bars_expected += missing_bars

        # Calculate completeness score
        total_bars_collected = len(data)
        total_bars_should_exist = total_bars_collected + total_bars_expected
        completeness_score = (
            total_bars_collected / total_bars_should_exist if total_bars_should_exist > 0 else 1.0
        )

        return {
            "analysis_performed": True,
            "total_gaps_detected": len(gaps_detected),
            "gaps_filled": 0,  # Will be updated during gap filling process
            "gaps_remaining": len(gaps_detected),
            "gap_details": gaps_detected[:10],  # Limit to first 10 gaps for metadata size
            "total_missing_bars": total_bars_expected,
            "gap_filling_method": "authentic_binance_api",
            "data_completeness_score": round(completeness_score, 4),
            "analysis_timestamp": datetime.now(timezone.utc).isoformat() + "Z",
            "analysis_parameters": {
                "timeframe": timeframe,
                "expected_interval_minutes": expected_gap_minutes,
                "tolerance_factor": 1.5,
            },
        }

    def _calculate_data_hash(self, data):
        """Calculate hash of data for integrity verification."""
        data_string = "\n".join(",".join(map(str, row)) for row in data)
        return hashlib.sha256(data_string.encode()).hexdigest()

    def save_data(self, timeframe: str, data: List[List], collection_stats: Dict[str, Any]) -> Path:
        """Save data to file with format determined by output_format (CSV or Parquet)."""
        if not data:
            print(f"‚ùå No data to save for {timeframe}")
            return None

        # Generate filename with appropriate extension
        start_date_str = self.start_date.strftime("%Y%m%d")
        end_date_str = datetime.strptime(data[-1][0], "%Y-%m-%d %H:%M:%S").strftime("%Y%m%d")
        version = "v2.10.0"  # Updated version for Parquet support
        file_extension = self.output_format
        filename = f"binance_spot_{self.symbol}-{timeframe}_{start_date_str}-{end_date_str}_{version}.{file_extension}"
        filepath = self.output_dir / filename

        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Perform gap analysis on collected data
        gap_analysis = self._perform_gap_analysis(data, timeframe)

        # Generate metadata with gap analysis results
        metadata = self.generate_metadata(timeframe, data, collection_stats, gap_analysis)

        # Convert data to DataFrame for both formats
        df = pd.DataFrame(
            data,
            columns=[
                "date",
                "open",
                "high",
                "low",
                "close",
                "volume",
                "close_time",
                "quote_asset_volume",
                "number_of_trades",
                "taker_buy_base_asset_volume",
                "taker_buy_quote_asset_volume",
            ],
        )

        # Convert date column to datetime
        df["date"] = pd.to_datetime(df["date"])

        if self.output_format == "parquet":
            # Save as Parquet with metadata
            df.to_parquet(filepath, engine="pyarrow", compression="snappy", index=False)
            print(f"üìä Saved {len(df):,} bars to {filepath.name} (Parquet format)")
        else:
            # Save as CSV with metadata headers (existing logic)
            with open(filepath, "w", newline="") as f:
                # Write metadata headers
                f.write(f"# Binance Spot Market Data {metadata['version']}\n")
                f.write(f"# Generated: {metadata['generation_timestamp']}\n")
                f.write(f"# Source: {metadata['data_source']}\n")
                f.write(
                    f"# Market: {metadata['market_type'].upper()} | Symbol: {metadata['symbol']} | Timeframe: {metadata['timeframe']}\n"
                )
                f.write(f"# Coverage: {metadata['actual_bars']:,} bars\n")
                f.write(
                    f"# Period: {metadata['date_range']['start']} to {metadata['date_range']['end']}\n"
                )
                f.write(
                    f"# Collection: {collection_stats['method']} in {collection_stats['duration']:.1f}s\n"
                )
                f.write(f"# Data Hash: {metadata['data_integrity']['data_hash'][:16]}...\n")
                f.write(
                    "# Compliance: Zero-Magic-Numbers, Temporal-Integrity, Official-Binance-Source\n"
                )
                f.write("#\n")

                # Write CSV data
                df.to_csv(f, index=False)
            print(f"üìä Saved {len(df):,} bars to {filepath.name} (CSV format)")

        # Save metadata as JSON
        metadata_filepath = filepath.with_suffix(".metadata.json")
        with open(metadata_filepath, "w") as f:
            json.dump(metadata, f, indent=2)

        file_size_mb = filepath.stat().st_size / (1024 * 1024)
        print(f"\n‚úÖ Created: {filepath.name} ({file_size_mb:.1f} MB)")
        print(f"‚úÖ Metadata: {metadata_filepath.name}")

        return filepath

    def collect_multiple_timeframes(
        self, timeframes: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """Collect data for multiple timeframes with comprehensive progress tracking.

        Efficiently collects historical data across multiple timeframes in sequence,
        providing a complete dataset for multi-timeframe analysis. Each timeframe
        is processed independently with full validation and progress reporting.

        Args:
            timeframes (list, optional): List of timeframes to collect.
                Each must be one of: "1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h".
                If None, defaults to ["1m", "3m", "5m", "15m", "30m", "1h", "2h"].

        Returns:
            dict: Collection results by timeframe, where each key is a timeframe string
                and each value is a dict containing:
                - dataframe (pd.DataFrame): Complete OHLCV data with 11 columns
                - filepath (Path): Path to saved CSV file
                - stats (dict): Collection statistics

        Raises:
            ValueError: If any timeframe in the list is not supported.
            ConnectionError: If download from Binance repository fails.

        Examples:
            Default comprehensive collection:

            >>> collector = BinancePublicDataCollector(symbol="ETHUSDT")
            >>> results = collector.collect_multiple_timeframes()
            >>> for timeframe, result in results.items():
            ...     df = result["dataframe"]
            ...     print(f"{timeframe}: {len(df):,} bars saved to {result['filepath'].name}")
            1m: 1,574,400 bars saved to ETHUSDT_1m_2020-08-15_to_2025-03-20.csv
            3m: 524,800 bars saved to ETHUSDT_3m_2020-08-15_to_2025-03-20.csv

            Custom timeframes for specific analysis:

            >>> collector = BinancePublicDataCollector(symbol="BTCUSDT")
            >>> results = collector.collect_multiple_timeframes(["1h", "4h"])
            >>> hourly_df = results["1h"]["dataframe"]
            >>> four_hour_df = results["4h"]["dataframe"]
            >>> print(f"Hourly data: {len(hourly_df)} bars")
            >>> print(f"4-hour data: {len(four_hour_df)} bars")
            Hourly data: 26,280 bars
            4-hour data: 6,570 bars

            Access collection statistics:

            >>> results = collector.collect_multiple_timeframes(["1h"])
            >>> stats = results["1h"]["stats"]
            >>> print(f"Collection took {stats['duration']:.1f} seconds")
            >>> print(f"Processing rate: {stats['bars_per_second']:,.0f} bars/sec")
            Collection took 45.2 seconds
            Processing rate: 582 bars/sec

        Note:
            Processing time scales with the number of timeframes and date range.
            Progress is displayed in real-time with Rich progress bars.
            All timeframes are collected sequentially to avoid overwhelming
            Binance's public data servers.
        """
        if timeframes is None:
            timeframes = ["1m", "3m", "5m", "15m", "30m", "1h", "2h"]

        print("\nüöÄ BINANCE PUBLIC DATA ULTRA-FAST COLLECTION")
        print(f"Timeframes: {timeframes}")
        print("=" * 80)

        results = {}
        overall_start = datetime.now()

        for i, timeframe in enumerate(timeframes):
            print(f"Processing {timeframe} ({i + 1}/{len(timeframes)})...")

            result = self.collect_timeframe_data(timeframe)

            if result and result.get("filepath"):
                filepath = result["filepath"]
                results[timeframe] = filepath
                file_size_mb = filepath.stat().st_size / (1024 * 1024)
                print(f"‚úÖ {timeframe}: {filepath.name} ({file_size_mb:.1f} MB)")
            else:
                print(f"‚ùå Failed to collect {timeframe} data")

        overall_duration = (datetime.now() - overall_start).total_seconds()

        print("\n" + "=" * 80)
        print("üéâ ULTRA-FAST COLLECTION COMPLETE")
        print(
            f"‚è±Ô∏è  Total time: {overall_duration:.1f} seconds ({overall_duration / 60:.1f} minutes)"
        )
        print(f"üìä Generated {len(results)} files")

        return results

    async def collect_timeframe_data_concurrent(self, trading_timeframe: str) -> Dict[str, Any]:
        """
        Collect data using high-performance concurrent hybrid strategy.

        This method uses the ConcurrentCollectionOrchestrator to achieve 10-15x faster
        data collection through parallel downloads of monthly and daily ZIP files.

        Args:
            trading_timeframe (str): Timeframe for data collection.
                Must be one of: "1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h".

        Returns:
            dict: Collection results containing:
                - dataframe (pd.DataFrame): Complete OHLCV data with 11 columns
                - filepath (Path): Path to saved CSV file
                - stats (dict): Collection statistics including performance metrics
                - collection_method (str): "concurrent_hybrid"

        Examples:
            >>> collector = BinancePublicDataCollector(symbol="BTCUSDT")
            >>> result = await collector.collect_timeframe_data_concurrent("1h")
            >>> df = result["dataframe"]
            >>> print(f"Collected {len(df)} bars in {result['stats']['collection_time']:.1f}s")
            >>> print(f"Performance: {result['stats']['bars_per_second']:.0f} bars/sec")
            Collected 8760 bars in 12.3s
            Performance: 712 bars/sec

        Note:
            This is the recommended high-performance method for new applications.
            Falls back to synchronous method if async context is not available.
        """
        from .concurrent_collection_orchestrator import ConcurrentCollectionOrchestrator

        print(f"\n{'=' * 60}")
        print(f"CONCURRENT COLLECTION: {trading_timeframe.upper()} DATA")
        print(f"Strategy: Hybrid Monthly+Daily with {13} Concurrent Downloads")
        print(f"{'=' * 60}")

        if trading_timeframe not in self.available_timeframes:
            print(f"‚ùå Timeframe '{trading_timeframe}' not available")
            print(f"üìä Available timeframes: {', '.join(self.available_timeframes)}")
            return {"dataframe": pd.DataFrame(), "filepath": None, "stats": {}}

        try:
            # Initialize concurrent orchestrator
            orchestrator = ConcurrentCollectionOrchestrator(
                symbol=self.symbol,
                start_date=self.start_date,
                end_date=self.end_date,
                output_dir=self.output_dir,
                max_concurrent=13,
            )

            async with orchestrator:
                # Execute concurrent collection
                collection_result = await orchestrator.collect_timeframe_concurrent(
                    trading_timeframe, progress_callback=self._progress_callback
                )

                if not collection_result.success or not collection_result.processed_data:
                    print(f"‚ùå Concurrent collection failed for {trading_timeframe}")
                    if collection_result.errors:
                        for error in collection_result.errors:
                            print(f"   Error: {error}")
                    return {"dataframe": pd.DataFrame(), "filepath": None, "stats": {}}

                # Process data using existing methods
                processed_data = collection_result.processed_data

                # Calculate performance stats
                bars_per_second = (
                    collection_result.total_bars / collection_result.collection_time
                    if collection_result.collection_time > 0
                    else 0
                )

                collection_stats = {
                    "method": "concurrent_hybrid",
                    "duration": collection_result.collection_time,
                    "bars_per_second": bars_per_second,
                    "total_bars": collection_result.total_bars,
                    "successful_downloads": collection_result.successful_downloads,
                    "failed_downloads": collection_result.failed_downloads,
                    "data_source_breakdown": collection_result.data_source_breakdown,
                    "concurrent_downloads": 13,
                    "strategy": "monthly_historical_daily_recent",
                }

                # Save to CSV using existing method
                filepath = self.save_data(trading_timeframe, processed_data, collection_stats)

                # Convert to DataFrame
                columns = [
                    "date",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "close_time",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                ]
                df = pd.DataFrame(processed_data, columns=columns)

                # Convert numeric columns
                numeric_cols = [
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                ]
                for col in numeric_cols:
                    df[col] = pd.to_numeric(df[col], errors="coerce")

                # Convert date columns
                df["date"] = pd.to_datetime(df["date"])
                df["close_time"] = pd.to_datetime(df["close_time"])

                print("\n‚úÖ CONCURRENT COLLECTION SUCCESS")
                print(f"üìä Collected: {len(df):,} bars")
                print(f"‚ö° Performance: {bars_per_second:.0f} bars/sec")
                print(
                    f"üöÄ Speed: {collection_result.collection_time:.1f}s vs ~{collection_result.collection_time * 10:.0f}s sequential"
                )
                print(
                    f"üìÅ Sources: {collection_result.data_source_breakdown['monthly']} monthly + {collection_result.data_source_breakdown['daily']} daily"
                )

                return {
                    "dataframe": df,
                    "filepath": filepath,
                    "stats": collection_stats,
                    "collection_method": "concurrent_hybrid",
                }

        except Exception as e:
            print(f"‚ùå Concurrent collection failed: {e}")
            print("‚èÆÔ∏è  Falling back to synchronous method...")
            # Fallback to synchronous method
            return self.collect_timeframe_data(trading_timeframe)

    async def collect_multiple_timeframes_concurrent(
        self, timeframes: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """
        Collect multiple timeframes using concurrent hybrid strategy.

        High-performance collection across multiple timeframes with optimal
        resource utilization and parallel processing.

        Args:
            timeframes (list, optional): List of timeframes to collect.
                If None, defaults to ["1m", "3m", "5m", "15m", "30m", "1h", "2h"].

        Returns:
            dict: Collection results by timeframe with comprehensive performance metrics.

        Examples:
            >>> collector = BinancePublicDataCollector(symbol="ETHUSDT")
            >>> results = await collector.collect_multiple_timeframes_concurrent(["1h", "4h"])
            >>> for timeframe, result in results.items():
            ...     stats = result["stats"]
            ...     print(f"{timeframe}: {stats['total_bars']} bars in {stats['duration']:.1f}s")
            1h: 8760 bars in 15.2s
            4h: 2190 bars in 8.7s

        Note:
            This method processes timeframes sequentially to avoid overwhelming
            servers, but each timeframe uses full concurrent downloading.
        """
        from .concurrent_collection_orchestrator import ConcurrentCollectionOrchestrator

        if timeframes is None:
            timeframes = ["1m", "3m", "5m", "15m", "30m", "1h", "2h"]

        print("\nüöÄ CONCURRENT MULTI-TIMEFRAME COLLECTION")
        print(f"Strategy: Hybrid Monthly+Daily with {13} Concurrent Downloads")
        print(f"Timeframes: {timeframes}")
        print("=" * 80)

        results = {}
        overall_start = datetime.now()

        try:
            # Initialize concurrent orchestrator
            orchestrator = ConcurrentCollectionOrchestrator(
                symbol=self.symbol,
                start_date=self.start_date,
                end_date=self.end_date,
                output_dir=self.output_dir,
                max_concurrent=13,
            )

            async with orchestrator:
                # Process each timeframe with concurrent downloads
                for i, timeframe in enumerate(timeframes):
                    print(f"\nüìä Processing {timeframe} ({i + 1}/{len(timeframes)})...")

                    result = await self.collect_timeframe_data_concurrent(timeframe)

                    if result and result.get("filepath"):
                        filepath = result["filepath"]
                        results[timeframe] = filepath
                        file_size_mb = filepath.stat().st_size / (1024 * 1024)
                        bars_per_sec = result["stats"]["bars_per_second"]
                        print(
                            f"‚úÖ {timeframe}: {filepath.name} ({file_size_mb:.1f} MB, {bars_per_sec:.0f} bars/sec)"
                        )
                    else:
                        print(f"‚ùå Failed to collect {timeframe} data")

        except Exception as e:
            print(f"‚ùå Concurrent collection failed: {e}")
            print("‚èÆÔ∏è  Falling back to synchronous method...")
            # Fallback to synchronous method
            return self.collect_multiple_timeframes(timeframes)

        overall_duration = (datetime.now() - overall_start).total_seconds()

        print("\n" + "=" * 80)
        print("üéâ CONCURRENT MULTI-TIMEFRAME COLLECTION COMPLETE")
        print(
            f"‚è±Ô∏è  Total time: {overall_duration:.1f} seconds ({overall_duration / 60:.1f} minutes)"
        )
        print(f"üìä Generated {len(results)} datasets")
        print("üöÄ Average speedup: ~10-15x faster than sequential downloads")

        return results

    def _progress_callback(self, completed: int, total: int, current_task):
        """Progress callback for concurrent downloads."""
        if completed % 5 == 0 or completed == total:  # Report every 5 downloads or at completion
            percentage = (completed / total) * 100
            source_type = current_task.source_type.value
            print(
                f"   üì• Progress: {completed}/{total} ({percentage:.1f}%) - {source_type}: {current_task.filename}"
            )

    def validate_csv_file(
        self, csv_filepath: Union[str, Path], expected_timeframe: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Comprehensive validation of CSV file data integrity, completeness, and quality.

        Args:
            csv_filepath: Path to CSV file to validate
            expected_timeframe: Expected timeframe (e.g., '30m') for interval validation

        Returns:
            dict: Validation results with detailed analysis
        """
        print(f"\n{'=' * 60}")
        print(f"VALIDATING: {csv_filepath.name}")
        print(f"{'=' * 60}")

        validation_results = {
            "validation_timestamp": datetime.now(timezone.utc).isoformat() + "Z",
            "file_path": str(csv_filepath),
            "file_exists": csv_filepath.exists(),
            "file_size_mb": 0,
            "total_errors": 0,
            "total_warnings": 0,
            "validation_summary": "UNKNOWN",
        }

        if not csv_filepath.exists():
            validation_results["validation_summary"] = "FAILED - File not found"
            validation_results["total_errors"] = 1
            return validation_results

        validation_results["file_size_mb"] = csv_filepath.stat().st_size / (1024 * 1024)

        try:
            # Load CSV data efficiently
            print("Loading and parsing CSV data...")
            df = pd.read_csv(csv_filepath, comment="#")
            validation_results["total_bars"] = len(df)
            print(f"  ‚úÖ Loaded {len(df):,} data bars")

            # 1. BASIC STRUCTURE VALIDATION
            print("\n1. BASIC STRUCTURE VALIDATION")
            structure_validation = self._validate_csv_structure(df)
            validation_results["structure_validation"] = structure_validation
            print(f"  Columns: {structure_validation['status']}")
            if structure_validation["errors"]:
                for error in structure_validation["errors"]:
                    print(f"    ‚ùå {error}")
                    validation_results["total_errors"] += 1

            # 2. DATE/TIME VALIDATION
            print("\n2. DATE/TIME VALIDATION")
            datetime_validation = self._validate_datetime_sequence(df, expected_timeframe)
            validation_results["datetime_validation"] = datetime_validation
            print(
                f"  Date Range: {datetime_validation['date_range']['start']} to {datetime_validation['date_range']['end']}"
            )
            print(f"  Duration: {datetime_validation['duration_days']:.1f} days")
            print(f"  Gaps Found: {datetime_validation['gaps_found']}")
            print(f"  Sequence: {datetime_validation['chronological_order']}")

            if datetime_validation["errors"]:
                for error in datetime_validation["errors"]:
                    print(f"    ‚ùå {error}")
                    validation_results["total_errors"] += 1
            if datetime_validation["warnings"]:
                for warning in datetime_validation["warnings"]:
                    print(f"    ‚ö†Ô∏è  {warning}")
                    validation_results["total_warnings"] += 1

            # 3. OHLCV DATA QUALITY VALIDATION
            print("\n3. OHLCV DATA QUALITY VALIDATION")
            ohlcv_validation = self._validate_ohlcv_quality(df)
            validation_results["ohlcv_validation"] = ohlcv_validation
            print(
                f"  Price Range: ${ohlcv_validation['price_range']['min']:.4f} - ${ohlcv_validation['price_range']['max']:.4f}"
            )
            print(
                f"  Volume Range: {ohlcv_validation['volume_stats']['min']:.2f} - {ohlcv_validation['volume_stats']['max']:,.0f}"
            )
            print(f"  OHLC Logic Errors: {ohlcv_validation['ohlc_errors']}")
            print(f"  Negative/Zero Values: {ohlcv_validation['negative_zero_values']}")

            if ohlcv_validation["errors"]:
                for error in ohlcv_validation["errors"]:
                    print(f"    ‚ùå {error}")
                    validation_results["total_errors"] += 1
            if ohlcv_validation["warnings"]:
                for warning in ohlcv_validation["warnings"]:
                    print(f"    ‚ö†Ô∏è  {warning}")
                    validation_results["total_warnings"] += 1

            # 4. EXPECTED COVERAGE VALIDATION
            print("\n4. EXPECTED COVERAGE VALIDATION")
            coverage_validation = self._validate_expected_coverage(df, expected_timeframe)
            validation_results["coverage_validation"] = coverage_validation
            print(f"  Expected Bars: {coverage_validation['expected_bars']:,}")
            print(f"  Actual Bars: {coverage_validation['actual_bars']:,}")
            print(f"  Coverage: {coverage_validation['coverage_percentage']:.1f}%")

            # 5. STATISTICAL ANOMALY DETECTION
            print("\n5. STATISTICAL ANOMALY DETECTION")
            anomaly_validation = self._validate_statistical_anomalies(df)
            validation_results["anomaly_validation"] = anomaly_validation
            print(f"  Price Outliers: {anomaly_validation['price_outliers']}")
            print(f"  Volume Outliers: {anomaly_validation['volume_outliers']}")
            print(f"  Suspicious Patterns: {anomaly_validation['suspicious_patterns']}")

            # FINAL VALIDATION SUMMARY
            if validation_results["total_errors"] == 0:
                if validation_results["total_warnings"] == 0:
                    validation_results["validation_summary"] = "PERFECT - No errors or warnings"
                    print("\n‚úÖ VALIDATION RESULT: PERFECT")
                    print("   No errors or warnings found. Data quality is excellent.")
                else:
                    validation_results["validation_summary"] = (
                        f"GOOD - {validation_results['total_warnings']} warnings"
                    )
                    print("\n‚úÖ VALIDATION RESULT: GOOD")
                    print(
                        f"   No errors, but {validation_results['total_warnings']} warnings found."
                    )
            else:
                validation_results["validation_summary"] = (
                    f"FAILED - {validation_results['total_errors']} errors, {validation_results['total_warnings']} warnings"
                )
                print("\n‚ùå VALIDATION RESULT: FAILED")
                print(
                    f"   {validation_results['total_errors']} errors and {validation_results['total_warnings']} warnings found."
                )

        except Exception as e:
            validation_results["validation_summary"] = f"ERROR - {str(e)}"
            validation_results["total_errors"] += 1
            print(f"‚ùå Validation failed with exception: {e}")

        return validation_results

    def _validate_csv_structure(self, df):
        """Validate CSV has correct structure and columns."""
        # Enhanced expected columns for complete microstructure data
        expected_columns = [
            "date",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "close_time",
            "quote_asset_volume",
            "number_of_trades",
            "taker_buy_base_asset_volume",
            "taker_buy_quote_asset_volume",
        ]

        # Legacy format for backward compatibility
        legacy_columns = ["date", "open", "high", "low", "close", "volume"]

        errors = []
        warnings = []

        # Check if it's enhanced or legacy format
        has_enhanced_format = all(col in df.columns for col in expected_columns)
        has_legacy_format = all(col in df.columns for col in legacy_columns)

        if has_enhanced_format:
            # Validate enhanced format
            missing_columns = [col for col in expected_columns if col not in df.columns]
            if missing_columns:
                errors.append(f"Missing enhanced columns: {missing_columns}")
        elif has_legacy_format:
            # Legacy format detected
            warnings.append(
                "Legacy format detected - missing microstructure columns for advanced analysis"
            )
            missing_enhanced = [col for col in expected_columns if col not in df.columns]
            warnings.append(f"Enhanced features unavailable: {missing_enhanced}")
        else:
            # Neither format complete
            missing_basic = [col for col in legacy_columns if col not in df.columns]
            errors.append(f"Missing basic required columns: {missing_basic}")

        extra_columns = [col for col in df.columns if col not in expected_columns]
        if extra_columns:
            warnings.append(f"Unexpected extra columns: {extra_columns}")

        # Check for empty data
        if len(df) == 0:
            errors.append("CSV file is empty (no data rows)")

        return {
            "status": "VALID" if not errors else "INVALID",
            "format_type": "enhanced"
            if has_enhanced_format
            else "legacy"
            if has_legacy_format
            else "incomplete",
            "errors": errors,
            "warnings": warnings,
            "columns_found": list(df.columns),
            "expected_columns": expected_columns,
            "legacy_columns": legacy_columns,
        }

    def _validate_datetime_sequence(self, df, expected_timeframe):
        """Validate datetime sequence is complete and chronological."""
        errors = []
        warnings = []
        gaps_found = 0

        # Convert date column to datetime
        try:
            df["datetime"] = pd.to_datetime(df["date"])
        except Exception as e:
            errors.append(f"Failed to parse dates: {e}")
            return {"status": "INVALID", "errors": errors, "warnings": warnings}

        # Check chronological order
        is_sorted = df["datetime"].is_monotonic_increasing

        # Find gaps if we have expected timeframe
        gap_details = []
        if expected_timeframe and len(df) > 1:
            # Calculate expected interval in minutes
            interval_map = {"1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30, "1h": 60, "2h": 120}
            expected_interval = interval_map.get(expected_timeframe, 0)

            if expected_interval > 0:
                expected_delta = pd.Timedelta(minutes=expected_interval)

                # Check for gaps
                for i in range(1, len(df)):
                    actual_delta = df["datetime"].iloc[i] - df["datetime"].iloc[i - 1]
                    if actual_delta > expected_delta:
                        gaps_found += 1
                        gap_details.append(
                            {
                                "position": i,
                                "expected_time": (
                                    df["datetime"].iloc[i - 1] + expected_delta
                                ).isoformat(),
                                "actual_time": df["datetime"].iloc[i].isoformat(),
                                "gap_duration": str(actual_delta - expected_delta),
                            }
                        )

                        # Record every single gap for complete validation tracking
                        warnings.append(
                            f"Gap at position {i}: expected {expected_delta}, got {actual_delta}"
                        )

        if not is_sorted:
            errors.append("Timestamps are not in chronological order")

        if gaps_found > 10:
            errors.append(f"Too many gaps found: {gaps_found} (data may be incomplete)")
        elif gaps_found > 0:
            warnings.append(f"{gaps_found} timestamp gaps found (market closures or data issues)")

        return {
            "status": "VALID" if not errors else "INVALID",
            "errors": errors,
            "warnings": warnings,
            "date_range": {
                "start": df["datetime"].min().isoformat(),
                "end": df["datetime"].max().isoformat(),
            },
            "duration_days": (df["datetime"].max() - df["datetime"].min()).days,
            "chronological_order": is_sorted,
            "gaps_found": gaps_found,
            "gap_details": gap_details,  # Complete gap details for thorough analysis
        }

    def _validate_ohlcv_quality(self, df):
        """Validate OHLCV data quality and logical consistency."""
        errors = []
        warnings = []

        # Check for negative or zero values
        negative_zero_count = 0
        for col in ["open", "high", "low", "close"]:
            negative_zero = (df[col] <= 0).sum()
            if negative_zero > 0:
                errors.append(f"Found {negative_zero} negative/zero values in {col}")
                negative_zero_count += negative_zero

        # Check volume (can be zero but not negative)
        negative_volume = (df["volume"] < 0).sum()
        if negative_volume > 0:
            errors.append(f"Found {negative_volume} negative volume values")

        zero_volume = (df["volume"] == 0).sum()
        if zero_volume > 0:
            warnings.append(f"Found {zero_volume} zero volume bars")

        # Check OHLC logic: High >= Low, Open/Close within High/Low range
        ohlc_errors = 0

        # High should be >= Low
        high_low_errors = (df["high"] < df["low"]).sum()
        if high_low_errors > 0:
            errors.append(f"Found {high_low_errors} bars where High < Low")
            ohlc_errors += high_low_errors

        # Open should be within High/Low range
        open_range_errors = ((df["open"] > df["high"]) | (df["open"] < df["low"])).sum()
        if open_range_errors > 0:
            errors.append(f"Found {open_range_errors} bars where Open is outside High/Low range")
            ohlc_errors += open_range_errors

        # Close should be within High/Low range
        close_range_errors = ((df["close"] > df["high"]) | (df["close"] < df["low"])).sum()
        if close_range_errors > 0:
            errors.append(f"Found {close_range_errors} bars where Close is outside High/Low range")
            ohlc_errors += close_range_errors

        return {
            "status": "VALID" if not errors else "INVALID",
            "errors": errors,
            "warnings": warnings,
            "price_range": {
                "min": min(df["low"].min(), df["high"].min(), df["open"].min(), df["close"].min()),
                "max": max(df["low"].max(), df["high"].max(), df["open"].max(), df["close"].max()),
            },
            "volume_stats": {
                "min": df["volume"].min(),
                "max": df["volume"].max(),
                "mean": df["volume"].mean(),
            },
            "ohlc_errors": ohlc_errors,
            "negative_zero_values": negative_zero_count,
        }

    def _validate_expected_coverage(self, df, expected_timeframe):
        """Validate data coverage matches expected timeframe and duration."""
        warnings = []

        if not expected_timeframe or len(df) == 0:
            return {"status": "SKIPPED", "warnings": ["Cannot validate coverage without timeframe"]}

        # Calculate expected bars based on timeframe and actual date range
        df["datetime"] = pd.to_datetime(df["date"])
        start_time = df["datetime"].min()
        end_time = df["datetime"].max()
        duration = end_time - start_time

        # Calculate expected number of bars
        interval_map = {"1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30, "1h": 60, "2h": 120}
        interval_minutes = interval_map.get(expected_timeframe, 0)

        if interval_minutes > 0:
            expected_bars = int(duration.total_seconds() / (interval_minutes * 60)) + 1
            actual_bars = len(df)
            coverage_percentage = (actual_bars / expected_bars) * 100

            if coverage_percentage < 95:
                warnings.append(
                    f"Low coverage: {coverage_percentage:.1f}% (may indicate missing data)"
                )
            elif coverage_percentage > 105:
                warnings.append(
                    f"High coverage: {coverage_percentage:.1f}% (may indicate duplicate data)"
                )
        else:
            expected_bars = 0
            coverage_percentage = 0
            warnings.append(f"Unknown timeframe '{expected_timeframe}' for coverage calculation")

        return {
            "status": "VALID" if not warnings else "WARNING",
            "warnings": warnings,
            "expected_bars": expected_bars,
            "actual_bars": len(df),
            "coverage_percentage": coverage_percentage,
            "duration_days": duration.days,
        }

    def _validate_statistical_anomalies(self, df):
        """Detect statistical anomalies in price and volume data."""
        warnings = []

        # Calculate basic statistics
        price_cols = ["open", "high", "low", "close"]

        # Price outliers (using IQR method)
        price_outliers = 0
        for col in price_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            price_outliers += outliers

        # Volume outliers
        vol_Q1 = df["volume"].quantile(0.25)
        vol_Q3 = df["volume"].quantile(0.75)
        vol_IQR = vol_Q3 - vol_Q1
        vol_upper_bound = vol_Q3 + 1.5 * vol_IQR
        volume_outliers = (df["volume"] > vol_upper_bound).sum()

        # Suspicious patterns
        suspicious_patterns = 0

        # Check for repeated identical prices (suspicious)
        for col in price_cols:
            repeated = df[col].value_counts()
            max_repeats = repeated.max()
            if max_repeats > len(df) * 0.1:  # More than 10% identical values
                warnings.append(f"Suspicious: {col} has {max_repeats} repeated values")
                suspicious_patterns += 1

        if price_outliers > len(df) * 0.05:  # More than 5% outliers
            warnings.append(
                f"High number of price outliers: {price_outliers} ({100 * price_outliers / len(df):.1f}%)"
            )

        if volume_outliers > len(df) * 0.02:  # More than 2% volume outliers
            warnings.append(
                f"High number of volume outliers: {volume_outliers} ({100 * volume_outliers / len(df):.1f}%)"
            )

        return {
            "status": "VALID" if not warnings else "WARNING",
            "warnings": warnings,
            "price_outliers": price_outliers,
            "volume_outliers": volume_outliers,
            "suspicious_patterns": suspicious_patterns,
        }

    def update_metadata_with_validation(self, csv_filepath, validation_results):
        """Update metadata JSON file with validation results."""
        metadata_filepath = csv_filepath.with_suffix(".metadata.json")

        if metadata_filepath.exists():
            with open(metadata_filepath, "r") as f:
                metadata = json.load(f)
        else:
            metadata = {}

        # Add validation results to metadata
        metadata["validation"] = validation_results

        # Update compliance status based on validation
        compliance = metadata.get("compliance", {})
        if validation_results["total_errors"] == 0:
            compliance["data_validation_passed"] = True
            compliance["validation_summary"] = validation_results["validation_summary"]
        else:
            compliance["data_validation_passed"] = False
            compliance["validation_summary"] = validation_results["validation_summary"]
            compliance["validation_errors"] = validation_results["total_errors"]
            compliance["validation_warnings"] = validation_results["total_warnings"]

        metadata["compliance"] = compliance

        # Save updated metadata with JSON serialization fix
        def convert_numpy_types(obj):
            """Convert numpy types to Python native types for JSON serialization."""
            if hasattr(obj, "item"):
                return obj.item()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj

        with open(metadata_filepath, "w") as f:
            json.dump(convert_numpy_types(metadata), f, indent=2)

        print(f"‚úÖ Updated metadata: {metadata_filepath.name}")

    def apply_gap_filling_to_validated_files(self):
        """Apply comprehensive gap filling to validated data files using authentic Binance API data"""

        try:
            print("\nüîß INTEGRATED GAP FILLING SYSTEM")
            print("Primary Source: Binance REST API (Authentic Data Only)")
            print("=" * 60)

            # Initialize gap filling components
            gap_filler = UniversalGapFiller()

            # Find CSV files to check for gaps
            csv_files = list(Path(self.output_dir).glob("*.csv"))

            if not csv_files:
                print("‚ùå No CSV files found for gap filling")
                return

            # Filter to only files for this symbol
            symbol_files = [f for f in csv_files if self.symbol in f.name]

            if not symbol_files:
                print(f"‚ùå No CSV files found for symbol {self.symbol}")
                return

            print(f"üîç Analyzing {len(symbol_files)} files for gaps...")

            total_gaps_detected = 0
            total_gaps_filled = 0
            total_gaps_failed = 0
            files_processed = 0
            results = []

            for csv_file in symbol_files:
                print(f"\nüìÅ Processing: {csv_file.name}")

                # Extract timeframe from filename
                file_timeframe = self._extract_timeframe_from_filename(csv_file.name)
                print(f"   üìä Detected timeframe: {file_timeframe}")

                # Use the proper UniversalGapFiller process_file method
                result = gap_filler.process_file(csv_file, file_timeframe)
                results.append(result)
                files_processed += 1

                # Update totals
                total_gaps_detected += result["gaps_detected"]
                total_gaps_filled += result["gaps_filled"]
                total_gaps_failed += result["gaps_failed"]

                # Report per-file results
                if result["gaps_detected"] == 0:
                    print(f"   ‚úÖ No gaps found in {file_timeframe}")
                else:
                    success_rate = result["success_rate"]
                    status = "‚úÖ" if success_rate == 100.0 else "‚ö†Ô∏è" if success_rate > 0 else "‚ùå"
                    print(
                        f"   {status} {result['gaps_filled']}/{result['gaps_detected']} gaps filled ({success_rate:.1f}%)"
                    )

            # Comprehensive summary
            print("\n" + "=" * 60)
            print("üìä GAP FILLING SUMMARY")
            print("=" * 60)

            for result in results:
                if result["gaps_detected"] > 0:
                    status = (
                        "‚úÖ"
                        if result["success_rate"] == 100.0
                        else "‚ö†Ô∏è"
                        if result["success_rate"] > 0
                        else "‚ùå"
                    )
                    print(
                        f"{status} {result['timeframe']:>3}: {result['gaps_filled']:>2}/{result['gaps_detected']:>2} gaps filled ({result['success_rate']:>5.1f}%)"
                    )

            print("-" * 60)
            overall_success = (
                (total_gaps_filled / total_gaps_detected * 100)
                if total_gaps_detected > 0
                else 100.0
            )
            print(
                f"üéØ OVERALL: {total_gaps_filled}/{total_gaps_detected} gaps filled ({overall_success:.1f}%)"
            )

            if overall_success == 100.0:
                print("üéâ ALL GAPS FILLED SUCCESSFULLY!")
                print("‚úÖ Datasets are now 100% gapless and ready for production use")
            else:
                print(
                    f"‚ö†Ô∏è  {total_gaps_failed} gaps failed to fill (may be legitimate exchange outages)"
                )
                print("üìã Review failed gaps to confirm they are legitimate market closures")

            print(f"\nFiles processed: {files_processed}")
            print("Data source: Authentic Binance REST API")
            print("Gap filling protocol: API-first validation (no synthetic data)")

        except Exception as e:
            print(f"‚ùå Gap filling error: {e}")
            print("‚ö†Ô∏è  Continuing without gap filling...")
            import traceback

            traceback.print_exc()

    def _extract_timeframe_from_filename(self, filename):
        """Extract timeframe from filename (e.g., 'SOLUSDT-15m-data.csv' -> '15m')"""
        for tf in [
            "1s",
            "1m",
            "3m",
            "5m",
            "15m",
            "30m",
            "1h",
            "2h",
            "4h",
            "6h",
            "8h",
            "12h",
            "1d",
            "3d",
            "1w",
            "1mo",
        ]:
            if f"-{tf}_" in filename or f"-{tf}-" in filename:
                return tf
        return "15m"  # Default


def main():
    """Main execution function with CLI argument support."""
    parser = argparse.ArgumentParser(
        description="Ultra-fast Binance spot data collector with validation"
    )
    parser.add_argument(
        "--symbol", default="SOLUSDT", help="Trading pair symbol (default: SOLUSDT)"
    )
    parser.add_argument(
        "--timeframes",
        default="1m,3m,5m,15m,30m,1h,2h",
        help="Comma-separated timeframes (default: 1m,3m,5m,15m,30m,1h,2h)",
    )
    parser.add_argument(
        "--start", default="2020-08-15", help="Start date YYYY-MM-DD (default: 2020-08-15)"
    )
    parser.add_argument(
        "--end", default="2025-03-20", help="End date YYYY-MM-DD (default: 2025-03-20)"
    )
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate existing CSV files, do not collect new data",
    )
    parser.add_argument(
        "--validate-files", nargs="+", help="Specific CSV files to validate (with --validate-only)"
    )
    parser.add_argument(
        "--no-validation",
        action="store_true",
        help="Skip validation after collection (not recommended)",
    )

    args = parser.parse_args()

    print("Binance Public Data Ultra-Fast Collector with Validation")
    print("Official Binance data repository - 10-100x faster than API")
    print("=" * 80)

    # Initialize collector
    collector = BinancePublicDataCollector(
        symbol=args.symbol, start_date=args.start, end_date=args.end
    )

    if args.validate_only:
        # VALIDATION-ONLY MODE
        print("üîç VALIDATION-ONLY MODE")

        if args.validate_files:
            # Validate specific files
            files_to_validate = [Path(f) for f in args.validate_files]
        else:
            # Auto-discover CSV files in sample_data directory
            pattern = f"*{args.symbol}*.csv"
            files_to_validate = list(collector.output_dir.glob(pattern))

        if not files_to_validate:
            print("‚ùå No CSV files found to validate")
            return 1

        print(f"Found {len(files_to_validate)} files to validate:")
        for file_path in files_to_validate:
            print(f"  üìÑ {file_path.name}")

        validation_summary = []
        for csv_file in files_to_validate:
            # Extract timeframe from filename for validation
            timeframe = None
            for tf in ["1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h", "1d"]:
                if f"-{tf}_" in csv_file.name:
                    timeframe = tf
                    break

            # Validate file
            validation_result = collector.validate_csv_file(csv_file, timeframe)

            # Update metadata with validation results
            collector.update_metadata_with_validation(csv_file, validation_result)

            validation_summary.append(
                {
                    "file": csv_file.name,
                    "status": validation_result["validation_summary"],
                    "errors": validation_result["total_errors"],
                    "warnings": validation_result["total_warnings"],
                }
            )

        # Print validation summary
        print("\n" + "=" * 80)
        print("VALIDATION SUMMARY")
        print("=" * 80)

        perfect_files = 0
        good_files = 0
        failed_files = 0

        for summary in validation_summary:
            if summary["errors"] == 0:
                if summary["warnings"] == 0:
                    status_icon = "‚úÖ"
                    perfect_files += 1
                else:
                    status_icon = "‚ö†Ô∏è "
                    good_files += 1
            else:
                status_icon = "‚ùå"
                failed_files += 1

            print(f"{status_icon} {summary['file']}: {summary['status']}")
            if summary["errors"] > 0 or summary["warnings"] > 0:
                print(f"   ‚îî‚îÄ {summary['errors']} errors, {summary['warnings']} warnings")

        print("\nOVERALL RESULTS:")
        print(f"  ‚úÖ Perfect: {perfect_files} files")
        print(f"  ‚ö†Ô∏è  Good: {good_files} files")
        print(f"  ‚ùå Failed: {failed_files} files")

        if failed_files == 0:
            print("\nüéâ ALL VALIDATIONS PASSED!")
            return 0
        else:
            print(f"\n‚ö†Ô∏è  {failed_files} files failed validation")
            return 1

    else:
        # COLLECTION MODE (with optional validation)
        timeframes = [tf.strip() for tf in args.timeframes.split(",")]
        print(f"Collecting timeframes: {timeframes}")

        # Collect data
        results = collector.collect_multiple_timeframes(timeframes)

        if results:
            print(f"\nüöÄ ULTRA-FAST COLLECTION SUCCESS: Generated {len(results)} datasets")

            # Auto-validation after collection (unless disabled)
            if not args.no_validation:
                print("\nüîç AUTO-VALIDATION AFTER COLLECTION")
                validation_passed = 0
                validation_failed = 0

                for timeframe, csv_file in results.items():
                    validation_result = collector.validate_csv_file(csv_file, timeframe)
                    collector.update_metadata_with_validation(csv_file, validation_result)

                    if validation_result["total_errors"] == 0:
                        validation_passed += 1
                    else:
                        validation_failed += 1

                print(
                    f"\nVALIDATION RESULTS: {validation_passed} passed, {validation_failed} failed"
                )

                if validation_failed == 0:
                    print("üéâ ALL FILES VALIDATED SUCCESSFULLY!")
                    print("Ready for ML training, backtesting, and production use")

                    # AUTOMATIC GAP FILLING - Now using comprehensive gap detection and filling
                    collector.apply_gap_filling_to_validated_files()

                else:
                    print("‚ö†Ô∏è  Some files failed validation - check errors above")

            return 0
        else:
            print("‚ùå Collection failed")
            return 1


if __name__ == "__main__":
    exit(main())
