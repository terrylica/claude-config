"""
IC Validation: 121-feature vs 79-feature (redundancy filtering).

Validates that removing 42 redundant features does not degrade predictive power
by more than 5%.

SLOs:
- Availability: 100% (all inputs validated, explicit errors)
- Correctness: 100% (uses scipy Spearman correlation)
- Observability: Full type hints, structured output to CSV/JSON
- Maintainability: Single-responsibility functions, ≤80 lines per function

Error Handling: raise_and_propagate
- ValueError on insufficient data or validation failures
- All calculation errors propagated (no silent handling)

Pass Criteria:
- ic_delta_pct > -5.0% (average IC must not degrade more than 5%)
- Individual feature IC comparison shows no critical losses

Reference:
- Data: Out-of-sample OHLCV (not used in correlation analysis)
- Methodology: Spearman rank correlation per feature
- Output: /tmp/ic_validation_redundancy_removal.csv
"""

import json
from datetime import datetime, timedelta
from pathlib import Path

import gapless_crypto_data as gcd
import numpy as np
import pandas as pd

from atr_adaptive_laguerre import ATRAdaptiveLaguerreRSI, ATRAdaptiveLaguerreRSIConfig
from atr_adaptive_laguerre.features.redundancy_filter import RedundancyFilter
from atr_adaptive_laguerre.validation import calculate_information_coefficient


def load_out_of_sample_data(
    symbol: str = "BTCUSDT",
    interval: str = "2h",
    start_date: str = "2025-01-01",
    end_date: str = "2025-09-30",
) -> pd.DataFrame:
    """
    Load out-of-sample OHLCV data (not used in correlation analysis).

    Original analysis used 2022-10-01 to 2025-09-30.
    This uses 2025-01-01 to 2025-09-30 as out-of-sample test set.

    Args:
        symbol: Trading pair symbol
        interval: Bar interval (e.g., '2h')
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)

    Returns:
        DataFrame with OHLCV columns

    Raises:
        ValueError: If data loading fails or insufficient data
    """
    print(f"Fetching {symbol} {interval} from {start_date} to {end_date}...")
    df = gcd.download(symbol, timeframe=interval, start=start_date, end=end_date)

    if df is None or df.empty:
        raise ValueError(f"No data loaded for {symbol} {interval} from {start_date} to {end_date}")

    if len(df) < 500:
        raise ValueError(f"Insufficient out-of-sample data: {len(df)} bars (need ≥500)")

    print(f"Loaded {len(df)} bars for {symbol} {interval} ({start_date} to {end_date})")
    return df


def calculate_feature_ics(
    features_df: pd.DataFrame, prices: pd.Series, forward_periods: int = 1
) -> pd.DataFrame:
    """
    Calculate IC for each feature column.

    Args:
        features_df: DataFrame with multiple feature columns
        prices: Price series for forward return calculation
        forward_periods: Look-ahead periods for IC calculation

    Returns:
        DataFrame with columns: [feature_name, ic, abs_ic]
        Sorted by abs_ic descending

    Raises:
        ValueError: If any IC calculation fails
    """
    ic_results = []

    for col in features_df.columns:
        try:
            ic = calculate_information_coefficient(
                feature=features_df[col],
                prices=prices,
                forward_periods=forward_periods,
                return_type="log",
            )
            ic_results.append({"feature_name": col, "ic": ic, "abs_ic": abs(ic)})
        except Exception as e:
            # Raise and propagate - no silent failures
            raise ValueError(f"IC calculation failed for feature '{col}': {e}") from e

    ic_df = pd.DataFrame(ic_results)
    ic_df = ic_df.sort_values("abs_ic", ascending=False).reset_index(drop=True)

    return ic_df


def compare_feature_sets(
    ic_121: pd.DataFrame, ic_79: pd.DataFrame, redundant_features: list[str]
) -> dict:
    """
    Compare IC statistics between 121-feature and 79-feature sets.

    Args:
        ic_121: IC DataFrame for 121 features
        ic_79: IC DataFrame for 79 features
        redundant_features: List of 42 removed feature names

    Returns:
        Dictionary with comparison metrics
    """
    # Calculate aggregate statistics
    avg_ic_121 = ic_121["ic"].mean()
    avg_ic_79 = ic_79["ic"].mean()
    avg_abs_ic_121 = ic_121["abs_ic"].mean()
    avg_abs_ic_79 = ic_79["abs_ic"].mean()

    # Calculate delta percentage
    ic_delta = avg_ic_79 - avg_ic_121
    ic_delta_pct = (ic_delta / abs(avg_ic_121)) * 100 if avg_ic_121 != 0 else 0

    abs_ic_delta = avg_abs_ic_79 - avg_abs_ic_121
    abs_ic_delta_pct = (abs_ic_delta / avg_abs_ic_121) * 100 if avg_abs_ic_121 != 0 else 0

    # Identify removed features with high IC
    ic_121_removed = ic_121[ic_121["feature_name"].isin(redundant_features)]
    high_ic_removed = ic_121_removed[ic_121_removed["abs_ic"] > 0.03]

    comparison = {
        "n_features_121": len(ic_121),
        "n_features_79": len(ic_79),
        "n_features_removed": len(redundant_features),
        "avg_ic_121": float(avg_ic_121),
        "avg_ic_79": float(avg_ic_79),
        "avg_abs_ic_121": float(avg_abs_ic_121),
        "avg_abs_ic_79": float(avg_abs_ic_79),
        "ic_delta": float(ic_delta),
        "ic_delta_pct": float(ic_delta_pct),
        "abs_ic_delta": float(abs_ic_delta),
        "abs_ic_delta_pct": float(abs_ic_delta_pct),
        "removed_features_with_high_ic": high_ic_removed["feature_name"].tolist(),
        "n_removed_high_ic": len(high_ic_removed),
        "validation_passed": ic_delta_pct > -5.0,
        "pass_criteria": "ic_delta_pct > -5.0%",
    }

    return comparison


def main():
    """Execute IC validation workflow."""
    print("=" * 80)
    print("IC Validation: 121-feature vs 79-feature (Redundancy Filtering)")
    print("=" * 80)
    print()

    # Step 1: Load out-of-sample data
    print("Step 1: Loading out-of-sample data...")
    df_ohlcv = load_out_of_sample_data(
        symbol="BTCUSDT",
        interval="2h",
        start_date="2025-01-01",
        end_date="2025-09-30",
    )
    prices = df_ohlcv["close"]
    print()

    # Step 2: Extract 121 features (no filtering)
    print("Step 2: Extracting 121 features (filter_redundancy=False)...")
    config_121 = ATRAdaptiveLaguerreRSIConfig.multi_interval(
        multiplier_1=4, multiplier_2=12, filter_redundancy=False
    )
    indicator_121 = ATRAdaptiveLaguerreRSI(config_121)
    features_121 = indicator_121.fit_transform_features(df_ohlcv)
    print(f"Extracted features shape: {features_121.shape}")
    assert features_121.shape[1] == 121, f"Expected 121 features, got {features_121.shape[1]}"
    print()

    # Step 3: Extract 79 features (with filtering)
    print("Step 3: Extracting 79 features (filter_redundancy=True)...")
    config_79 = ATRAdaptiveLaguerreRSIConfig.multi_interval(
        multiplier_1=4, multiplier_2=12, filter_redundancy=True
    )
    indicator_79 = ATRAdaptiveLaguerreRSI(config_79)
    features_79 = indicator_79.fit_transform_features(df_ohlcv)
    print(f"Extracted features shape: {features_79.shape}")
    assert features_79.shape[1] == 79, f"Expected 79 features, got {features_79.shape[1]}"
    print()

    # Step 4: Calculate IC for 121 features
    print("Step 4: Calculating IC for 121 features...")
    ic_121 = calculate_feature_ics(features_121, prices, forward_periods=1)
    print(f"Top 5 features by |IC|:")
    print(ic_121.head(5))
    print()

    # Step 5: Calculate IC for 79 features
    print("Step 5: Calculating IC for 79 features...")
    ic_79 = calculate_feature_ics(features_79, prices, forward_periods=1)
    print(f"Top 5 features by |IC|:")
    print(ic_79.head(5))
    print()

    # Step 6: Compare feature sets
    print("Step 6: Comparing feature sets...")
    redundant_features = RedundancyFilter.get_redundant_features()
    comparison = compare_feature_sets(ic_121, ic_79, redundant_features)

    print(f"Average IC (121 features): {comparison['avg_ic_121']:.6f}")
    print(f"Average IC (79 features): {comparison['avg_ic_79']:.6f}")
    print(f"IC Delta: {comparison['ic_delta']:.6f} ({comparison['ic_delta_pct']:.2f}%)")
    print()
    print(f"Average |IC| (121 features): {comparison['avg_abs_ic_121']:.6f}")
    print(f"Average |IC| (79 features): {comparison['avg_abs_ic_79']:.6f}")
    print(f"|IC| Delta: {comparison['abs_ic_delta']:.6f} ({comparison['abs_ic_delta_pct']:.2f}%)")
    print()
    print(f"Removed features with |IC| > 0.03: {comparison['n_removed_high_ic']}")
    if comparison["n_removed_high_ic"] > 0:
        print(f"High-IC removed features: {comparison['removed_features_with_high_ic']}")
    print()

    # Step 7: Validation decision
    print("=" * 80)
    print("VALIDATION RESULT")
    print("=" * 80)
    print(f"Pass Criteria: {comparison['pass_criteria']}")
    print(f"IC Delta Percentage: {comparison['ic_delta_pct']:.2f}%")
    print(f"Validation: {'PASSED ✅' if comparison['validation_passed'] else 'FAILED ❌'}")
    print()

    if not comparison["validation_passed"]:
        raise ValueError(
            f"IC validation FAILED!\n"
            f"IC degradation: {comparison['ic_delta_pct']:.2f}% (threshold: -5.0%)\n"
            f"Redundancy filtering causes unacceptable IC loss."
        )

    # Step 8: Save outputs
    print("Step 7: Saving validation outputs...")
    ic_121.to_csv("/tmp/ic_121_features.csv", index=False)
    ic_79.to_csv("/tmp/ic_79_features.csv", index=False)

    comparison_df = pd.DataFrame([comparison])
    comparison_df.to_csv("/tmp/ic_validation_redundancy_removal.csv", index=False)

    with open("/tmp/ic_validation_redundancy_removal.json", "w") as f:
        json.dump(comparison, f, indent=2)

    print("Outputs saved:")
    print("  - /tmp/ic_121_features.csv")
    print("  - /tmp/ic_79_features.csv")
    print("  - /tmp/ic_validation_redundancy_removal.csv")
    print("  - /tmp/ic_validation_redundancy_removal.json")
    print()
    print("IC Validation Complete ✅")


if __name__ == "__main__":
    main()
