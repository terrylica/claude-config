#!/usr/bin/env python3
"""
Benchmark: Iterative Feature Development Workflow v2

Real-world scenario comparing TWO approaches:

Approach A (DuckDB):
- Store data in DuckDB
- Add indicators by: read → compute → rewrite table
- Benefit: SQL metadata, direct SQL queries, transactional safety

Approach B (Parquet):
- Store data in Parquet
- Add indicators by: read → compute → rewrite file
- Benefit: Smaller size, universal compatibility

KEY INSIGHT: Both require rewriting data when adding columns.
The difference is in metadata, querying, and ecosystem features.
"""

import time
from pathlib import Path
import pandas as pd
import duckdb
import numpy as np


def calculate_rsi(close: pd.Series, period: int = 14) -> pd.Series:
    """Calculate RSI indicator."""
    delta = close.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))


def calculate_macd(close: pd.Series) -> tuple:
    """Calculate MACD indicator."""
    ema_12 = close.ewm(span=12, adjust=False).mean()
    ema_26 = close.ewm(span=26, adjust=False).mean()
    macd_line = ema_12 - ema_26
    signal_line = macd_line.ewm(span=9, adjust=False).mean()
    histogram = macd_line - signal_line
    return macd_line, signal_line, histogram


def calculate_bollinger_bands(close: pd.Series, period: int = 20, std_dev: float = 2.0) -> tuple:
    """Calculate Bollinger Bands."""
    middle = close.rolling(window=period).mean()
    std = close.rolling(window=period).std()
    upper = middle + (std_dev * std)
    lower = middle - (std_dev * std)
    return upper, middle, lower


def calculate_atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
    """Calculate Average True Range."""
    tr1 = high - low
    tr2 = abs(high - close.shift())
    tr3 = abs(low - close.shift())
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    return tr.rolling(window=period).mean()


def benchmark_duckdb_workflow(base_df: pd.DataFrame):
    """Benchmark DuckDB: Add indicators iteratively with metadata."""
    print("█ DuckDB Workflow: Iterative Feature Development")
    print("━" * 70)

    db_path = Path('/tmp/strategy_dev.duckdb')
    if db_path.exists():
        db_path.unlink()

    conn = duckdb.connect(str(db_path))

    # Step 1: Create initial table (5 columns: Timestamp, OHLC)
    start = time.perf_counter()
    conn.execute("CREATE TABLE strategy AS SELECT * FROM base_df")
    conn.execute("COMMENT ON TABLE strategy IS 'EURUSD 1m OHLC with iteratively added indicators'")
    for col in ['Timestamp', 'Open', 'High', 'Low', 'Close']:
        conn.execute(f"COMMENT ON COLUMN strategy.{col} IS 'Base OHLC data from Binance'")
    step1_time = time.perf_counter() - start
    print(f"✓ Step 1: Create base table (5 cols)    {step1_time*1000:.1f} ms")

    # Step 2: Add RSI (read → compute → recreate with new column)
    start = time.perf_counter()
    df = conn.execute("SELECT * FROM strategy").df()
    df['rsi_14'] = calculate_rsi(df['Close'])
    conn.execute("DROP TABLE strategy")
    conn.execute("CREATE TABLE strategy AS SELECT * FROM df")
    conn.execute("COMMENT ON TABLE strategy IS 'EURUSD 1m OHLC with iteratively added indicators'")
    conn.execute("COMMENT ON COLUMN strategy.rsi_14 IS '14-period RSI: oversold <30, overbought >70'")
    step2_time = time.perf_counter() - start
    print(f"✓ Step 2: Add RSI (6 cols)              {step2_time*1000:.1f} ms")

    # Step 3: Add MACD (3 columns)
    start = time.perf_counter()
    df = conn.execute("SELECT * FROM strategy").df()
    macd_line, signal_line, histogram = calculate_macd(df['Close'])
    df['macd_line'] = macd_line
    df['macd_signal'] = signal_line
    df['macd_histogram'] = histogram
    conn.execute("DROP TABLE strategy")
    conn.execute("CREATE TABLE strategy AS SELECT * FROM df")
    conn.execute("COMMENT ON TABLE strategy IS 'EURUSD 1m OHLC with iteratively added indicators'")
    conn.execute("COMMENT ON COLUMN strategy.macd_line IS 'MACD line: 12-EMA minus 26-EMA'")
    conn.execute("COMMENT ON COLUMN strategy.macd_signal IS 'MACD signal: 9-EMA of MACD line'")
    conn.execute("COMMENT ON COLUMN strategy.macd_histogram IS 'MACD histogram: MACD - Signal'")
    step3_time = time.perf_counter() - start
    print(f"✓ Step 3: Add MACD (9 cols)             {step3_time*1000:.1f} ms")

    # Step 4: Add Bollinger Bands (3 columns)
    start = time.perf_counter()
    df = conn.execute("SELECT * FROM strategy").df()
    upper, middle, lower = calculate_bollinger_bands(df['Close'])
    df['bb_upper'] = upper
    df['bb_middle'] = middle
    df['bb_lower'] = lower
    conn.execute("DROP TABLE strategy")
    conn.execute("CREATE TABLE strategy AS SELECT * FROM df")
    conn.execute("COMMENT ON TABLE strategy IS 'EURUSD 1m OHLC with iteratively added indicators'")
    conn.execute("COMMENT ON COLUMN strategy.bb_upper IS 'Bollinger Band Upper: 20-MA + 2*StdDev'")
    conn.execute("COMMENT ON COLUMN strategy.bb_middle IS 'Bollinger Band Middle: 20-period SMA'")
    conn.execute("COMMENT ON COLUMN strategy.bb_lower IS 'Bollinger Band Lower: 20-MA - 2*StdDev'")
    step4_time = time.perf_counter() - start
    print(f"✓ Step 4: Add Bollinger Bands (12 cols) {step4_time*1000:.1f} ms")

    # Step 5: Add ATR
    start = time.perf_counter()
    df = conn.execute("SELECT * FROM strategy").df()
    df['atr_14'] = calculate_atr(df['High'], df['Low'], df['Close'])
    conn.execute("DROP TABLE strategy")
    conn.execute("CREATE TABLE strategy AS SELECT * FROM df")
    conn.execute("COMMENT ON TABLE strategy IS 'EURUSD 1m OHLC with iteratively added indicators'")
    conn.execute("COMMENT ON COLUMN strategy.atr_14 IS '14-period ATR: volatility/risk measure'")
    step5_time = time.perf_counter() - start
    print(f"✓ Step 5: Add ATR (13 cols)             {step5_time*1000:.1f} ms")

    # Step 6: Query metadata
    start = time.perf_counter()
    metadata = conn.execute("""
        SELECT column_name, comment
        FROM duckdb_columns()
        WHERE table_name = 'strategy'
    """).df()
    step6_time = time.perf_counter() - start
    print(f"✓ Step 6: Query metadata                {step6_time*1000:.1f} ms")

    total_time = step1_time + step2_time + step3_time + step4_time + step5_time + step6_time

    # Show final state
    final_size = db_path.stat().st_size / 1024
    row_count = conn.execute("SELECT COUNT(*) FROM strategy").fetchone()[0]
    col_count = len(conn.execute("SELECT * FROM strategy LIMIT 1").df().columns)

    conn.close()

    print(f"\n  Total Time: {total_time*1000:.1f} ms")
    print(f"  Final Size: {final_size:.1f} KB ({row_count:,} rows × {col_count} cols)")
    print(f"  Metadata: ✓ Stored in database (SQL COMMENT)")

    return total_time, final_size


def benchmark_parquet_workflow(base_df: pd.DataFrame):
    """Benchmark Parquet: Add indicators iteratively (requires manual tracking)."""
    print("\n█ Parquet Workflow: Iterative Feature Development")
    print("━" * 70)

    pq_path = Path('/tmp/strategy_dev.parquet')
    metadata_path = Path('/tmp/strategy_metadata.txt')

    # Metadata must be stored separately
    metadata = {}

    # Step 1: Write initial file (5 columns)
    start = time.perf_counter()
    df = base_df.copy()
    df.to_parquet(pq_path, index=False, compression='zstd')
    metadata.update({
        'Timestamp': 'Base OHLC data from Binance',
        'Open': 'Base OHLC data from Binance',
        'High': 'Base OHLC data from Binance',
        'Low': 'Base OHLC data from Binance',
        'Close': 'Base OHLC data from Binance'
    })
    step1_time = time.perf_counter() - start
    print(f"✓ Step 1: Write base file (5 cols)      {step1_time*1000:.1f} ms")

    # Step 2: Add RSI (read → compute → rewrite)
    start = time.perf_counter()
    df = pd.read_parquet(pq_path)
    df['rsi_14'] = calculate_rsi(df['Close'])
    df.to_parquet(pq_path, index=False, compression='zstd')
    metadata['rsi_14'] = '14-period RSI: oversold <30, overbought >70'
    step2_time = time.perf_counter() - start
    print(f"✓ Step 2: Add RSI (6 cols)              {step2_time*1000:.1f} ms")

    # Step 3: Add MACD (3 columns)
    start = time.perf_counter()
    df = pd.read_parquet(pq_path)
    macd_line, signal_line, histogram = calculate_macd(df['Close'])
    df['macd_line'] = macd_line
    df['macd_signal'] = signal_line
    df['macd_histogram'] = histogram
    df.to_parquet(pq_path, index=False, compression='zstd')
    metadata.update({
        'macd_line': 'MACD line: 12-EMA minus 26-EMA',
        'macd_signal': 'MACD signal: 9-EMA of MACD line',
        'macd_histogram': 'MACD histogram: MACD - Signal'
    })
    step3_time = time.perf_counter() - start
    print(f"✓ Step 3: Add MACD (9 cols)             {step3_time*1000:.1f} ms")

    # Step 4: Add Bollinger Bands (3 columns)
    start = time.perf_counter()
    df = pd.read_parquet(pq_path)
    upper, middle, lower = calculate_bollinger_bands(df['Close'])
    df['bb_upper'] = upper
    df['bb_middle'] = middle
    df['bb_lower'] = lower
    df.to_parquet(pq_path, index=False, compression='zstd')
    metadata.update({
        'bb_upper': 'Bollinger Band Upper: 20-MA + 2*StdDev',
        'bb_middle': 'Bollinger Band Middle: 20-period SMA',
        'bb_lower': 'Bollinger Band Lower: 20-MA - 2*StdDev'
    })
    step4_time = time.perf_counter() - start
    print(f"✓ Step 4: Add Bollinger Bands (12 cols) {step4_time*1000:.1f} ms")

    # Step 5: Add ATR
    start = time.perf_counter()
    df = pd.read_parquet(pq_path)
    df['atr_14'] = calculate_atr(df['High'], df['Low'], df['Close'])
    df.to_parquet(pq_path, index=False, compression='zstd')
    metadata['atr_14'] = '14-period ATR: volatility/risk measure'
    step5_time = time.perf_counter() - start
    print(f"✓ Step 5: Add ATR (13 cols)             {step5_time*1000:.1f} ms")

    # Step 6: Write metadata to separate file
    start = time.perf_counter()
    with open(metadata_path, 'w') as f:
        for col, desc in metadata.items():
            f.write(f"{col}: {desc}\n")
    step6_time = time.perf_counter() - start
    print(f"✓ Step 6: Write metadata file           {step6_time*1000:.1f} ms")

    total_time = step1_time + step2_time + step3_time + step4_time + step5_time + step6_time

    # Show final state
    final_size = pq_path.stat().st_size / 1024
    df_final = pd.read_parquet(pq_path)
    row_count = len(df_final)
    col_count = len(df_final.columns)

    metadata_path.unlink()

    print(f"\n  Total Time: {total_time*1000:.1f} ms")
    print(f"  Final Size: {final_size:.1f} KB ({row_count:,} rows × {col_count} cols)")
    print(f"  Metadata: ✗ Must store separately (external file)")

    return total_time, final_size


def main():
    print("╔═══════════════════════════════════════════════════════════════╗")
    print("║    Iterative Feature Development: DuckDB vs Parquet          ║")
    print("║                                                                ║")
    print("║  Real-world workflow: Adding indicators one at a time         ║")
    print("║  as your strategy evolves                                     ║")
    print("╚═══════════════════════════════════════════════════════════════╝\n")

    # Load base data
    print("Loading August 2024 dataset...")
    conn = duckdb.connect('/tmp/eurusd_1m_2024_08.duckdb', read_only=True)
    base_df = conn.execute("SELECT Timestamp, Open, High, Low, Close FROM ohlc").df()
    conn.close()
    print(f"Loaded {len(base_df):,} rows × {len(base_df.columns)} columns\n")

    # Run workflows
    duckdb_time, duckdb_size = benchmark_duckdb_workflow(base_df)
    parquet_time, parquet_size = benchmark_parquet_workflow(base_df)

    # Summary
    print("\n╔═══════════════════════════════════════════════════════════════╗")
    print("║                    FINAL COMPARISON                            ║")
    print("╚═══════════════════════════════════════════════════════════════╝\n")

    print(f"{'Metric':<30} {'DuckDB':<20} {'Parquet':<20}")
    print("─" * 70)
    print(f"{'Total Development Time':<30} {duckdb_time*1000:>6.1f} ms{'':<13} {parquet_time*1000:>6.1f} ms")
    print(f"{'Final File Size':<30} {duckdb_size:>6.1f} KB{'':<13} {parquet_size:>6.1f} KB")
    print(f"{'Metadata Storage':<30} {'Embedded (SQL)':<20} {'External file':<20}")
    print(f"{'Query Without Pandas':<30} {'✓ SQL queries':<20} {'✗ Must load':<20}")
    print(f"{'Transactional Safety':<30} {'✓ ACID':<20} {'✗ No rollback':<20}")
    print(f"{'Ecosystem Compatibility':<30} {'Limited':<20} {'Universal':<20}")

    # Recommendation
    print("\n" + "═" * 70)
    print("RECOMMENDATION FOR YOUR WORKFLOW:")
    print("═" * 70)

    print("\n✓ Use DuckDB because:")
    print("  • Embedded metadata (SQL COMMENT on each indicator)")
    print("  • Direct SQL queries without loading into pandas")
    print("  • Transactional safety (rollback if calculation fails)")
    print("  • Better for iterative development with historical context")
    print("  • Can store derivation logic as views/functions")

    print("\n✗ Avoid Parquet for active development:")
    print("  • No metadata storage (need separate file)")
    print("  • Must load entire dataset into pandas for any operation")
    print("  • No transactional safety")
    print("  • Better for archival/distribution, not active development")

    print("\n💡 Hybrid Approach:")
    print("  1. Develop in DuckDB (active work with metadata)")
    print("  2. Export to Parquet for archival/sharing")
    print("  3. Use: COPY strategy TO 'archive.parquet' (FORMAT PARQUET)")


if __name__ == '__main__':
    main()
