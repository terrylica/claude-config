"""
OKX REST API candles downloader.

This module downloads OHLC candles from OKX REST API and caches them in Parquet.
Simpler than CDN approach - no aggregation needed, just store candles for point-in-time lookups.

Key features:
- REST API with confirmed historical coverage (back to 2019+)
- 4H candles by default (efficient, covers rebalancing timestamps)
- Automatic pagination for large date ranges
- Rate limiting to avoid 429 errors
- No aggregation - store candles as-is
"""

import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional

import httpx
import pandas as pd

from .utils import normalize_symbol

logger = logging.getLogger(__name__)

# OKX API constants
OKX_API_BASE = "https://www.okx.com"
HISTORY_CANDLES_ENDPOINT = "/api/v5/market/history-candles"

# Rate limiting
DEFAULT_RATE_LIMIT = 10  # requests per second (conservative for public API)


class OKXCandlesDownloader:
    """
    Download OHLC candles from OKX REST API.

    This class handles:
    - API pagination (100 candles per request)
    - Rate limiting
    - Date range splitting
    - Error handling and retries

    Examples:
        >>> downloader = OKXCandlesDownloader(rate_limit=10)
        >>>
        >>> # Download 1 week of 4H candles for BTC
        >>> candles = downloader.download_candles(
        ...     symbol="BTC-USDT",
        ...     start_date="2023-10-01",
        ...     end_date="2023-10-07",
        ...     bar_size="4H"
        ... )
        >>> print(f"Downloaded {len(candles)} candles")
    """

    def __init__(
        self,
        rate_limit: float = DEFAULT_RATE_LIMIT,
        timeout: int = 30,
        max_retries: int = 3,
    ):
        """
        Initialize downloader.

        Args:
            rate_limit: Max requests per second
            timeout: HTTP timeout in seconds
            max_retries: Number of retries on failure
        """
        self.rate_limit = rate_limit
        self.timeout = timeout
        self.max_retries = max_retries
        self.last_request_time = 0.0

        # HTTP client (reuse connection pool)
        self.client = httpx.Client(
            base_url=OKX_API_BASE,
            timeout=timeout,
            headers={"User-Agent": "okx-price-provider/0.1.0"},
        )

        logger.info(
            f"Initialized OKXCandlesDownloader: rate_limit={rate_limit} req/s, "
            f"timeout={timeout}s"
        )

    def _rate_limit(self):
        """Enforce rate limiting."""
        if self.rate_limit <= 0:
            return

        min_interval = 1.0 / self.rate_limit
        elapsed = time.time() - self.last_request_time

        if elapsed < min_interval:
            sleep_time = min_interval - elapsed
            time.sleep(sleep_time)

        self.last_request_time = time.time()

    def _fetch_candles_page(
        self,
        inst_id: str,
        bar_size: str,
        after: Optional[str] = None,
        before: Optional[str] = None,
    ) -> dict:
        """
        Fetch single page of candles (max 100 candles).

        Args:
            inst_id: Instrument ID (e.g., "BTC-USDT")
            bar_size: Bar size (1m, 5m, 15m, 30m, 1H, 2H, 4H, 1D, etc.)
            after: Pagination - timestamp in ms (exclusive, for earlier data)
            before: Pagination - timestamp in ms (exclusive, for later data)

        Returns:
            API response dict with 'code', 'msg', 'data' fields
        """
        self._rate_limit()

        params = {
            "instId": inst_id,
            "bar": bar_size,
            "limit": "100",  # Max per request
        }

        if after:
            params["after"] = str(after)
        if before:
            params["before"] = str(before)

        for attempt in range(self.max_retries):
            try:
                response = self.client.get(HISTORY_CANDLES_ENDPOINT, params=params)
                response.raise_for_status()

                data = response.json()

                if data.get("code") != "0":
                    logger.warning(
                        f"API error for {inst_id}: code={data.get('code')}, "
                        f"msg={data.get('msg')}"
                    )

                return data

            except httpx.HTTPStatusError as e:
                logger.warning(
                    f"HTTP {e.response.status_code} for {inst_id} (attempt {attempt + 1}/{self.max_retries})"
                )
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    raise

            except Exception as e:
                logger.error(f"Unexpected error fetching {inst_id}: {e}")
                raise

    def download_candles(
        self,
        symbol: str,
        start_date: str,
        end_date: str,
        bar_size: str = "4H",
        market_type: str = "spot",
    ) -> pd.DataFrame:
        """
        Download candles for symbol over date range.

        Automatically handles pagination to fetch all candles in range.

        Args:
            symbol: Base symbol (e.g., "BTC", "ETH") - will be normalized
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            bar_size: Candle size (1m, 5m, 15m, 30m, 1H, 2H, 4H, 1D, 1W, 1M)
            market_type: "spot" or "swap"

        Returns:
            DataFrame with columns: timestamp, open, high, low, close, volume

        Examples:
            >>> downloader = OKXCandlesDownloader()
            >>> df = downloader.download_candles(
            ...     symbol="BTC",
            ...     start_date="2023-10-01",
            ...     end_date="2023-10-31",
            ...     bar_size="4H"
            ... )
        """
        # Normalize symbol (BTC â†’ BTC-USDT)
        inst_id = normalize_symbol(symbol, market_type=market_type)

        # Convert dates to timestamps (ms)
        start_ts = int(pd.Timestamp(start_date, tz="UTC").timestamp() * 1000)
        end_ts = int(pd.Timestamp(end_date, tz="UTC").timestamp() * 1000)

        logger.info(
            f"Downloading {inst_id} candles: {start_date} to {end_date}, bar={bar_size}"
        )

        all_candles = []
        after = None  # Pagination cursor
        page_count = 0

        # Paginate backwards from end_date
        while True:
            page_count += 1

            # Fetch page
            response = self._fetch_candles_page(
                inst_id=inst_id,
                bar_size=bar_size,
                after=after,
                before=str(end_ts) if after is None else None,  # Only use before on first page
            )

            if response.get("code") != "0":
                logger.error(f"API error: {response.get('msg')}")
                break

            data = response.get("data", [])

            if not data:
                logger.debug(f"No more data after page {page_count}")
                break

            # Parse candles
            for candle in data:
                ts_ms = int(candle[0])

                # Stop if we've gone before start_date
                if ts_ms < start_ts:
                    logger.debug(f"Reached start_date, stopping pagination")
                    break

                all_candles.append({
                    "timestamp": pd.Timestamp(ts_ms, unit="ms", tz="UTC"),
                    "open": float(candle[1]),
                    "high": float(candle[2]),
                    "low": float(candle[3]),
                    "close": float(candle[4]),
                    "volume": float(candle[5]),  # Volume in base currency
                    "volume_usd": float(candle[6]),  # Volume in USD
                })

            # Check if we should stop
            oldest_ts = int(data[-1][0])
            if oldest_ts < start_ts:
                break

            # Update pagination cursor (oldest timestamp in this page)
            after = str(oldest_ts)

            logger.debug(
                f"Page {page_count}: fetched {len(data)} candles, "
                f"oldest={pd.Timestamp(oldest_ts, unit='ms', tz='UTC')}"
            )

            # Safety limit
            if page_count > 1000:
                logger.warning(f"Hit pagination limit (1000 pages), stopping")
                break

        # Convert to DataFrame
        df = pd.DataFrame(all_candles)

        if df.empty:
            logger.warning(f"No candles found for {inst_id}")
            return df

        # Filter to exact date range and sort
        df = df[
            (df["timestamp"] >= pd.Timestamp(start_date, tz="UTC"))
            & (df["timestamp"] <= pd.Timestamp(end_date, tz="UTC"))
        ].sort_values("timestamp").reset_index(drop=True)

        # Add normalized symbol column (keep BTC-USDT format for consistency with API)
        df.insert(1, "symbol", inst_id)

        logger.info(
            f"Downloaded {len(df)} candles for {inst_id} "
            f"({page_count} API calls)"
        )

        return df

    def download_multiple_symbols(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        bar_size: str = "4H",
        market_type: str = "spot",
    ) -> pd.DataFrame:
        """
        Download candles for multiple symbols (sequential requests).

        Args:
            symbols: List of symbols (e.g., ["BTC", "ETH", "SOL"])
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
            bar_size: Candle size
            market_type: "spot" or "swap"

        Returns:
            Combined DataFrame with all symbols

        Examples:
            >>> downloader = OKXCandlesDownloader()
            >>> df = downloader.download_multiple_symbols(
            ...     symbols=["BTC", "ETH", "SOL"],
            ...     start_date="2023-10-01",
            ...     end_date="2023-10-07",
            ...     bar_size="4H"
            ... )
        """
        logger.info(
            f"Downloading {len(symbols)} symbols: {start_date} to {end_date}"
        )

        all_dfs = []
        success_count = 0
        failed_symbols = []

        for i, symbol in enumerate(symbols, 1):
            logger.info(f"[{i}/{len(symbols)}] Downloading {symbol}...")

            try:
                df = self.download_candles(
                    symbol=symbol,
                    start_date=start_date,
                    end_date=end_date,
                    bar_size=bar_size,
                    market_type=market_type,
                )

                if not df.empty:
                    all_dfs.append(df)
                    success_count += 1
                else:
                    logger.warning(f"No data for {symbol}")
                    failed_symbols.append(symbol)

            except Exception as e:
                logger.error(f"Failed to download {symbol}: {e}")
                failed_symbols.append(symbol)

        # Combine all DataFrames
        if all_dfs:
            combined = pd.concat(all_dfs, ignore_index=True)
            combined = combined.sort_values(["timestamp", "symbol"]).reset_index(drop=True)
        else:
            combined = pd.DataFrame()

        logger.info(
            f"Download complete: {success_count}/{len(symbols)} symbols, "
            f"{len(combined):,} total candles"
        )

        if failed_symbols:
            logger.warning(f"Failed symbols: {', '.join(failed_symbols)}")

        return combined

    def close(self):
        """Close HTTP client."""
        self.client.close()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


def download_okx_candles(
    symbols: List[str],
    start_date: str,
    end_date: str,
    output_parquet: Path,
    bar_size: str = "4H",
    market_type: str = "spot",
    rate_limit: float = DEFAULT_RATE_LIMIT,
) -> dict:
    """
    Convenience function to download candles and save to Parquet.

    Args:
        symbols: List of symbols to download
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        output_parquet: Path to save Parquet file
        bar_size: Candle size (default: 4H)
        market_type: "spot" or "swap"
        rate_limit: Max requests per second

    Returns:
        Dict with download statistics

    Examples:
        >>> from pathlib import Path
        >>> stats = download_okx_candles(
        ...     symbols=["BTC", "ETH", "SOL"],
        ...     start_date="2023-10-01",
        ...     end_date="2023-10-07",
        ...     output_parquet=Path("data/okx_price_cache/candles_4h.parquet"),
        ...     bar_size="4H"
        ... )
        >>> print(f"Downloaded {stats['num_candles']:,} candles")
    """
    with OKXCandlesDownloader(rate_limit=rate_limit) as downloader:
        # Download
        df = downloader.download_multiple_symbols(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            bar_size=bar_size,
            market_type=market_type,
        )

        if df.empty:
            logger.error("No data downloaded, cannot save Parquet")
            return {
                "symbols_downloaded": 0,
                "num_candles": 0,
                "cache_size_mb": 0.0,
            }

        # Save to Parquet
        output_parquet.parent.mkdir(parents=True, exist_ok=True)

        df.to_parquet(
            output_parquet,
            index=False,
            engine="pyarrow",
            compression="zstd",
            compression_level=9,
        )

        size_mb = output_parquet.stat().st_size / 1024 / 1024

        logger.info(
            f"Saved {len(df):,} candles to {output_parquet} "
            f"({size_mb:.2f} MB, zstd-9)"
        )

        return {
            "symbols_downloaded": df["symbol"].nunique(),
            "num_candles": len(df),
            "cache_size_mb": size_mb,
        }
