"""Platt scaling calibration for conformal prediction.

Implements post-hoc calibration via Platt scaling to resolve model
overconfidence and achieve conformal coverage guarantees.

Platt scaling fits a logistic regression per class on validation data:
    P(y=c|x) = σ(A_c · logit_c + B_c)

This is more flexible than temperature scaling (2 parameters per class vs
single global T), allowing for non-uniform miscalibration correction.

Reference:
    Platt "Probabilistic Outputs for Support Vector Machines" (1999)

Usage:
    uv run --active python -m research.ml_ood.calibrate_platt \
        --checkpoint research/ml_ood/experiments/run1/final_model.pt \
        --data output/solusdt_historical_2022_2025/SOLUSDT_rangebar_20220101_20250930_0.500pct.csv \
        --output research/ml_ood/experiments/run1 \
        --batch-size 256 \
        --device auto \
        --val-split 0.3
"""

import argparse
import json
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Subset

from research.ml_ood.data import RangeBarDataset, FixedPointConverter, create_splits
from research.ml_ood.models import OODRobustRangeBarModel
from research.ml_ood.training import ConformalCalibrator
from research.ml_ood.evaluation import OODMetrics


class PlattScaledModel(nn.Module):
    """Wrapper that applies Platt scaling to model logits.

    Platt scaling learns per-class parameters (A_c, B_c) that map logits
    to calibrated probabilities via sigmoid:
        P(y=c|x) = σ(A_c · logit_c + B_c)

    Args:
        model: Base model to wrap
        platt_params: Dictionary mapping class indices to (A, B) tuples
    """

    def __init__(self, model: nn.Module, platt_params: Dict[int, Tuple[float, float]]):
        super().__init__()
        self.model = model

        # Store Platt scaling parameters as tensors
        n_classes = len(platt_params)
        self.platt_A = nn.Parameter(
            torch.tensor([platt_params[i][0] for i in range(n_classes)]),
            requires_grad=False
        )
        self.platt_B = nn.Parameter(
            torch.tensor([platt_params[i][1] for i in range(n_classes)]),
            requires_grad=False
        )

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Forward pass with Platt-scaled probabilities.

        Args:
            x: Input tensor

        Returns:
            Dictionary with Platt-scaled direction_logits
        """
        outputs = self.model(x)
        logits = outputs['direction_logits']

        # Apply Platt scaling: σ(A · logit + B)
        # Shape: (batch, n_classes)
        scaled_logits = logits * self.platt_A + self.platt_B

        # Return scaled logits (will be passed through softmax elsewhere)
        outputs['direction_logits'] = scaled_logits

        return outputs


def fit_platt_scaling(
    model: nn.Module,
    val_loader: DataLoader,
    device: torch.device,
) -> Tuple[Dict[int, Tuple[float, float]], Dict[str, float]]:
    """Fit Platt scaling parameters on validation set.

    Uses sklearn.linear_model.LogisticRegression to fit per-class
    parameters (A_c, B_c) that map logits to calibrated probabilities.

    Args:
        model: Model to calibrate
        val_loader: Validation data loader
        device: Device to run on

    Returns:
        Tuple of (platt_params, fit_metrics)
        platt_params: Dict mapping class idx to (A, B) tuple
        fit_metrics: Dictionary of fitting metrics

    Raises:
        RuntimeError: If logistic regression fitting fails
    """
    print("\n" + "="*60)
    print("Platt Scaling Calibration")
    print("="*60)

    # Collect validation logits and targets
    print("\nCollecting validation predictions...")
    all_logits = []
    all_targets = []

    model.eval()
    with torch.no_grad():
        for batch in val_loader:
            sequences = batch["features"].to(device)
            targets = batch["target"]

            outputs = model(sequences)
            all_logits.append(outputs["direction_logits"].cpu())
            all_targets.append(targets)

    logits = torch.cat(all_logits).numpy()  # (n_samples, n_classes)
    targets = torch.cat(all_targets).numpy()  # (n_samples,)

    n_samples, n_classes = logits.shape
    print(f"Validation samples: {n_samples:,}")
    print(f"Number of classes: {n_classes}")

    # Compute baseline metrics
    baseline_probs = F.softmax(torch.from_numpy(logits), dim=1).numpy()
    baseline_ece = OODMetrics.compute_ece(baseline_probs, targets, n_bins=10)

    # Check class distribution
    unique_targets = np.unique(targets)
    print(f"Classes present in validation set: {unique_targets.tolist()}")

    # Fit Platt scaling per class using one-vs-rest
    print("\nFitting Platt scaling parameters (one-vs-rest)...")
    print("Method: sklearn.linear_model.LogisticRegression")

    platt_params = {}
    fit_info = {}

    for c in range(n_classes):
        # One-vs-rest binary labels
        binary_targets = (targets == c).astype(int)

        # Check if this class exists in the data
        n_positive = binary_targets.sum()
        n_negative = len(binary_targets) - n_positive

        if n_positive == 0:
            # Class not present in validation set - use identity scaling (no calibration)
            A, B = 1.0, 0.0
            platt_params[c] = (A, B)
            fit_info[f'class_{c}_A'] = A
            fit_info[f'class_{c}_B'] = B
            print(f"  Class {c}: SKIPPED (0 samples) - using identity scaling A=1.0, B=0.0")
            continue

        if n_negative == 0:
            raise RuntimeError(
                f"Class {c} calibration failed: all {n_samples} samples belong to class {c}. "
                f"This indicates data imbalance or incorrect class labeling."
            )

        # Fit logistic regression on this class's logits
        # sigmoid(A * logit + B)
        logits_c = logits[:, c].reshape(-1, 1)  # (n_samples, 1)

        # Use sklearn's LogisticRegression
        lr = LogisticRegression(
            penalty=None,  # No regularization (Platt scaling standard)
            solver='lbfgs',
            max_iter=1000,
        )

        lr.fit(logits_c, binary_targets)

        # Extract A and B parameters
        # sklearn parameterization: sigmoid(coef * x + intercept)
        A = lr.coef_[0, 0]
        B = lr.intercept_[0]

        platt_params[c] = (A, B)
        fit_info[f'class_{c}_A'] = A
        fit_info[f'class_{c}_B'] = B
        fit_info[f'class_{c}_samples'] = int(n_positive)

        print(f"  Class {c}: A={A:.4f}, B={B:.4f} (n={n_positive:,})")

    # Compute calibrated probabilities
    print("\nComputing calibrated probabilities...")
    calibrated_logits = np.zeros_like(logits)
    for c in range(n_classes):
        A, B = platt_params[c]
        calibrated_logits[:, c] = A * logits[:, c] + B

    calibrated_probs = F.softmax(torch.from_numpy(calibrated_logits), dim=1).numpy()
    calibrated_ece = OODMetrics.compute_ece(calibrated_probs, targets, n_bins=10)

    metrics = {
        'n_classes': n_classes,
        'n_samples': n_samples,
        'baseline_ece': baseline_ece,
        'calibrated_ece': calibrated_ece,
        'ece_improvement': baseline_ece - calibrated_ece,
        **fit_info,
    }

    print(f"\nPlatt Scaling Results:")
    print(f"  Baseline ECE:    {baseline_ece:.4f}")
    print(f"  Calibrated ECE:  {calibrated_ece:.4f}")
    print(f"  ECE Improvement: {metrics['ece_improvement']:.4f}")

    return platt_params, metrics


def evaluate_platt_scaling(
    base_model: nn.Module,
    platt_params: Dict[int, Tuple[float, float]],
    calib_loader: DataLoader,
    test_loader: DataLoader,
    device: torch.device,
) -> Dict[str, float]:
    """Evaluate conformal coverage with Platt scaling.

    Args:
        base_model: Base model (uncalibrated)
        platt_params: Platt scaling parameters per class
        calib_loader: Calibration loader for conformal quantile
        test_loader: Test loader for coverage evaluation
        device: Device to run on

    Returns:
        Dictionary of evaluation metrics

    Raises:
        RuntimeError: If coverage gap > 5% tolerance (SLO violation)
    """
    print("\n" + "="*60)
    print("Conformal Evaluation (Platt Scaling)")
    print("="*60)

    # Wrap model with Platt scaling
    scaled_model = PlattScaledModel(base_model, platt_params)
    scaled_model = scaled_model.to(device)
    scaled_model.eval()

    # Conformal calibration
    calibrator = ConformalCalibrator(alpha=0.1)

    print(f"\nCalibrating conformal predictor...")
    print(f"  Calibration samples: {len(calib_loader.dataset):,}")
    print(f"  Target coverage: 90.0%")

    quantile = calibrator.calibrate(scaled_model, calib_loader, device)

    print(f"  Computed quantile: {quantile:.4f}")

    # Evaluate coverage
    print(f"\nEvaluating coverage on test set...")
    print(f"  Test samples: {len(test_loader.dataset):,}")

    coverage_metrics = calibrator.evaluate_coverage(scaled_model, test_loader, device)

    empirical_coverage = coverage_metrics['coverage']
    target_coverage = 0.9
    coverage_gap = abs(empirical_coverage - target_coverage)

    print(f"\nResults:")
    print(f"  Empirical Coverage: {empirical_coverage*100:.1f}%")
    print(f"  Target Coverage:    {target_coverage*100:.1f}%")
    print(f"  Coverage Gap:       {coverage_gap*100:.1f}%")
    print(f"  Avg Set Size:       {coverage_metrics['avg_set_size']:.2f}")
    print(f"  Median Set Size:    {coverage_metrics['median_set_size']:.1f}")

    # SLO validation
    if coverage_gap > 0.05:
        raise RuntimeError(
            f"SLO violation: coverage gap {coverage_gap*100:.1f}% > 5% tolerance "
            f"even after Platt scaling"
        )

    print(f"\n✓ SLO: coverage gap {coverage_gap*100:.1f}% < 5% tolerance")

    # Add Platt scaling info to metrics
    coverage_metrics['calibration_method'] = 'platt_scaling'
    coverage_metrics['coverage_gap_pct'] = coverage_gap * 100

    return coverage_metrics


def main():
    parser = argparse.ArgumentParser(
        description="Platt scaling calibration for conformal prediction"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="Path to model checkpoint",
    )
    parser.add_argument(
        "--data",
        type=str,
        default="output/solusdt_historical_2022_2025/SOLUSDT_rangebar_20220101_20250930_0.500pct.csv",
        help="Path to range bar CSV",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output directory for calibrated model and results",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=256,
        help="Batch size",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="Device (auto/cpu/cuda/mps)",
    )
    parser.add_argument(
        "--val-split",
        type=float,
        default=0.3,
        help="Fraction of test data to use for Platt fitting",
    )

    args = parser.parse_args()

    # Device selection
    if args.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(args.device)

    print(f"Using device: {device}")

    # Load checkpoint
    print(f"\nLoading checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device, weights_only=False)

    if 'model_state_dict' not in checkpoint:
        raise RuntimeError("Checkpoint missing 'model_state_dict' key")

    if 'train_stats' not in checkpoint:
        raise RuntimeError("Checkpoint missing 'train_stats' key")

    # Load model
    print("Loading model...")
    base_model = OODRobustRangeBarModel()
    base_model.load_state_dict(checkpoint['model_state_dict'])
    base_model = base_model.to(device)
    base_model.eval()

    print(f"Model parameters: {sum(p.numel() for p in base_model.parameters()):,}")

    # Load data
    print(f"\nLoading data: {args.data}")
    df = FixedPointConverter.load_csv(Path(args.data))
    df = FixedPointConverter.compute_features(df)

    train_df, val_df, test_df = create_splits(df)

    print(f"Data splits:")
    print(f"  Train: {len(train_df)} bars")
    print(f"  Val:   {len(val_df)} bars")
    print(f"  Test:  {len(test_df)} bars")

    # Create dataset
    print("\nCreating dataset...")
    full_dataset = RangeBarDataset(
        csv_path=Path(args.data),
        sequence_len=64,
        detect_regimes=True,
    )

    # Apply training normalization
    full_dataset.normalize_features(checkpoint['train_stats'])

    # Collect all targets for stratified splitting
    print("\nCollecting targets for stratified split...")
    all_targets = []
    temp_loader = DataLoader(full_dataset, batch_size=args.batch_size, shuffle=False)
    for batch in temp_loader:
        all_targets.append(batch["target"])
    all_targets = torch.cat(all_targets).numpy()

    # Verify class distribution
    unique, counts = np.unique(all_targets, return_counts=True)
    print(f"Class distribution:")
    for cls, count in zip(unique, counts):
        print(f"  Class {cls}: {count:,} samples ({count/len(all_targets)*100:.1f}%)")

    # Check minimum samples per class for stratification
    min_samples_per_class = counts.min()
    if min_samples_per_class < 10:
        raise RuntimeError(
            f"Insufficient samples for stratified split: "
            f"class {unique[counts.argmin()]} has only {min_samples_per_class} samples. "
            f"Need at least 10 samples per class for reliable calibration."
        )

    # Stratified split: val (30%), calib (35%), test (35%)
    # First split: val vs rest
    all_indices = np.arange(len(full_dataset))
    val_indices, rest_indices = train_test_split(
        all_indices,
        test_size=1.0 - args.val_split,
        stratify=all_targets,
        random_state=42,
    )

    # Second split: calib vs test (50/50 split of remaining 70%)
    rest_targets = all_targets[rest_indices]
    calib_indices, test_indices = train_test_split(
        rest_indices,
        test_size=0.5,
        stratify=rest_targets,
        random_state=42,
    )

    print(f"\nStratified splits for calibration:")
    print(f"  Platt fitting:         {len(val_indices):,} samples")
    print(f"  Conformal calibration: {len(calib_indices):,} samples")
    print(f"  Coverage evaluation:   {len(test_indices):,} samples")

    # Verify stratification worked
    for split_name, indices in [("Platt", val_indices), ("Conformal", calib_indices), ("Test", test_indices)]:
        split_targets = all_targets[indices]
        split_unique, split_counts = np.unique(split_targets, return_counts=True)
        print(f"  {split_name} class distribution: {dict(zip(split_unique.tolist(), split_counts.tolist()))}")

    # Create data loaders
    val_dataset = Subset(full_dataset, val_indices)
    calib_dataset = Subset(full_dataset, calib_indices)
    test_dataset = Subset(full_dataset, test_indices)

    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    calib_loader = DataLoader(calib_dataset, batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

    # Step 1: Fit Platt scaling parameters
    platt_params, platt_metrics = fit_platt_scaling(
        base_model,
        val_loader,
        device,
    )

    # Step 2: Evaluate conformal coverage with Platt scaling
    coverage_metrics = evaluate_platt_scaling(
        base_model,
        platt_params,
        calib_loader,
        test_loader,
        device,
    )

    # Combine metrics
    all_results = {
        'platt_fitting': platt_metrics,
        'conformal_coverage': coverage_metrics,
        'platt_parameters': {
            f'class_{c}': {'A': float(A), 'B': float(B)}
            for c, (A, B) in platt_params.items()
        },
    }

    # Save results
    if args.output:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Save Platt-scaled model
        calibrated_checkpoint = {
            'model_state_dict': base_model.state_dict(),
            'train_stats': checkpoint['train_stats'],
            'config': checkpoint.get('config', {}),
            'calibration_method': 'platt_scaling',
            'platt_parameters': {
                f'class_{c}': {'A': float(A), 'B': float(B)}
                for c, (A, B) in platt_params.items()
            },
            'calibration_metrics': all_results,
        }

        model_file = output_dir / "final_model_platt_calibrated.pt"
        torch.save(calibrated_checkpoint, model_file)
        print(f"\n✓ Saved Platt-calibrated model: {model_file}")

        # Save metrics
        results_file = output_dir / "platt_calibration_results.json"
        with open(results_file, 'w') as f:
            # Convert numpy/torch types to python types
            def convert(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert(v) for v in obj]
                return obj

            json.dump(convert(all_results), f, indent=2)

        print(f"✓ Saved calibration results: {results_file}")

    # Summary
    print("\n" + "="*60)
    print("PLATT SCALING CALIBRATION COMPLETE")
    print("="*60)
    print(f"\nCalibration Method: Platt Scaling (per-class logistic regression)")
    print(f"ECE Improvement:    {platt_metrics['baseline_ece']:.4f} → {platt_metrics['calibrated_ece']:.4f}")
    print(f"Conformal Coverage: {coverage_metrics['coverage']*100:.1f}%")
    print(f"Coverage Gap:       {coverage_metrics['coverage_gap_pct']:.1f}%")
    print(f"\n✓ All SLOs met")


if __name__ == "__main__":
    main()
