"""Evaluation script for OOD-robust range bar model.

Usage:
    uv run --active python -m research.ml_ood.evaluate --checkpoint research/ml_ood/experiments/run1/final_model.pt
"""

import argparse
import json
from pathlib import Path
from typing import Dict

import torch
from torch.utils.data import DataLoader

from research.ml_ood.data import RangeBarDataset, FixedPointConverter, create_splits
from research.ml_ood.models import OODRobustRangeBarModel
from research.ml_ood.evaluation import OODMetrics, StressTestSuite
from research.ml_ood.training import ConformalCalibrator


def evaluate_test_set(
    model: torch.nn.Module,
    test_loader: DataLoader,
    device: torch.device,
) -> Dict[str, float]:
    """Evaluate model on test set.

    Args:
        model: Trained model
        test_loader: Test data loader
        device: Device to run on

    Returns:
        Dictionary of test metrics

    Raises:
        RuntimeError: If evaluation fails
    """
    print("\n" + "="*60)
    print("EVAL-001: Test Set Evaluation")
    print("="*60)

    metrics = OODMetrics.compute_all_metrics(
        model=model,
        loader=test_loader,
        device=device,
        regime_conditional=True,
    )

    # Print results
    print(f"\nTest Set Metrics:")
    print(f"  Accuracy: {metrics['accuracy']:.4f}")
    print(f"  Precision: {metrics['precision']:.4f}")
    print(f"  Recall: {metrics['recall']:.4f}")
    print(f"  F1: {metrics['f1']:.4f}")

    if 'ece' in metrics:
        print(f"  ECE: {metrics['ece']:.4f}")

    if 'regime_accuracy_std' in metrics:
        print(f"  Regime Accuracy Std: {metrics['regime_accuracy_std']:.4f}")
        if 'regime_accuracy_mean' in metrics:
            print(f"  Regime Accuracy Mean: {metrics['regime_accuracy_mean']:.4f}")

    print(f"\nAnomaly Detection:")
    print(f"  Mean Score: {metrics['anomaly_mean']:.6f}")
    print(f"  Std Score: {metrics['anomaly_std']:.6f}")

    # Validation against SLOs
    if metrics['accuracy'] <= 0.33:
        raise RuntimeError(f"SLO violation: accuracy {metrics['accuracy']:.4f} <= 0.33 (random baseline)")

    print("\n✓ SLO: accuracy > 0.33 (random baseline)")

    return metrics


def evaluate_conformal_calibration(
    model: torch.nn.Module,
    calibration_loader: DataLoader,
    test_loader: DataLoader,
    device: torch.device,
    alpha: float = 0.1,
) -> Dict[str, float]:
    """Evaluate conformal prediction calibration.

    Args:
        model: Trained model
        calibration_loader: Calibration data loader (for quantile estimation)
        test_loader: Test data loader (for coverage evaluation)
        device: Device to run on
        alpha: Miscoverage level (1-alpha = target coverage)

    Returns:
        Dictionary of conformal metrics

    Raises:
        RuntimeError: If coverage deviates >5% from target
    """
    print("\n" + "="*60)
    print("EVAL-002: Conformal Calibration")
    print("="*60)

    calibrator = ConformalCalibrator(alpha=alpha)

    print(f"\nCalibrating on {len(calibration_loader.dataset)} samples...")
    quantile = calibrator.calibrate(model, calibration_loader, device)

    print(f"Computed quantile: {quantile:.4f}")
    print(f"Target coverage: {(1-alpha)*100:.1f}%")

    # Evaluate coverage on test set
    print(f"\nEvaluating coverage on {len(test_loader.dataset)} test samples...")
    coverage_metrics = calibrator.evaluate_coverage(model, test_loader, device)

    empirical_coverage = coverage_metrics['coverage']
    coverage_gap = abs(empirical_coverage - (1 - alpha))

    print(f"\nResults:")
    print(f"  Empirical Coverage: {empirical_coverage*100:.1f}%")
    print(f"  Target Coverage: {(1-alpha)*100:.1f}%")
    print(f"  Coverage Gap: {coverage_gap*100:.1f}%")

    # SLO validation: coverage gap < 5%
    if coverage_gap > 0.05:
        raise RuntimeError(
            f"SLO violation: coverage gap {coverage_gap*100:.1f}% > 5% tolerance"
        )

    print("\n✓ SLO: coverage gap < 5%")

    # Regime-conditional coverage
    if 'regime_conditional_coverage' in coverage_metrics:
        print(f"\nRegime-Conditional Coverage:")
        for regime, coverage in coverage_metrics['regime_conditional_coverage'].items():
            print(f"  Regime {regime}: {coverage*100:.1f}%")

    return coverage_metrics


def evaluate_stress_testing(
    model: torch.nn.Module,
    dataset: RangeBarDataset,
    device: torch.device,
    batch_size: int = 256,
) -> Dict[str, Dict]:
    """Evaluate model on stress test periods.

    Args:
        model: Trained model
        dataset: Full dataset (for temporal filtering)
        device: Device to run on
        batch_size: Batch size for evaluation

    Returns:
        Dictionary mapping stress test name to metrics

    Raises:
        RuntimeError: If stress period not found in data
    """
    print("\n" + "="*60)
    print("EVAL-003: Stress Testing")
    print("="*60)

    stress_tester = StressTestSuite(model, dataset, device)

    results = {}

    # Terra/Luna crash
    print("\nTesting: Terra/Luna crash (2022-05-07 to 2022-05-12)")
    try:
        terra_metrics = stress_tester.run_stress_test(
            "terra_luna_crash",
            batch_size=batch_size,
        )
        results['terra_luna_crash'] = terra_metrics

        print(f"  Bars in period: {terra_metrics['n_samples']}")
        print(f"  Accuracy: {terra_metrics['accuracy']:.4f}")
        print(f"  Mean Anomaly Score: {terra_metrics['anomaly_mean']:.6f}")

    except ValueError as e:
        print(f"  ⚠️  Skipped: {e}")
        results['terra_luna_crash'] = {'error': str(e)}

    # FTX collapse
    print("\nTesting: FTX collapse (2022-11-06 to 2022-11-11)")
    try:
        ftx_metrics = stress_tester.run_stress_test(
            "ftx_collapse",
            batch_size=batch_size,
        )
        results['ftx_collapse'] = ftx_metrics

        print(f"  Bars in period: {ftx_metrics['n_samples']}")
        print(f"  Accuracy: {ftx_metrics['accuracy']:.4f}")
        print(f"  Mean Anomaly Score: {ftx_metrics['anomaly_mean']:.6f}")

    except ValueError as e:
        print(f"  ⚠️  Skipped: {e}")
        results['ftx_collapse'] = {'error': str(e)}

    # Validation: At least one stress test must succeed
    successful_tests = [k for k, v in results.items() if 'error' not in v]
    if not successful_tests:
        raise RuntimeError("SLO violation: No stress tests succeeded (data period issue)")

    print(f"\n✓ SLO: {len(successful_tests)}/{len(results)} stress tests completed")

    return results


def main():
    parser = argparse.ArgumentParser(description="Evaluate OOD-robust model")
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to model checkpoint")
    parser.add_argument("--data", type=str, default="output/solusdt_historical_2022_2025/SOLUSDT_rangebar_20220101_20250930_0.500pct.csv",
                        help="Path to range bar CSV")
    parser.add_argument("--output", type=str, default=None, help="Output directory for results")
    parser.add_argument("--batch-size", type=int, default=256, help="Batch size")
    parser.add_argument("--device", type=str, default="auto", help="Device (auto/cpu/cuda/mps)")
    parser.add_argument("--skip-conformal", action="store_true", help="Skip conformal calibration")
    parser.add_argument("--skip-stress", action="store_true", help="Skip stress testing")

    args = parser.parse_args()

    # Device selection
    if args.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(args.device)

    print(f"Using device: {device}")

    # Load checkpoint
    print(f"\nLoading checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device, weights_only=False)

    if 'model_state_dict' not in checkpoint:
        raise RuntimeError("Checkpoint missing 'model_state_dict' key")

    if 'train_stats' not in checkpoint:
        raise RuntimeError("Checkpoint missing 'train_stats' key")

    # Load model
    print("Loading model...")
    model = OODRobustRangeBarModel()
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Load data
    print(f"\nLoading data: {args.data}")
    df = FixedPointConverter.load_csv(Path(args.data))
    df = FixedPointConverter.compute_features(df)

    train_df, val_df, test_df = create_splits(df)

    print(f"Data splits:")
    print(f"  Train: {len(train_df)} bars")
    print(f"  Val: {len(val_df)} bars")
    print(f"  Test: {len(test_df)} bars")

    # Create datasets
    print("\nCreating datasets...")
    test_dataset = RangeBarDataset(
        csv_path=Path(args.data),
        sequence_len=64,
        detect_regimes=True,
    )

    # Apply training normalization
    test_dataset.normalize_features(checkpoint['train_stats'])

    # Create data loaders
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
    )

    # Run evaluations
    all_results = {}

    # 1. Test set evaluation
    test_metrics = evaluate_test_set(model, test_loader, device)
    all_results['test_metrics'] = test_metrics

    # 2. Conformal calibration
    if not args.skip_conformal:
        # Use first 50% of test set for calibration, last 50% for coverage evaluation
        test_size = len(test_dataset)
        calib_size = test_size // 2

        calib_indices = list(range(calib_size))
        eval_indices = list(range(calib_size, test_size))

        from torch.utils.data import Subset
        calib_dataset = Subset(test_dataset, calib_indices)
        eval_dataset = Subset(test_dataset, eval_indices)

        calib_loader = DataLoader(calib_dataset, batch_size=args.batch_size, shuffle=False)
        eval_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=False)

        conformal_metrics = evaluate_conformal_calibration(
            model, calib_loader, eval_loader, device
        )
        all_results['conformal_metrics'] = conformal_metrics

    # 3. Stress testing
    if not args.skip_stress:
        stress_results = evaluate_stress_testing(
            model, test_dataset, device, args.batch_size
        )
        all_results['stress_test_results'] = stress_results

    # Save results
    if args.output:
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)

        results_file = output_dir / "evaluation_results.json"
        with open(results_file, 'w') as f:
            # Convert numpy types to python types for JSON serialization
            def convert(obj):
                if hasattr(obj, 'item'):
                    return obj.item()
                elif isinstance(obj, dict):
                    return {k: convert(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert(v) for v in obj]
                return obj

            json.dump(convert(all_results), f, indent=2)

        print(f"\n✓ Results saved to: {results_file}")

    # Summary
    print("\n" + "="*60)
    print("EVALUATION COMPLETE")
    print("="*60)
    print(f"\nTest Accuracy: {test_metrics['accuracy']:.4f}")

    if 'conformal_metrics' in all_results:
        print(f"Conformal Coverage: {all_results['conformal_metrics']['coverage']*100:.1f}%")

    if 'stress_test_results' in all_results:
        stress_results = all_results['stress_test_results']
        successful = sum(1 for v in stress_results.values() if 'error' not in v)
        print(f"Stress Tests Passed: {successful}/{len(stress_results)}")

    print("\n✓ All SLOs met")


if __name__ == "__main__":
    main()
