"""
Parquet-backed cache for VWAP data with aggressive compression.

This module provides persistent storage for computed VWAP data using Parquet
with zstd-9 compression, achieving 25-50x compression ratio vs raw CSV.

Key features:
- zstd level 9 compression (maximum compression ratio)
- Dictionary encoding for symbol column
- Predicate pushdown for fast filtered queries
- Incremental updates (append + deduplicate)
- Cache statistics without loading full data
"""

import logging
from pathlib import Path
from typing import List, Optional

import pandas as pd
import pyarrow.parquet as pq

from .utils import format_bytes, estimate_compression_ratio

logger = logging.getLogger(__name__)


class ParquetVWAPCache:
    """
    Manage VWAP data in Parquet format with aggressive zstd compression.

    This class provides efficient storage and retrieval of VWAP data with:
    - 25-50x compression vs raw CSV
    - Fast filtered queries using predicate pushdown
    - Incremental updates with deduplication
    - Low memory footprint

    Attributes:
        cache_dir: Directory containing Parquet cache
        cache_file: Path to main Parquet file
        parquet_config: Compression configuration

    Examples:
        >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
        >>>
        >>> # Save VWAP data
        >>> cache.save(vwap_df)
        >>>
        >>> # Load with filters
        >>> prices = cache.load(
        ...     start_date=pd.Timestamp("2023-10-01", tz="UTC"),
        ...     symbols=["BTC", "ETH"]
        ... )
        >>>
        >>> # Get statistics
        >>> stats = cache.get_coverage_stats()
        >>> print(f"Cache size: {stats['file_size_mb']:.2f} MB")
    """

    def __init__(
        self,
        cache_dir: Path,
        cache_filename: str = "vwap_8h.parquet",
        compression_level: int = 9,
    ):
        """
        Initialize Parquet cache.

        Args:
            cache_dir: Directory to store cache files
            cache_filename: Name of Parquet file
            compression_level: zstd compression level (1-22, default: 9)
                - 1: Fastest, larger files
                - 9: Balanced (recommended)
                - 22: Maximum compression, slower
        """
        self.cache_dir = Path(cache_dir)
        self.cache_file = self.cache_dir / cache_filename
        self.compression_level = compression_level

        # Parquet write configuration with aggressive compression
        self.parquet_config = {
            "engine": "pyarrow",
            "compression": "zstd",
            "compression_level": compression_level,
            "use_dictionary": ["symbol"],  # Dictionary encode repeated symbols
            "row_group_size": 100_000,  # Balance compression vs query performance
            "version": "2.6",  # Use latest Parquet format
        }

        logger.info(
            f"Initialized ParquetVWAPCache: {self.cache_file}, "
            f"compression=zstd-{compression_level}"
        )

    def save(self, vwap_df: pd.DataFrame) -> None:
        """
        Save VWAP DataFrame to Parquet cache (overwrites existing).

        Args:
            vwap_df: DataFrame with columns: timestamp, symbol, vwap, volume, ...

        Raises:
            ValueError: If required columns are missing
        """
        required_cols = ["timestamp", "symbol", "vwap"]
        missing_cols = [col for col in required_cols if col not in vwap_df.columns]

        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")

        # Ensure cache directory exists
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Sort by timestamp and symbol for better compression
        vwap_df = vwap_df.sort_values(["timestamp", "symbol"]).reset_index(drop=True)

        # Write to Parquet
        vwap_df.to_parquet(self.cache_file, index=False, **self.parquet_config)

        size_mb = self.cache_file.stat().st_size / 1024 / 1024
        logger.info(
            f"Saved {len(vwap_df):,} rows to {self.cache_file} "
            f"(size: {size_mb:.2f} MB, compression: zstd-{self.compression_level})"
        )

    def load(
        self,
        start_date: Optional[pd.Timestamp] = None,
        end_date: Optional[pd.Timestamp] = None,
        symbols: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        """
        Load VWAP data with optional filters (uses Parquet predicate pushdown).

        Predicate pushdown means filtering happens during file read, not after,
        making filtered queries very fast even on large files.

        Args:
            start_date: Filter timestamp >= start_date
            end_date: Filter timestamp <= end_date
            symbols: List of symbols to include

        Returns:
            Filtered VWAP DataFrame

        Raises:
            FileNotFoundError: If cache file doesn't exist

        Examples:
            >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
            >>>
            >>> # Load all data
            >>> all_data = cache.load()
            >>>
            >>> # Load with date range
            >>> oct_data = cache.load(
            ...     start_date=pd.Timestamp("2023-10-01", tz="UTC"),
            ...     end_date=pd.Timestamp("2023-10-31", tz="UTC")
            ... )
            >>>
            >>> # Load specific symbols
            >>> btc_eth = cache.load(symbols=["BTC", "ETH"])
        """
        if not self.cache_file.exists():
            raise FileNotFoundError(f"Cache file not found: {self.cache_file}")

        # Build filter expressions for predicate pushdown
        filters = []

        if start_date:
            filters.append(("timestamp", ">=", start_date))
        if end_date:
            filters.append(("timestamp", "<=", end_date))
        if symbols:
            filters.append(("symbol", "in", symbols))

        # Read with filters (predicate pushdown for speed)
        if filters:
            df = pd.read_parquet(self.cache_file, filters=filters)
            logger.debug(
                f"Loaded {len(df):,} rows with filters: "
                f"start={start_date}, end={end_date}, symbols={symbols}"
            )
        else:
            df = pd.read_parquet(self.cache_file)
            logger.debug(f"Loaded {len(df):,} rows (no filters)")

        return df

    def append(self, new_vwap_df: pd.DataFrame) -> None:
        """
        Append new VWAP data to cache (deduplicates on timestamp+symbol).

        If cache exists, this method:
        1. Loads existing data
        2. Concatenates with new data
        3. Deduplicates (keeps latest value for each timestamp+symbol)
        4. Sorts and re-saves

        If cache doesn't exist, this simply saves the new data.

        Args:
            new_vwap_df: New VWAP data to append

        Examples:
            >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
            >>>
            >>> # First month
            >>> cache.save(october_vwap)
            >>>
            >>> # Add more months
            >>> cache.append(november_vwap)
            >>> cache.append(december_vwap)
        """
        if not new_vwap_df.empty and self.cache_file.exists():
            logger.info(f"Appending {len(new_vwap_df):,} rows to existing cache")

            # Load existing data
            existing = self.load()

            # Concatenate
            combined = pd.concat([existing, new_vwap_df], ignore_index=True)

            # Deduplicate (keep last occurrence for each timestamp+symbol)
            before_dedup = len(combined)
            combined = combined.drop_duplicates(subset=["timestamp", "symbol"], keep="last")
            after_dedup = len(combined)

            if before_dedup > after_dedup:
                logger.info(f"Deduplicated: {before_dedup:,} â†’ {after_dedup:,} rows")

            # Sort for better compression
            combined = combined.sort_values(["timestamp", "symbol"])

            # Save
            self.save(combined)

        else:
            # No existing cache or empty new data, just save
            self.save(new_vwap_df)

    def get_coverage_stats(self) -> dict:
        """
        Get cache statistics without loading full data (fast metadata query).

        Returns:
            Dict with statistics:
            - num_rows: Total number of rows
            - file_size_mb: File size in MB
            - file_size_formatted: Human-readable file size
            - num_row_groups: Number of Parquet row groups
            - compression_ratio_estimate: Estimated compression ratio vs uncompressed

        Raises:
            FileNotFoundError: If cache file doesn't exist

        Examples:
            >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
            >>> stats = cache.get_coverage_stats()
            >>> print(f"Rows: {stats['num_rows']:,}")
            >>> print(f"Size: {stats['file_size_formatted']}")
            >>> print(f"Compression: {stats['compression_ratio_estimate']:.1f}x")
        """
        if not self.cache_file.exists():
            raise FileNotFoundError(f"Cache file not found: {self.cache_file}")

        # Read Parquet metadata (fast, doesn't load data)
        metadata = pq.read_metadata(self.cache_file)

        # File size
        file_size_bytes = self.cache_file.stat().st_size
        file_size_mb = file_size_bytes / 1024 / 1024

        # Estimate uncompressed size from metadata
        uncompressed_size_estimate = 0
        for i in range(metadata.num_row_groups):
            rg_metadata = metadata.row_group(i)
            uncompressed_size_estimate += rg_metadata.total_byte_size

        compression_ratio = estimate_compression_ratio(
            uncompressed_size_estimate, file_size_bytes
        )

        return {
            "num_rows": metadata.num_rows,
            "file_size_mb": file_size_mb,
            "file_size_formatted": format_bytes(file_size_bytes),
            "num_row_groups": metadata.num_row_groups,
            "compression_ratio_estimate": compression_ratio,
            "compression_type": "zstd",
            "compression_level": self.compression_level,
        }

    def get_date_range(self) -> tuple:
        """
        Get min and max timestamps in cache (requires loading timestamp column).

        Returns:
            Tuple of (min_timestamp, max_timestamp)

        Examples:
            >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
            >>> start, end = cache.get_date_range()
            >>> print(f"Coverage: {start.date()} to {end.date()}")
        """
        if not self.cache_file.exists():
            raise FileNotFoundError(f"Cache file not found: {self.cache_file}")

        # Read only timestamp column (fast)
        timestamps = pd.read_parquet(self.cache_file, columns=["timestamp"])

        return timestamps["timestamp"].min(), timestamps["timestamp"].max()

    def get_symbol_list(self) -> List[str]:
        """
        Get list of unique symbols in cache (requires loading symbol column).

        Returns:
            Sorted list of symbols

        Examples:
            >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
            >>> symbols = cache.get_symbol_list()
            >>> print(f"Symbols: {', '.join(symbols[:10])}...")
        """
        if not self.cache_file.exists():
            raise FileNotFoundError(f"Cache file not found: {self.cache_file}")

        # Read only symbol column (fast with dictionary encoding)
        symbols = pd.read_parquet(self.cache_file, columns=["symbol"])

        return sorted(symbols["symbol"].unique().tolist())

    def clear(self) -> None:
        """
        Delete cache file.

        Examples:
            >>> cache = ParquetVWAPCache(cache_dir=Path("data/okx_price_cache"))
            >>> cache.clear()
        """
        if self.cache_file.exists():
            size_mb = self.cache_file.stat().st_size / 1024 / 1024
            self.cache_file.unlink()
            logger.info(f"Cleared cache: {self.cache_file} ({size_mb:.2f} MB)")
        else:
            logger.warning(f"Cache file doesn't exist: {self.cache_file}")
