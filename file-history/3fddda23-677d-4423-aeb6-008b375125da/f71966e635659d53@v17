# CLAUDE.md

## Execution Pattern

**Always run Python as module via Docker:**
```bash
docker exec ml-dev python -m <module_path>
```

## Docker Container

- **Runtime**: Colima (pure CLI, no GUI) - `colima start`
- **Container name**: `ml-dev`
- **Working directory**: `/workspace`
- **Install command**: `docker exec ml-dev pip install -e '.[dev]'`

## Project Context

- **Dockerfile fix**: `ENV CONDA_PLUGINS_AUTO_ACCEPT_TOS=true` resolves Anaconda ToS requirement (July 2025)
- **Templates** (output format for validated features):
  - Templates define the FeatureSet structure for validation
  - Use IPSS+VIF pipeline to select features → then conform to template format
  - Examples: `ohlcv_comprehensive_sizex_v5.py`, `ohlcv_fluid-dynamics_sizex_v1.py`

## PR Policy (ZERO-TRUST WHITELIST)

**Philosophy:** BLOCK EVERYTHING until explicitly approved

**Automated enforcement:** `.github/workflows/enforce-production-only.yml`

**Current whitelist:** EMPTY (nothing allowed)

**Workflow:**
1. Create PR with code changes
2. PR will FAIL (expected - whitelist is empty)
3. Review failed PR to see blocked files
4. If approved, manually add pattern to whitelist in workflow file
5. Commit whitelist update to main first
6. Rebase/update PR - it will now pass

**Example whitelist patterns:**
```bash
'^ml_feature_set/bundled/.*\.py$'     # Python files in bundled/
'^pyproject\.toml$'                    # Project config
'^README\.md$'                         # Root README only
```

**Whitelist location:** Line 45 in `.github/workflows/enforce-production-only.yml`

**Policy:** Default deny - no files merge to main unless pattern is explicitly added to whitelist

## Sample Data

- **Binance timestamps**: Always use `datetime.fromtimestamp(ts/1000, tz=timezone.utc)` (naive conversion creates fake DST duplicates)

## Key Architecture Points

- **`actual_ready_time`**: Framework-generated (not in CSVs), simulates data availability delay
- **`resample_factors`**: Multi-timeframe (different intervals via OHLCV aggregation), NOT multi-period (different lookbacks)

## Feature Construction Patterns (Off-the-Shelf)

**Pandas methods:**
- `.ewm()` - Exponentially weighted (MACD, adaptive indicators)
- `.expanding()` - Walk-forward cumulative features
- `.rank(pct=True)` - Percentile normalization (0-1)
- `.pipe()` - Method chaining for pipelines
- `.groupby().transform()` - Group stats → original rows
- `.interpolate(method='time')` - Time-aware missing data
- `.resample().interpolate()` - Upsample with interpolation
- Named aggs: `agg(vol_mean=('volume', 'mean'))`

**sklearn transformers (OOD-robust):**
- `PolynomialFeatures(interaction_only=True)` - Feature crosses (RSI×Volume)
- `QuantileTransformer(output_distribution='normal')` - Outliers → Gaussian (robust)
- `RobustScaler()` - IQR-based scaling (outlier-resistant)

**Robust statistics:**
- `scipy.stats.median_abs_deviation()` - MAD (more robust than std)
- `scipy.stats.mstats.winsorize()` - Cap extremes (vs trimming)
- Downside deviation - Semi-variance for risk metrics

**Normalization principles:**
- `np.arctan()` - Smooth squashing (unbounded → bounded, vs hard clip)
- Multi-period families - Sweep all periods (5,10,15,20,25,30), let model choose vs expert
- Second-order features - Derivatives of moving averages (acceleration = Δ(MA))

**Higher-order derivatives (velocity/acceleration/jerk):**
- `scipy.signal.savgol_filter(deriv=1/2/3)` - Industry standard (Savitzky-Golay smoothing + differentiation)
- `derivative` package (2024, experimental) - Total Variation Regularization for extremely noisy data
- Note: 3rd+ order rarely used in production (noise-sensitive)

**Signal decomposition (production):**
- `pywt.wavedec()` - Wavelet decomposition (trend/detail separation)
- `statsmodels.tsa.seasonal.STL` - Seasonal/trend/residual split

**Information theory (production):**
- `sklearn.feature_selection.mutual_info_regression()` - Mutual information (non-linear dependence)

**Regime detection (feature generation only, not for selection validation):**
- `hmmlearn.GaussianHMM` - Hidden Markov Models (unsupervised regime features)

**Spectral & cross-series analysis:**
- `scipy.signal.coherence()` - Frequency coherence between series
- `scipy.signal.csd()` - Cross-spectral density (phase relationships)
- `scipy.fft` - Frequency domain features

**Financial risk metrics:**
- Maximum drawdown - Steepest peak-to-trough decline
- Sortino ratio - Downside deviation (vs total volatility in Sharpe)
- Calmar ratio - Return / max drawdown

**Advanced aggregations:**
- `statsmodels.regression.rolling.RollingOLS` - Rolling regression (beta over time)
- `scipy.signal.fftconvolve()` - Fast convolution (moving averages)
- `scipy.signal.find_peaks()` - Peak detection

**Realized measures (Andersen-Bollerslev framework):**
- Realized volatility - Sum of squared intraperiod returns (5-min benchmark)
- Bipower variation - Barndorff-Nielsen & Shephard (separates jumps from continuous volatility)
- Jump detection - Realized variance minus bipower variation
- Range-based volatility:
  - Parkinson (5x more efficient than close-to-close)
  - Garman-Klass (7.4x more efficient, assumes no jumps)
  - Yang-Zhang (14x more efficient, handles opening jumps & drift)

**Microstructure proxies (OHLCV-compatible):**
- Amihud ILLIQ - `|return|/dollar_volume` (illiquidity measure)
- High-low Amihud - `(high-low)/volume` (range-based illiquidity)
- VPIN estimation - Volume-synchronized informed trading (classify buy/sell from price changes)
- Kyle's lambda proxy - Market impact/adverse selection
- Order flow imbalance - Tick rule (price > prev = buy, < prev = sell)

**Fractal analysis:**
- Higuchi fractal dimension - Complexity of time series trajectory
- Box-counting dimension - Fractal structure of price patterns

**Math utilities:**
- `np.einsum('ij,ik->jk')` - Efficient covariance matrices
- Broadcasting - Pairwise operations without loops

---

### Research/Experimental (not in minimal production stack)

**Complexity measures:**
- `ordpy` - Permutation entropy (order-based)
- `antropy` - Sample/approximate/multiscale entropy
- `nolds.dfa()` - Detrended Fluctuation Analysis (Hurst parameter)
- `PyRQA` - Recurrence Quantification Analysis

**Causal discovery:**
- `tigramite` - Transfer entropy, Granger causality

**Advanced decomposition:**
- `PyEMD` - Empirical Mode Decomposition (from "Signal decomposition" section)

**Nonlinear dependence:**
- `pyvinecopulib`/`pycop` - Vine copulas, tail dependence

**Relational:**
- `getML` - Automated cross-table aggregations (requires license)

**Fast alternatives:**
- `infomeasure` - Entropy/MI (10x faster than scipy)

---

### Methods to Avoid (Blacklist)

**Multi-environment knockoffs:**
- ❌ Avoid unless environments are objectively defined (e.g., BTC/ETH/SPY, pre/post regulatory)
- Reason: Subjective "bull/bear/sideways" regimes risk circularity (using outcome to validate features)

**Universal HSIC kernel scans:**
- ❌ Avoid O(n²) pairwise checks across all features
- Reason: Most OHLCV transforms are near-linear; costly with marginal benefit over stability selection + VIF
- Use sparingly: Only targeted checks on high-Pearson pairs if needed

**Performance reference:** See `.claude/performance-blacklist.md` (not priority)

**OOD robustness principles (2025 research):**
- **Invariant features** - Extract features stable across distribution shifts (e.g., permutation entropy, DFA)
- **Decoupling** - Separate task-relevant from task-irrelevant features (e.g., EMD separates trend/noise)
- **Information-theoretic** - Mutual information, transfer entropy resist spurious correlations
- **Robust estimators** - MAD, winsorization, quantile transforms handle outliers without overfitting
- **Causal structures** - Focus on causal relationships (tigramite) vs correlations for regime shifts

**Excluded:** tsfresh, catch22/featuretools (automated but not OOD-robust enough)

## Orthogonality Assessment (Irreducible Essentials — 2025)

### Default, production-oriented approach (OHLCV-only)
- **Two-stage pipeline (simple, OOD-practical):**
  1) **IPSS** (*stability selection via block/stationary bootstrap — `arch`*) → keep features with **selection frequency ≥ 0.70**.  
     - Defaults: `B=100–200`, block size ≈ 64 bars (TF-appropriate), **purged/embargoed TS-CV** (*`scikit-learn`* `TimeSeriesSplit`) for model tuning.  
     - Selector: **`Lasso (L1)`** (*`scikit-learn`*); **`RandomForest`** is a drop-in alternative (importance-based).
  2) **VIF prune** (*linear redundancy — `statsmodels`*) → iteratively drop features with **VIF ≥ 5** (stop when all < 5).
- **Controls:**  
  - **Count**: pre-cap candidates `atoms_k` (e.g., 60); hard-cap `final_k` (e.g., 15–20) by highest stability freq post-VIF.  
  - **Multi-timeframes**: explicit list (e.g., `1m,5m,15m,1h`) with safe OHLCV aggregation (*`pandas`*): **O=first, H=max, L=min, C=last, V=sum**.  
  - **Lookbacks**: per-TF **min–max bars** fences (e.g., `1m:5–240`, `5m:3–96`). No feature may exceed bounds.

### Optional, use sparingly
- **Tree-guided atoms** (fast, shallow, subsampled trees) to propose diverse, low-redundancy candidates **before IPSS+VIF**.  
  Works with **`sklearn` HGBT** by default; if present, optionally use **`lightgbm` / `xgboost` / `catboost`** (fail-soft, degrade to HGBT).
- **Graphical Lasso** (*`sklearn.covariance.GraphicalLasso(CV)`*) for conditional de-duplication on survivors; run *before* VIF if dimensionality remains high.
- **Targeted nonlinearity check**: **distance correlation** (*`dcor`*) or **HSIC** (*`hyppo`*) **only on flagged pairs** (e.g., high Pearson) to avoid O(n²) cost.

### What not to overuse (finance context)
- **Multi-environment knockoffs**: **avoid** unless environments are **objectively defined** (cross-asset BTC/ETH/SPY; pre/post regulatory). Subjective “bull/bear/sideways” risks circularity.
- **Heavy universal kernel scans (HSIC everywhere)**: costly; most OHLCV transforms are near-linear—prefer **stability + VIF** and targeted checks.

### OOD-robustness principles we keep
- **Perturbation stability** (bootstrap) > hand-labeled regimes.  
- **No leakage** (purged/embargoed CV).  
- **Parsimony** (L1 / shallow trees) + **de-duplication** (Graphical Lasso optional, **VIF mandatory**).  
- **Explicit fences** (TF list, per-TF lookback min–max).

### Minimal implementation stack (well-maintained)
- **Core:** `numpy`, `pandas`, `pyarrow` (Parquet), `scikit-learn`, `statsmodels` (VIF), `arch` (Stationary/Block Bootstrap), `tqdm`, `pydantic`, `typer` (CLI), `pytest` (tests)  
- **Optional:** `lightgbm`, `xgboost`, `catboost`, `dcor`, `hyppo`  
- **Dev quality:** `ruff`, `black`  
- **Env probe (tmp, before repo changes):** `uv`, `uvx`

### CLI & Docker (run as module)
- Run inside Docker as:
  `python -m ml_feature_set.cli churn --data ... --target ... --tfs 1m,5m --win 1m:5-240 ... --atoms-k 60 --stability-B 100 --stability-thr 0.70 --vif 5.0 --final-k 20`
- Always **probe in /tmp** with `uv/uvx` (install & import-smoke core + optional libs) **before** touching repo env; degrade gracefully to `sklearn`-only path.

### Report, always
- Input feature count; survivors at **freq ≥ 0.70**; post-VIF count; **final_k**; thresholds used.  
- Per-feature selection frequency (0–1); removals with reasons (**VIF** or targeted nonlinearity).


**Infrastructure:**
- Anaconda ToS: https://stackoverflow.com/questions/79702788/
