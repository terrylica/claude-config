---
# Feature Pruning Manifest
# Machine-readable exclusion list with keep_instead mappings
# Source: experiments/orthogonality_filtering_20251003
# Date: 2025-10-03

pruned_features:
  # Tier 1: Perfect Correlation (r = 1.0)
  hour_of_day_sin:
    keep_instead: fourier_daily_sin_1
    reason: Perfect correlation - mathematical duplicate
    correlation: 1.0
    tier: 1
  hour_of_day_cos:
    keep_instead: fourier_daily_cos_1
    reason: Perfect correlation - mathematical duplicate
    correlation: 1.0
    tier: 1
  day_of_week_sin:
    keep_instead: fourier_weekly_sin_1
    reason: Perfect correlation - mathematical duplicate
    correlation: 1.0
    tier: 1
  day_of_week_cos:
    keep_instead: fourier_weekly_cos_1
    reason: Perfect correlation - mathematical duplicate
    correlation: 1.0
    tier: 1

  # Tier 2: STL Trend Redundancy (r > 0.99)
  stl_trend_s7_t21:
    keep_instead: [close_lag_1, close_lag_3, close_lag_5, stl_seasonal_s7_t21, stl_resid_s7_t21]
    reason: Trend captured by raw price lags, keep orthogonal seasonal/residual
    correlation: 0.995
    tier: 2
  stl_trend_s13_t31:
    keep_instead: [close_lag_5, close_lag_10, stl_seasonal_s13_t31, stl_resid_s13_t31]
    reason: Trend captured by raw price lags, keep orthogonal seasonal/residual
    correlation: 0.995
    tier: 2

  # Tier 2: Rolling Mean Redundancy (r > 0.99)
  rolling_mean_5:
    keep_instead: [close_lag_1, close_lag_2, close_lag_3]
    reason: Redundant with close lags, lags are simpler/more interpretable
    correlation: 0.990
    tier: 2
  rolling_mean_10:
    keep_instead: [close_lag_3, close_lag_5, close_lag_10]
    reason: Redundant with close lags, lags are simpler/more interpretable
    correlation: 0.992
    tier: 2
  rolling_mean_20:
    keep_instead: [close_lag_5, close_lag_10, close_lag_20]
    reason: Redundant with close lags, lags are simpler/more interpretable
    correlation: 0.994
    tier: 2
  rolling_mean_50:
    keep_instead: [close_lag_10, close_lag_20]
    reason: Redundant with close lags, lags are simpler/more interpretable
    correlation: 0.991
    tier: 2
  rolling_mean_100:
    keep_instead: [close_lag_20, expanding_mean]
    reason: Redundant with close lags and expanding mean
    correlation: 0.984
    tier: 2

  # Tier 2: EWM Mean Redundancy (r > 0.99)
  ewm_mean_5:
    keep_instead: [close_lag_1, close_lag_2]
    reason: Redundant with close lags, lags are simpler (no smoothing parameter)
    correlation: 0.991
    tier: 2
  ewm_mean_10:
    keep_instead: [close_lag_3, close_lag_5]
    reason: Redundant with close lags, lags are simpler (no smoothing parameter)
    correlation: 0.993
    tier: 2
  ewm_mean_20:
    keep_instead: [close_lag_5, close_lag_10]
    reason: Redundant with close lags, lags are simpler (no smoothing parameter)
    correlation: 0.994
    tier: 2
  ewm_mean_50:
    keep_instead: [close_lag_10, close_lag_20]
    reason: Redundant with close lags, lags are simpler (no smoothing parameter)
    correlation: 0.993
    tier: 2

  # Tier 3: Close Lag Redundancy
  close_lag_2:
    keep_instead: [close_lag_1, close_lag_3]
    reason: Redundant with neighboring lags
    correlation: 0.983
    tier: 3

  # Tier 3: Rolling Min/Max Redundancy
  rolling_min_20:
    keep_instead: expanding_min
    reason: Expanding is more general, captures all-time minimum
    correlation: 0.986
    tier: 3
  rolling_min_50:
    keep_instead: expanding_min
    reason: Expanding is more general, captures all-time minimum
    correlation: 0.976
    tier: 3
  rolling_min_100:
    keep_instead: expanding_min
    reason: Expanding is more general, captures all-time minimum
    correlation: 0.952
    tier: 3
  rolling_max_20:
    keep_instead: rolling_max_100
    reason: Keep longer window only for max
    correlation: 0.985
    tier: 3
  rolling_max_50:
    keep_instead: rolling_max_100
    reason: Keep longer window only for max
    correlation: 0.975
    tier: 3

  # Tier 3: Volatility Redundancy (rolling_std)
  rolling_std_5:
    keep_instead: rolling_std_20
    reason: High correlation within rolling_std family
    correlation: 0.935
    tier: 3
  rolling_std_10:
    keep_instead: rolling_std_20
    reason: High correlation within rolling_std family
    correlation: 0.937
    tier: 3
  rolling_std_50:
    keep_instead: rolling_std_20
    reason: High correlation within rolling_std family
    correlation: 0.936
    tier: 3
  rolling_std_100:
    keep_instead: rolling_std_20
    reason: High correlation within rolling_std family
    correlation: 0.921
    tier: 3

  # Tier 3: Volatility Redundancy (ewm_std - all eliminated)
  ewm_std_5:
    keep_instead: rolling_std_20
    reason: High correlation across volatility measures
    correlation: 0.926
    tier: 3
  ewm_std_10:
    keep_instead: rolling_std_20
    reason: High correlation across volatility measures
    correlation: 0.922
    tier: 3
  ewm_std_20:
    keep_instead: rolling_std_20
    reason: High correlation across volatility measures
    correlation: 0.866
    tier: 3
  ewm_std_50:
    keep_instead: rolling_std_20
    reason: High correlation across volatility measures
    correlation: 0.883
    tier: 3
  expanding_std:
    keep_instead: rolling_std_20
    reason: High correlation with rolling_std
    correlation: 0.983
    tier: 3

  # Tier 3: Volatility Redundancy (realized_vol, parkinson_vol)
  realized_vol_10:
    keep_instead: realized_vol_50
    reason: Keep longer window for stability
    correlation: 0.880
    tier: 3
  realized_vol_20:
    keep_instead: realized_vol_50
    reason: Keep longer window for stability
    correlation: 0.828
    tier: 3
  parkinson_vol_10:
    keep_instead: [rolling_std_20, realized_vol_50]
    reason: High correlation with other volatility measures
    correlation: 0.847
    tier: 3
  parkinson_vol_20:
    keep_instead: [rolling_std_20, realized_vol_50]
    reason: High correlation with other volatility measures
    correlation: 0.874
    tier: 3

  # Tier 3: Price Position Redundancy
  pct_from_ma_10:
    keep_instead: z_score_50
    reason: z_score is normalized and more stable
    correlation: 0.817
    tier: 3
  pct_from_ma_20:
    keep_instead: z_score_50
    reason: z_score is normalized and more stable
    correlation: 0.868
    tier: 3
  pct_from_ma_50:
    keep_instead: z_score_50
    reason: z_score is normalized and more stable
    correlation: 0.816
    tier: 3
  z_score_10:
    keep_instead: z_score_50
    reason: Keep longer window for stability
    correlation: 0.861
    tier: 3
  z_score_20:
    keep_instead: z_score_50
    reason: Keep longer window for stability
    correlation: 0.805
    tier: 3

kept_features:
  # Returns (7 features)
  - returns
  - returns_lag_1
  - returns_lag_2
  - returns_lag_3
  - returns_lag_5
  - returns_lag_10
  - returns_lag_20

  # Fourier (10 features)
  - fourier_daily_sin_1
  - fourier_daily_cos_1
  - fourier_daily_sin_2
  - fourier_daily_cos_2
  - fourier_daily_sin_3
  - fourier_daily_cos_3
  - fourier_weekly_sin_1
  - fourier_weekly_cos_1
  - fourier_weekly_sin_2
  - fourier_weekly_cos_2

  # Calendar (2 features)
  - is_month_end
  - is_quarter_end

  # Close Lags (5 features)
  - close_lag_1
  - close_lag_3
  - close_lag_5
  - close_lag_10
  - close_lag_20

  # Volume Lags (5 features)
  - volume_lag_1
  - volume_lag_2
  - volume_lag_3
  - volume_lag_5
  - volume_lag_10

  # STL (4 features - seasonal + residual only)
  - stl_seasonal_s7_t21
  - stl_resid_s7_t21
  - stl_seasonal_s13_t31
  - stl_resid_s13_t31

  # Rolling Stats (6 features)
  - rolling_std_20
  - rolling_max_100
  - rolling_skew_20
  - rolling_kurt_20
  - rolling_skew_50
  - rolling_kurt_50

  # Expanding (2 features)
  - expanding_min
  - expanding_max

  # Calendar (additional - should be in calendar section above)
  - is_month_start
  - is_year_end

  # Volatility (1 feature)
  - realized_vol_50

  # Price Position (1 feature)
  - z_score_50

  # Autocorrelation (1 feature)
  - returns_acf_lag_1

metadata:
  source_experiment: experiments/orthogonality_filtering_20251003
  date_decided: 2025-10-03
  baseline_feature_count: 85
  final_feature_count: 48
  reduction_pct: 44
  method: Correlation-based filtering with threshold 0.90
  data_source: SOL 5-min (9,901 samples)
  validation: Survived correlation threshold |r| <= 0.90
---

# Feature Pruning Manifest

**Purpose:** Permanent exclusion list of redundant features with superior alternatives

**Status:** Active - Enforce in all feature computation pipelines

**Usage:**
- Check this list before adding features to models
- Parse YAML frontmatter programmatically for automated validation
- Update when new pruning decisions made (version control changes)

**Validation:** Based on correlation analysis (experiments/orthogonality_filtering_20251003)

---

## Pruning Decision Framework

### Tier 1: MUST PRUNE (Perfect Correlation, r = 1.0)

**Zero information loss - mathematical duplicates**

These features have perfect correlation (r = 1.0) with their replacements. Pruning is **mandatory**.

#### Calendar Sine/Cosine → Fourier Harmonics

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `hour_of_day_sin` | `fourier_daily_sin_1` | Fourier framework supports multiple harmonics, identical information |
| `hour_of_day_cos` | `fourier_daily_cos_1` | Same mathematical content, Fourier is more extensible |
| `day_of_week_sin` | `fourier_weekly_sin_1` | Fourier framework supports multiple harmonics |
| `day_of_week_cos` | `fourier_weekly_cos_1` | Same mathematical content, Fourier is more extensible |

**Impact**: 4 features pruned, 0% information loss

**Rationale**: Fourier representation is mathematically identical but more general (supports harmonic series). No reason to keep both.

---

### Tier 2: STRONG PRUNE (Near-Perfect Correlation, r > 0.99)

**Minimal information loss - essentially duplicates**

These features have r > 0.99 correlation. Pruning is **strongly recommended**.

#### STL Trend Components (r = 0.995)

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `stl_trend_s7_t21` | `close_lag_*` + `stl_seasonal_s7_t21` + `stl_resid_s7_t21` | Trend captured by raw lags, keep orthogonal seasonal/residual |
| `stl_trend_s13_t31` | `close_lag_*` + `stl_seasonal_s13_t31` + `stl_resid_s13_t31` | Trend captured by raw lags, keep orthogonal seasonal/residual |

**Impact**: 2 features pruned, <1% information loss

**Rationale**:
- STL trend is smoothed price estimate (r=0.995 with close lags)
- Raw price lags are simpler and capture same information
- STL seasonal and residual are orthogonal components → **keep these**

#### Rolling Mean Redundancy (r = 0.99+)

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `rolling_mean_5` | `close_lag_1`, `close_lag_2`, `close_lag_3` | r=0.990, lags simpler/more interpretable |
| `rolling_mean_10` | `close_lag_3`, `close_lag_5`, `close_lag_10` | r=0.992, lags simpler/more interpretable |
| `rolling_mean_20` | `close_lag_5`, `close_lag_10`, `close_lag_20` | r=0.994, lags simpler/more interpretable |
| `rolling_mean_50` | `close_lag_10`, `close_lag_20` | r=0.991, lags simpler/more interpretable |
| `rolling_mean_100` | `close_lag_20`, `expanding_mean` | r=0.984, lags simpler/more interpretable |

**Impact**: 5 features pruned, <1% information loss

**Rationale**:
- Rolling means are smoothed versions of close lags
- Close lags are simpler (no window parameter, direct T-N lookback)
- Close lags have lower mutual correlation (0.98 vs 0.99+)

#### EWM Mean Redundancy (r = 0.99+)

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `ewm_mean_5` | `close_lag_1`, `close_lag_2` | r=0.991, lags simpler (no span parameter) |
| `ewm_mean_10` | `close_lag_3`, `close_lag_5` | r=0.993, lags simpler (no span parameter) |
| `ewm_mean_20` | `close_lag_5`, `close_lag_10` | r=0.994, lags simpler (no span parameter) |
| `ewm_mean_50` | `close_lag_10`, `close_lag_20` | r=0.993, lags simpler (no span parameter) |

**Impact**: 4 features pruned, <1% information loss

**Rationale**:
- EWM is exponentially weighted moving average (causal, efficient)
- However, close lags are even simpler (no parameter tuning)
- Close lags survived correlation filtering better in experiment

---

### Tier 3: CONSIDER PRUNE (High Correlation, r > 0.95)

**Some information loss - keep most diverse representatives**

These features have r > 0.95 correlation. Pruning is **recommended for efficiency**.

#### Close Lag Pruning Within Group

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `close_lag_2` | `close_lag_1`, `close_lag_3` | r=0.983, redundant with neighbors |

**Keep**: `close_lag_1`, `close_lag_3`, `close_lag_5`, `close_lag_10`, `close_lag_20`

**Impact**: 1 feature pruned, spans sufficient lag range

#### Rolling Min/Max Redundancy

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `rolling_min_20/50/100` | `expanding_min` | r=0.952-0.986, expanding more general |
| `rolling_max_20/50` | `rolling_max_100` | r=0.975-0.985, keep longest window |

**Keep**: `expanding_min`, `expanding_max`, `rolling_max_100`

**Impact**: 5 features pruned, keep expanding + one long-window max

#### Volatility Feature Consolidation

**Rolling Std** (keep 1 of 4):
- ❌ `rolling_std_5/10/50/100`
- ✅ KEEP: `rolling_std_20` (middle window, representative)

**EWM Std** (prune all):
- ❌ `ewm_std_5/10/20/50` (r=0.866-0.926 with rolling_std)
- ❌ `expanding_std` (r=0.983 with rolling_std_20)

**Realized Vol** (keep 1 of 3):
- ❌ `realized_vol_10/20`
- ✅ KEEP: `realized_vol_50` (longer window, more stable)

**Parkinson Vol** (prune all):
- ❌ `parkinson_vol_10/20` (r=0.847-0.874 with other volatility)

**Final volatility features**: `rolling_std_20`, `realized_vol_50` (2 diverse measures)

**Impact**: 11 features pruned, 2 volatility measures retained

#### Price Position Consolidation

| PRUNED ❌ | KEEP ✅ | Justification |
|-----------|---------|---------------|
| `pct_from_ma_10/20/50` | `z_score_50` | r=0.816-0.868, z_score normalized |
| `z_score_10/20` | `z_score_50` | r=0.805-0.861, keep longest window |

**Keep**: `z_score_50` (normalized, stable)

**Impact**: 5 features pruned, single normalized price position metric

---

## Final Feature Set (48 Features)

### Breakdown by Category

| Category | Count | Retention | Features |
|----------|-------|-----------|----------|
| **Returns** | 7 | 100% | All returns + lags (momentum signals) |
| **Fourier** | 10 | 100% | Daily/weekly harmonics (cyclical patterns) |
| **Calendar** | 4 | 67% | `month_end`, `month_start`, `quarter_end`, `year_end` |
| **Close Lags** | 5 | N/A | `lag_1/3/5/10/20` (price memory, NEW preference over means) |
| **Volume Lags** | 5 | 100% | `lag_1/2/3/5/10` (volume memory) |
| **STL** | 4 | 67% | Seasonal + residual (orthogonal), trend pruned |
| **Rolling Stats** | 6 | 38% | `std_20`, `max_100`, `skew_20/50`, `kurt_20/50` |
| **Expanding** | 2 | 67% | `min`, `max` (expanding_mean redundant) |
| **Volatility** | 1 | 9% | `realized_vol_50` only |
| **Price Position** | 1 | 20% | `z_score_50` only |
| **Autocorr** | 1 | 100% | `returns_acf_lag_1` |

**Total**: 48 features (44% reduction from 85)

---

## When to Reconsider This Manifest

**Re-evaluate pruning decisions if**:

1. **New data regime**: Distribution shift where pruned features become decorrelated
   - Example: STL trends become orthogonal to close lags in high-volatility regime

2. **Model architecture change**: Non-linear models that benefit from redundant features
   - Example: Deep learning models with dropout may benefit from feature redundancy

3. **Feature importance analysis**: Pruned feature shows unexpected importance in ablation study
   - Run SHAP/permutation importance on full 85-feature set periodically

4. **OOD robustness**: Pruned feature improves performance under distribution shift
   - Test during Phase 6 (OOD robustness validation)

5. **New correlation analysis**: Re-run experiment with different data or longer timeframe
   - Recommended: Annual re-validation with updated data

**Current decision**: Pruning decisions based on SOL 5-min data (9,901 samples, 2022 timeframe). Validate on other symbols/timeframes before generalizing.

---

## Cross-References

### Source Experiment
- **Directory**: `experiments/orthogonality_filtering_20251003/`
- **Method**: Correlation-based filtering with thresholds 0.95, 0.90, 0.80
- **Results**: `FINDINGS.md`, `EXPERIMENT_COMPLETE.md`
- **Data**: SOL 5-min OHLCV (9,901 samples after NaN drop)

### Related Documentation
- `.claude/atom-library-catalog.md` - Full atom specifications
- `.claude/orthogonality-layers.md` - Layer A/B organization
- `NEXT_STEPS.md` - Phase 5 (Feature Selection) roadmap

### Implementation
- **Feature list**: `experiments/orthogonality_filtering_20251003/results/raw/features_corr_0.9.txt`
- **Removal log**: `experiments/orthogonality_filtering_20251003/results/metrics/corr_removal_log_all.csv`

---

## Usage Examples

### Programmatic Enforcement

```python
import yaml

# Load manifest
with open('.claude/feature-pruning-manifest.md') as f:
    content = f.read()
    yaml_section = content.split('---')[1]
    manifest = yaml.safe_load(yaml_section)

# Get pruned features
pruned = set(manifest['pruned_features'].keys())

# Validate feature list
my_features = ['returns', 'hour_of_day_sin', 'close_lag_1']
invalid = [f for f in my_features if f in pruned]

if invalid:
    for feat in invalid:
        replacement = manifest['pruned_features'][feat]['keep_instead']
        print(f"❌ {feat} is pruned, use {replacement} instead")
```

### Manual Validation

Before adding features to model:

1. Check if feature in `pruned_features` section
2. If yes, use `keep_instead` alternative
3. If no, proceed with feature

---

**Last Updated**: 2025-10-03
**Next Review**: 2026-10-03 (annual re-validation recommended)
