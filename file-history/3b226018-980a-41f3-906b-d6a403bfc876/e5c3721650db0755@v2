# ðŸ” Discovery & Reorganization Plan

**Date**: 2025-10-01
**Goal**: Organize research repo by pipeline stage (Option A)
**Method**: Discover â†’ Prune â†’ Grow â†’ Automate

---

## PHASE 1: DISCOVERY RESULTS âœ…

### ðŸ“Š Usage Analysis

#### **Recent Activity (Last 3 Months)**
| Component | Status | Files Modified |
|-----------|--------|----------------|
| **data_collection/** | ðŸŸ¢ ACTIVE | binance_collector, gap_fillers (2 commits) |
| **core/sync/** | ðŸŸ¢ ACTIVE | DataSourceManager wrapper (created 2025-10-01) |
| **utils/** | ðŸŸ¢ ACTIVE | market_constraints (created 2025-10-01) |
| **temporal_validation_utils.py** | ðŸŸ¢ ACTIVE | Created 2025-10-01, tested âœ… |
| **feature_engineering/playground/** | ðŸŸ¡ STALE | Last modified Sep 15 (~4.5 months ago) |
| **docs/ideas_brewing/** | ðŸ”´ DEAD | 2 empty files, archived experiments |

#### **Code Inventory**
| Category | Count | Total Lines | Notes |
|----------|-------|-------------|-------|
| **Executable Scripts** | 20+ | ~15,000 | Have `if __name__ == "__main__"` |
| **Library Modules** | 15+ | ~5,000 | Imported by others |
| **Dead Files** | 2 | 0 | Empty Python files |
| **Documentation** | 10+ | N/A | READMEs, summaries |

#### **Largest Scripts** (Complexity Hotspots)
1. `nested_cv_temporal_slicing.py` - 1,879 lines
2. `convergence_monitor_integration.py` - 1,874 lines
3. `complete_framework.py` - 1,286 lines
4. `simple_multi_objective_demo.py` - 1,260 lines (âœ… Fixed for temporal leakage)

#### **Import Dependencies**
- **All scripts depend on**: `core.sync.data_source_manager`, `utils.market_constraints`
- **No cross-imports**: Playground scripts are independent (good for modularization)
- **External deps**: sklearn, pandas, numpy (standard); optional: sktime, mlflow, catch22, tsfresh

---

## PHASE 2: PRUNE STRATEGY ðŸ—‘ï¸

### Immediate Deletions
```bash
# 1. Empty files
rm docs/ideas_brewing/multi_objective_mae_mfe_meta_features/fail_fast/sota_financial_ts_generators/quality_evaluation_framework/improved_grading_analysis.py
rm docs/ideas_brewing/multi_objective_mae_mfe_meta_features/fail_fast/sota_financial_ts_generators/quality_evaluation_framework/direct_quality_measurement_v2.py

# 2. Archive completed experiments
mv docs/ideas_brewing/multi_objective_mae_mfe_meta_features/archive/ archive/ideas_brewing_2025_01/
```

### Consolidation Opportunities

#### **Duplicate Microstructure Scripts**
Current:
- `realized_variance.py` + `realized_variance_sota.py`
- `bipower_variation.py` + `bipower_variation_sota.py`
- `higher_moments.py` + `higher_moments_sota.py`
- `order_flow_metrics.py` + `order_flow_metrics_sota.py`
- `microstructure_utils.py` + `microstructure_utils_sota.py`

**Decision needed**: Keep SOTA versions OR merge best parts?

**Recommendation**:
- Archive old versions to `archive/microstructure_v1/`
- Keep `*_sota.py` as canonical versions
- Create unified `microstructure_library.py` module

#### **Redundant Demos**
Current:
- `simple_multi_objective_demo.py` (1,260 lines, temporal-safe âœ…)
- `complete_framework.py` (1,286 lines, unknown temporal safety)

**Recommendation**:
- Keep `simple_multi_objective_demo.py` (already fixed)
- Archive or merge `complete_framework.py`
- Extract reusable components into library

---

## PHASE 3: GROW STRATEGY ðŸŒ±

### New Directory Structure (Pipeline Stage Model)

```
ml-feature-experiments/
â”œâ”€â”€ 0_core/                      # â† Core utilities (MOVE HERE)
â”‚   â”œâ”€â”€ sync/
â”‚   â”‚   â””â”€â”€ data_source_manager.py      # âœ… Already here
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ temporal_validation_utils.py # â† MOVE from playground
â”‚   â””â”€â”€ constraints/
â”‚       â””â”€â”€ market_constraints.py        # â† MOVE from utils/
â”‚
â”œâ”€â”€ 1_data_collection/           # âœ… Already good
â”‚   â”œâ”€â”€ binance_public_data_collector.py
â”‚   â”œâ”€â”€ multi_source_gap_filler.py
â”‚   â”œâ”€â”€ universal_gap_filler.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ 2_feature_engineering/       # â† Organize by feature type
â”‚   â”œâ”€â”€ microstructure/          # â† MOVE from playground/core_microstructure
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ realized_variance.py
â”‚   â”‚   â”œâ”€â”€ bipower_variation.py
â”‚   â”‚   â”œâ”€â”€ higher_moments.py
â”‚   â”‚   â”œâ”€â”€ order_flow_metrics.py
â”‚   â”‚   â””â”€â”€ microstructure_utils.py
â”‚   â”œâ”€â”€ cycleness/               # â† MOVE from playground/cycleness_prediction_mvp
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ regime_detection.py
â”‚   â”‚   â”œâ”€â”€ change_point_detection.py
â”‚   â”‚   â””â”€â”€ lstm_meta_features.py
â”‚   â””â”€â”€ fitness/                 # â† MOVE from playground/custom_fitness
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ fitness_engine.py
â”‚       â””â”€â”€ directional_diagnostic.py
â”‚
â”œâ”€â”€ 3_model_validation/          # â† Advanced CV methods
â”‚   â”œâ”€â”€ nested_cv/               # â† MOVE from playground/nested_hv_blocked_cv
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ nested_cv_temporal_slicing.py
â”‚   â”‚   â””â”€â”€ convergence_monitor_integration.py
â”‚   â””â”€â”€ walk_forward/
â”‚       â””â”€â”€ rolling_origin_validator.py  # â† Extract from demos
â”‚
â”œâ”€â”€ 4_experiments/               # â† Active research (keep playground feel)
â”‚   â”œâ”€â”€ multi_objective_optimization/
â”‚   â”‚   â””â”€â”€ demo.py              # â† simple_multi_objective_demo.py
â”‚   â””â”€â”€ generator_evaluation/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ 5_automation/                # â† NEW: Pipeline orchestration
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ data_to_features.py
â”‚   â”‚   â”œâ”€â”€ features_to_models.py
â”‚   â”‚   â””â”€â”€ end_to_end.py
â”‚   â””â”€â”€ cli/
â”‚       â””â”€â”€ run_pipeline.py
â”‚
â”œâ”€â”€ archive/                     # â† Dead code graveyard
â”‚   â”œâ”€â”€ ideas_brewing_2025_01/
â”‚   â”œâ”€â”€ microstructure_v1/
â”‚   â””â”€â”€ failed_experiments/
â”‚
â”œâ”€â”€ examples/                    # â† NEW: Usage examples
â”‚   â”œâ”€â”€ 01_data_collection.py
â”‚   â”œâ”€â”€ 02_feature_generation.py
â”‚   â”œâ”€â”€ 03_temporal_validation.py
â”‚   â””â”€â”€ 04_end_to_end_pipeline.py
â”‚
â”œâ”€â”€ tests/                       # âœ… Already exists
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ docs/                        # â† Consolidated documentation
    â”œâ”€â”€ api/
    â”œâ”€â”€ guides/
    â””â”€â”€ architecture/
```

### Consolidation Targets

#### **1. Microstructure Library**
**File**: `2_feature_engineering/microstructure/__init__.py`
```python
"""
Microstructure Feature Engineering Library

Combines validated SOTA implementations:
- Realized Variance (Andersen et al. 2003)
- Bipower Variation (Barndorff-Nielsen & Shephard 2004)
- Higher Moments (skewness, kurtosis)
- Order Flow Metrics (Kyle 1985, O'Hara 2015)

Usage:
    from ml_feature_experiments.feature_engineering.microstructure import (
        realized_variance,
        bipower_variation,
        order_flow_imbalance
    )
"""
from .realized_variance import calculate_realized_variance
from .bipower_variation import calculate_bipower_variation
from .higher_moments import calculate_higher_moments
from .order_flow_metrics import calculate_order_flow_metrics
from .microstructure_utils import validate_ohlcv_data

__all__ = [
    'calculate_realized_variance',
    'calculate_bipower_variation',
    'calculate_higher_moments',
    'calculate_order_flow_metrics',
    'validate_ohlcv_data'
]
```

#### **2. Temporal Validation Library**
**Already created**: `temporal_validation_utils.py`
**Move to**: `0_core/validation/temporal_validation.py`
**Add**: MLflow integration templates, sktime integration

#### **3. Pipeline Automation**
**File**: `5_automation/pipelines/data_to_features.py`
```python
"""
Automated Data â†’ Features Pipeline

One command to:
1. Collect data from Binance
2. Generate microstructure features
3. Validate temporal safety
4. Save to feature store
"""
import click
from ml_feature_experiments.data_collection import BinancePublicDataCollector
from ml_feature_experiments.feature_engineering.microstructure import *
from ml_feature_experiments.core.validation import WalkForwardValidator

@click.command()
@click.option('--symbol', default='SOLUSDT')
@click.option('--start-date', required=True)
@click.option('--end-date', required=True)
@click.option('--interval', default='1h')
@click.option('--output', default='features.parquet')
def run_pipeline(symbol, start_date, end_date, interval, output):
    """Run data â†’ features pipeline"""
    # Implementation...
    pass

if __name__ == '__main__':
    run_pipeline()
```

---

## PHASE 4: MIGRATION STEPS ðŸš€

### Step 1: Create New Structure
```bash
# Run this script to create directory structure
cat > create_structure.sh << 'EOF'
#!/bin/bash
mkdir -p 0_core/{sync,validation,constraints}
mkdir -p 2_feature_engineering/{microstructure,cycleness,fitness}
mkdir -p 3_model_validation/{nested_cv,walk_forward}
mkdir -p 4_experiments/{multi_objective_optimization,generator_evaluation}
mkdir -p 5_automation/{pipelines,cli}
mkdir -p archive/{ideas_brewing_2025_01,microstructure_v1,failed_experiments}
mkdir -p examples
mkdir -p docs/{api,guides,architecture}
EOF

chmod +x create_structure.sh
./create_structure.sh
```

### Step 2: Move Core Utilities (Preserve Git History)
```bash
# Move temporal validation utils to core
git mv feature_engineering/playground/temporal_validation_utils.py \
        0_core/validation/temporal_validation.py

# Move market constraints to core
git mv utils/market_constraints.py \
        0_core/constraints/market_constraints.py

# Update __init__.py
touch 0_core/__init__.py
touch 0_core/validation/__init__.py
touch 0_core/constraints/__init__.py
```

### Step 3: Reorganize Feature Engineering
```bash
# DECISION POINT: Keep SOTA or merge duplicates?
# Option A: Keep SOTA only
git mv feature_engineering/playground/core_microstructure/*_sota.py \
        2_feature_engineering/microstructure/

# Option B: Merge and create unified versions (manual work)
# - Compare *_sota.py vs regular versions
# - Take best implementation
# - Move to 2_feature_engineering/microstructure/

# Move cycleness
git mv feature_engineering/playground/cycleness_prediction_mvp/mvp_utils/* \
        2_feature_engineering/cycleness/

# Move fitness
git mv feature_engineering/playground/custom_fitness/* \
        2_feature_engineering/fitness/
```

### Step 4: Reorganize Model Validation
```bash
# Move nested CV
git mv feature_engineering/playground/nested_hv_blocked_cv/* \
        3_model_validation/nested_cv/

# Extract walk-forward from demos (manual)
# - Open simple_multi_objective_demo.py
# - Extract WalkForwardValidator (if not using temporal_validation_utils)
# - Create 3_model_validation/walk_forward/rolling_origin_validator.py
```

### Step 5: Move Experiments
```bash
# Keep temporal-safe demo
git mv feature_engineering/playground/rolling_origin_demo/simple_multi_objective_demo.py \
        4_experiments/multi_objective_optimization/demo.py

# Archive old demo
git mv feature_engineering/playground/rolling_origin_demo/complete_framework.py \
        archive/microstructure_v1/complete_framework.py
```

### Step 6: Archive Dead Code
```bash
# Archive ideas_brewing
git mv docs/ideas_brewing/ \
        archive/ideas_brewing_2025_01/

# Archive old microstructure if keeping SOTA only
git mv feature_engineering/playground/core_microstructure/*.py \
        archive/microstructure_v1/ \
        # (Only non-SOTA versions)
```

### Step 7: Create Automation
```bash
# Create pipeline script
cat > 5_automation/pipelines/data_to_features.py << 'EOF'
# (Implementation from section above)
EOF

# Create CLI entrypoint
cat > 5_automation/cli/run_pipeline.py << 'EOF'
#!/usr/bin/env python3
import click
from ..pipelines import data_to_features

@click.group()
def cli():
    pass

cli.add_command(data_to_features.run_pipeline)

if __name__ == '__main__':
    cli()
EOF
```

### Step 8: Update All Imports
```bash
# This will be automated with a script
cat > fix_imports.py << 'EOF'
#!/usr/bin/env python3
import os
import re
from pathlib import Path

# Map old imports to new paths
IMPORT_MAPPINGS = {
    'from feature_engineering.playground.temporal_validation_utils': 'from ml_feature_experiments.core.validation.temporal_validation',
    'from utils.market_constraints': 'from ml_feature_experiments.core.constraints.market_constraints',
    'from feature_engineering.playground.core_microstructure': 'from ml_feature_experiments.feature_engineering.microstructure',
    # ... add more mappings
}

def fix_imports_in_file(filepath):
    with open(filepath, 'r') as f:
        content = f.read()

    modified = False
    for old, new in IMPORT_MAPPINGS.items():
        if old in content:
            content = content.replace(old, new)
            modified = True

    if modified:
        with open(filepath, 'w') as f:
            f.write(content)
        print(f"âœ… Fixed imports in {filepath}")

# Run on all Python files
for py_file in Path('.').rglob('*.py'):
    if 'archive' not in str(py_file) and '.venv' not in str(py_file):
        fix_imports_in_file(py_file)
EOF

python fix_imports.py
```

### Step 9: Create __init__.py Files
```bash
# Create package __init__ files with proper exports
cat > 0_core/__init__.py << 'EOF'
"""Core utilities for ML feature experiments"""
from .validation.temporal_validation import (
    TemporalSafePipeline,
    WalkForwardValidator,
    TemporalLabelGenerator
)
from .constraints.market_constraints import DataProvider, Interval, MarketType
from .sync.data_source_manager import DataSourceManager

__all__ = [
    'TemporalSafePipeline',
    'WalkForwardValidator',
    'TemporalLabelGenerator',
    'DataProvider',
    'Interval',
    'MarketType',
    'DataSourceManager'
]
EOF

# Create similar __init__.py for each package
# ...
```

### Step 10: Create Examples
```bash
cat > examples/01_data_collection.py << 'EOF'
#!/usr/bin/env python3
"""
Example 1: Data Collection

Demonstrates how to collect Binance data using the unified interface.
"""
from ml_feature_experiments.data_collection import BinancePublicDataCollector
from ml_feature_experiments.core import DataProvider, Interval, MarketType

def main():
    # Collect SOLUSDT data
    collector = BinancePublicDataCollector(
        symbol='SOLUSDT',
        start_date='2024-01-01',
        end_date='2024-12-31'
    )

    # Collect 1-hour data
    result = collector.collect_timeframe_data('1h')
    df = result['dataframe']

    print(f"âœ… Collected {len(df)} bars")
    print(df.head())

if __name__ == '__main__':
    main()
EOF

cat > examples/02_feature_generation.py << 'EOF'
#!/usr/bin/env python3
"""
Example 2: Feature Generation

Demonstrates microstructure feature extraction.
"""
from ml_feature_experiments.core import DataSourceManager, DataProvider, MarketType, Interval
from ml_feature_experiments.feature_engineering.microstructure import (
    calculate_realized_variance,
    calculate_bipower_variation,
    calculate_order_flow_metrics
)

def main():
    # Get data
    dsm = DataSourceManager.create(DataProvider.BINANCE, MarketType.SPOT)
    df = dsm.get_data(
        symbol='SOLUSDT',
        interval=Interval.ONE_HOUR,
        start_date='2024-01-01',
        end_date='2024-01-31'
    )

    # Calculate features
    rv = calculate_realized_variance(df)
    bv = calculate_bipower_variation(df)
    of = calculate_order_flow_metrics(df)

    print(f"âœ… Realized Variance: {rv}")
    print(f"âœ… Bipower Variation: {bv}")
    print(f"âœ… Order Flow Imbalance: {of['imbalance']}")

if __name__ == '__main__':
    main()
EOF

cat > examples/03_temporal_validation.py << 'EOF'
#!/usr/bin/env python3
"""
Example 3: Temporal Validation

Demonstrates walk-forward validation with temporal safety.
"""
from ml_feature_experiments.core import (
    TemporalSafePipeline,
    WalkForwardValidator,
    TemporalCVConfig
)
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge

def main():
    # Create temporal-safe pipeline
    pipeline = TemporalSafePipeline.create_feature_pipeline(
        scaler=StandardScaler(),
        feature_transformer=PCA(n_components=3),
        model=Ridge()
    )

    # Walk-forward validation
    validator = WalkForwardValidator(
        cv_config=TemporalCVConfig(n_splits=5, test_size=10, gap=1)
    )

    # Load data (example)
    import numpy as np
    X = np.random.randn(100, 10)
    y = np.random.randn(100)

    results = validator.validate(pipeline, X, y)

    print("âœ… Validation Results:")
    for metric, scores in results.items():
        if metric != 'fold_indices':
            print(f"   {metric}: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")

if __name__ == '__main__':
    main()
EOF

cat > examples/04_end_to_end_pipeline.py << 'EOF'
#!/usr/bin/env python3
"""
Example 4: End-to-End Pipeline

Demonstrates full data â†’ features â†’ validation pipeline.
"""
# Implementation combining all examples above
pass
EOF
```

### Step 11: Update pyproject.toml
```toml
[project.scripts]
ml-pipeline = "ml_feature_experiments.automation.cli.run_pipeline:cli"
ml-collect-data = "ml_feature_experiments.data_collection.binance_public_data_collector:main"
ml-generate-features = "ml_feature_experiments.automation.pipelines.data_to_features:run_pipeline"
```

### Step 12: Test Everything
```bash
# Install in editable mode
uv pip install -e .

# Test imports
python -c "from ml_feature_experiments.core import TemporalSafePipeline; print('âœ… Core imports work')"
python -c "from ml_feature_experiments.feature_engineering.microstructure import *; print('âœ… Microstructure imports work')"

# Run examples
python examples/01_data_collection.py
python examples/02_feature_generation.py
python examples/03_temporal_validation.py

# Run tests
uv run pytest tests/ -v
```

---

## PHASE 5: AUTOMATION ðŸ¤–

### Create One-Command Pipeline

**File**: `5_automation/cli/run_pipeline.py`
```python
#!/usr/bin/env python3
import click
from ml_feature_experiments.data_collection import BinancePublicDataCollector
from ml_feature_experiments.feature_engineering.microstructure import *
from ml_feature_experiments.core.validation import WalkForwardValidator

@click.group()
def cli():
    """ML Feature Experiments Pipeline CLI"""
    pass

@cli.command()
@click.option('--symbol', default='SOLUSDT', help='Trading symbol')
@click.option('--start', required=True, help='Start date (YYYY-MM-DD)')
@click.option('--end', required=True, help='End date (YYYY-MM-DD)')
@click.option('--interval', default='1h', help='Data interval')
@click.option('--output', default='features.parquet', help='Output file')
def collect_and_generate(symbol, start, end, interval, output):
    """Collect data and generate features"""
    # 1. Collect data
    click.echo(f"ðŸ“Š Collecting {symbol} data from {start} to {end}...")
    collector = BinancePublicDataCollector(symbol=symbol, start_date=start, end_date=end)
    result = collector.collect_timeframe_data(interval)
    df = result['dataframe']

    # 2. Generate microstructure features
    click.echo(f"ðŸ”§ Generating microstructure features...")
    features = {}
    features['realized_variance'] = calculate_realized_variance(df)
    features['bipower_variation'] = calculate_bipower_variation(df)
    features['order_flow'] = calculate_order_flow_metrics(df)

    # 3. Save
    import pandas as pd
    feature_df = pd.DataFrame([features])
    feature_df.to_parquet(output)
    click.echo(f"âœ… Features saved to {output}")

@cli.command()
@click.argument('feature_file')
def validate(feature_file):
    """Run temporal validation on features"""
    click.echo(f"ðŸ”„ Running walk-forward validation on {feature_file}...")
    # Load features and run validation
    pass

if __name__ == '__main__':
    cli()
```

**Usage**:
```bash
# One command to collect and generate features
ml-pipeline collect-and-generate --symbol SOLUSDT --start 2024-01-01 --end 2024-12-31

# Validate features
ml-pipeline validate features.parquet
```

---

## PHASE 6: DOCUMENTATION ðŸ“š

### Update Main README
```markdown
# ML Feature Experiments

Research repository for feature engineering and model validation.

## Structure

- **0_core/**: Core utilities (DataSourceManager, temporal validation)
- **1_data_collection/**: Binance data collection
- **2_feature_engineering/**: Microstructure, cycleness, fitness features
- **3_model_validation/**: Nested CV, walk-forward validation
- **4_experiments/**: Active research experiments
- **5_automation/**: Pipeline orchestration
- **examples/**: Usage examples
- **tests/**: Test suite
- **archive/**: Deprecated code

## Quick Start

```bash
# Install
uv pip install -e .

# Collect data and generate features
ml-pipeline collect-and-generate --symbol SOLUSDT --start 2024-01-01 --end 2024-12-31

# Run examples
python examples/01_data_collection.py
```

## Research Workflow

1. **Data Collection**: `1_data_collection/`
2. **Feature Engineering**: `2_feature_engineering/`
3. **Validation**: `3_model_validation/` + `0_core/validation/`
4. **Experiments**: `4_experiments/`

See `examples/` for usage patterns.
```

---

## DECISION POINTS â“

### 1. Microstructure Duplicates
**Options**:
- A. Keep only `*_sota.py` versions (simpler, assumes SOTA is better)
- B. Compare and merge best parts (manual work, ensures quality)
- C. Keep both, deprecate old versions (safest, most clutter)

**Recommendation**: **Option B** - One-time manual comparison, create definitive versions

### 2. Large CV Scripts
**nested_cv_temporal_slicing.py (1,879 lines)** and **convergence_monitor_integration.py (1,874 lines)**

**Options**:
- A. Move as-is to `3_model_validation/nested_cv/`
- B. Refactor into smaller modules first
- C. Extract core logic, archive examples

**Recommendation**: **Option A** first (quick win), then refactor in place

### 3. Experiments Directory
**Options**:
- A. Keep failed experiments in `4_experiments/failed/`
- B. Archive all failed experiments to `archive/`
- C. Delete failed experiments (lose history)

**Recommendation**: **Option B** - Clean `4_experiments/` for active research only

---

## EXECUTION TIMELINE â±ï¸

### **Day 1: Setup & Core Migration**
- [ ] Create directory structure
- [ ] Move core utilities (temporal_validation, market_constraints)
- [ ] Update imports in moved files
- [ ] Test core imports

### **Day 2: Feature Engineering Migration**
- [ ] Compare microstructure duplicates, decide keep/merge
- [ ] Move microstructure files
- [ ] Move cycleness and fitness modules
- [ ] Create __init__.py files
- [ ] Test feature imports

### **Day 3: Validation & Experiments**
- [ ] Move nested CV scripts
- [ ] Move demo to experiments
- [ ] Archive dead code
- [ ] Test validation imports

### **Day 4: Automation & Examples**
- [ ] Create pipeline scripts
- [ ] Create CLI entrypoint
- [ ] Write 4 example scripts
- [ ] Update pyproject.toml
- [ ] Test end-to-end

### **Day 5: Documentation & Polish**
- [ ] Update main README
- [ ] Create architecture docs
- [ ] Run all tests
- [ ] Fix any broken imports
- [ ] Commit and tag release

---

## SUCCESS CRITERIA âœ…

1. **Imports Work**: All `from ml_feature_experiments.*` imports succeed
2. **Examples Run**: All 4 example scripts execute without errors
3. **Tests Pass**: `uv run pytest tests/ -v` passes
4. **Pipeline Works**: `ml-pipeline collect-and-generate` runs end-to-end
5. **No Dead Code**: `archive/` contains all unused code, main tree is clean
6. **Documentation**: README accurately reflects new structure

---

## ROLLBACK PLAN ðŸ”„

If migration fails:
```bash
# All changes are git commits, easy to rollback
git log --oneline -20  # Find commit before migration
git reset --hard <commit-hash>
```

**Safety**: Test in branch first
```bash
git checkout -b reorganize-pipeline-stage
# Do all migration work here
# Test thoroughly
# Only merge to main when confirmed working
```

---

## NEXT STEPS ðŸš€

**Ready to execute?** Say "go" and I'll:

1. Create migration script (`migrate_to_pipeline_stage.sh`)
2. Execute Phase 1 (directory structure)
3. Execute Phase 2 (core utilities migration)
4. Show you diffs before committing

**Or need to decide?** Answer:
- Keep SOTA only or merge microstructure duplicates?
- Archive failed experiments or keep in `4_experiments/failed/`?
- Want me to auto-execute or step-by-step with approval?

Let me know your preference! ðŸŽ¯
