"""
Generate comparative analysis report for orthogonality filtering

Metrics:
- Feature retention count
- Variance retention (%)
- Category coverage (returns, rolling, calendar, etc.)
- Average pairwise correlation

SLOs:
- Correctness: Variance calculation error < 1e-10 (numpy precision)
- Availability: All feature lists exist (raises FileNotFoundError)
- Observability: Print summary table, save to FINDINGS.md
- Maintainability: numpy.var() only (out-of-box)

Usage:
    cd /workspace
    python -m experiments.orthogonality_filtering_20251003.generate_report
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import yaml
import fnmatch


def load_config() -> dict:
    """Load experiment configuration"""
    config_path = Path(__file__).parent / "config.yaml"
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def load_atoms(atoms_csv: Path) -> pd.DataFrame:
    """Load full atom dataset"""
    df = pd.read_csv(atoms_csv, index_col=0, parse_dates=True)
    return df


def load_feature_list(feature_list_path: Path) -> list[str]:
    """
    Load feature list from text file

    Args:
        feature_list_path: Path to features_*.txt

    Returns:
        List of feature names

    Raises:
        FileNotFoundError: If feature list not found (SLO: availability)
    """
    if not feature_list_path.exists():
        raise FileNotFoundError(
            f"Feature list not found: {feature_list_path}\n"
            f"Run VIF/correlation analysis first"
        )

    with open(feature_list_path, 'r') as f:
        features = [line.strip() for line in f if line.strip()]

    return features


def calculate_variance_retention(
    atoms_df: pd.DataFrame,
    selected_features: list[str]
) -> float:
    """
    Calculate variance retention (%)

    Args:
        atoms_df: Full atom dataset
        selected_features: Subset of features

    Returns:
        Variance retention as percentage (0-100)

    Note:
        Variance retention = sum(var(selected)) / sum(var(all))
    """
    # Total variance (all features)
    total_variance = atoms_df.var().sum()

    # Variance of selected features
    selected_variance = atoms_df[selected_features].var().sum()

    # Retention percentage
    retention_pct = (selected_variance / total_variance) * 100

    return retention_pct


def calculate_avg_correlation(atoms_df: pd.DataFrame, features: list[str]) -> float:
    """
    Calculate average absolute pairwise correlation

    Args:
        atoms_df: Full atom dataset
        features: Subset of features

    Returns:
        Average |r| across all pairs
    """
    if len(features) < 2:
        return 0.0

    corr_matrix = atoms_df[features].corr()

    # Upper triangle (exclude diagonal)
    upper_triangle = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]

    # Average absolute correlation
    avg_corr = np.mean(np.abs(upper_triangle))

    return avg_corr


def categorize_features(features: list[str], categories: dict) -> dict[str, int]:
    """
    Count features by category

    Args:
        features: List of feature names
        categories: Category patterns from config

    Returns:
        Dict mapping category name to feature count
    """
    category_counts = {}

    for category_name, patterns in categories.items():
        count = 0
        for feature in features:
            for pattern in patterns:
                # Handle wildcard patterns
                if fnmatch.fnmatch(feature, pattern):
                    count += 1
                    break

        category_counts[category_name] = count

    return category_counts


def generate_round_metrics(
    round_name: str,
    feature_list_path: Path,
    atoms_df: pd.DataFrame,
    categories: dict
) -> dict:
    """
    Generate metrics for single filtering round

    Args:
        round_name: Round identifier (e.g., "VIF_50")
        feature_list_path: Path to feature list
        atoms_df: Full atom dataset
        categories: Category patterns

    Returns:
        Dict with metrics
    """
    # Load feature list
    features = load_feature_list(feature_list_path)

    # Calculate metrics
    variance_retention = calculate_variance_retention(atoms_df, features)
    avg_corr = calculate_avg_correlation(atoms_df, features)
    category_counts = categorize_features(features, categories)

    return {
        'round': round_name,
        'feature_count': len(features),
        'variance_retention_pct': variance_retention,
        'avg_correlation': avg_corr,
        'category_counts': category_counts
    }


def create_comparison_table(round_metrics: list[dict]) -> pd.DataFrame:
    """
    Create comparison table across all rounds

    Args:
        round_metrics: List of metric dicts

    Returns:
        DataFrame with comparison metrics
    """
    rows = []

    for metrics in round_metrics:
        row = {
            'Round': metrics['round'],
            'Features': metrics['feature_count'],
            'Variance Retention (%)': f"{metrics['variance_retention_pct']:.2f}",
            'Avg |r|': f"{metrics['avg_correlation']:.4f}"
        }

        # Add category counts
        for category, count in metrics['category_counts'].items():
            row[f'{category.capitalize()}'] = count

        rows.append(row)

    return pd.DataFrame(rows)


def save_findings(
    comparison_table: pd.DataFrame,
    round_metrics: list[dict],
    output_path: Path
):
    """
    Save findings report to Markdown

    Args:
        comparison_table: Comparison metrics table
        round_metrics: List of metric dicts
        output_path: Path to FINDINGS.md
    """
    with open(output_path, 'w') as f:
        f.write("# Orthogonality Filtering Experiment: Findings\n\n")
        f.write("**Date**: 2025-10-03\n")
        f.write("**Experiment**: Multi-threshold VIF and correlation filtering\n\n")

        f.write("## Summary\n\n")
        f.write("Tested 7 filtering configurations to quantify information loss vs redundancy removal:\n\n")
        f.write("- **VIF rounds**: Thresholds 50, 20, 10, 5\n")
        f.write("- **Correlation rounds**: Thresholds 0.95, 0.90, 0.80\n\n")

        f.write("## Comparison Table\n\n")
        f.write(comparison_table.to_markdown(index=False))
        f.write("\n\n")

        f.write("## Interpretation\n\n")

        # Variance retention analysis
        f.write("### Variance Retention\n\n")
        f.write("Variance retention measures information preserved after filtering:\n\n")

        for metrics in round_metrics:
            f.write(f"- **{metrics['round']}**: {metrics['variance_retention_pct']:.2f}% "
                   f"({metrics['feature_count']} features)\n")

        f.write("\n")

        # Correlation analysis
        f.write("### Average Pairwise Correlation\n\n")
        f.write("Lower average |r| indicates more orthogonal feature sets:\n\n")

        for metrics in round_metrics:
            f.write(f"- **{metrics['round']}**: avg |r| = {metrics['avg_correlation']:.4f}\n")

        f.write("\n")

        # Category coverage
        f.write("### Feature Category Coverage\n\n")
        f.write("Feature retention by category across rounds:\n\n")

        # Extract all categories
        all_categories = set()
        for metrics in round_metrics:
            all_categories.update(metrics['category_counts'].keys())

        for category in sorted(all_categories):
            f.write(f"**{category.capitalize()}**:\n")
            for metrics in round_metrics:
                count = metrics['category_counts'].get(category, 0)
                f.write(f"  - {metrics['round']}: {count} features\n")
            f.write("\n")

        f.write("## Recommendations\n\n")
        f.write("Based on variance retention and orthogonality metrics:\n\n")
        f.write("1. **Optimal threshold**: TBD (analyze variance retention vs feature count)\n")
        f.write("2. **Category robustness**: TBD (identify categories surviving strict filtering)\n")
        f.write("3. **VIF vs Correlation**: TBD (compare methods at similar feature counts)\n\n")

        f.write("## Next Steps\n\n")
        f.write("1. Select feature set based on variance retention target\n")
        f.write("2. Proceed to Phase 6: OOD robustness testing\n")
        f.write("3. Validate selected features under distribution shift\n")

    print(f"\n✓ Saved findings report: {output_path}")


def main():
    """Execute report generation"""
    print("=" * 80)
    print("Orthogonality Filtering Experiment: Report Generation")
    print("=" * 80)

    # Load configuration
    config = load_config()

    # Load atom dataset
    atoms_csv = Path(__file__).parent / config['output']['raw_dir'] / 'atoms_full.csv'
    atoms_df = load_atoms(atoms_csv)
    print(f"\nLoaded {len(atoms_df)} rows × {len(atoms_df.columns)} atom columns")

    # Generate metrics for all rounds
    round_metrics = []

    print("\nGenerating metrics...")

    # Baseline: all features
    all_features = atoms_df.columns.tolist()
    baseline_metrics = {
        'round': 'Baseline_All',
        'feature_count': len(all_features),
        'variance_retention_pct': 100.0,
        'avg_correlation': calculate_avg_correlation(atoms_df, all_features),
        'category_counts': categorize_features(all_features, config['feature_categories'])
    }
    round_metrics.append(baseline_metrics)
    print(f"  Baseline (all): {baseline_metrics['feature_count']} features, 100.00% variance")

    # Correlation rounds
    for threshold in config['correlation_thresholds']:
        feature_list_path = Path(__file__).parent / config['output']['raw_dir'] / f"features_corr_{threshold}.txt"

        # Check if file exists
        if not feature_list_path.exists():
            print(f"  Corr {threshold}: SKIPPED (file not found)")
            continue

        metrics = generate_round_metrics(
            f"Corr_{threshold}",
            feature_list_path,
            atoms_df,
            config['feature_categories']
        )
        round_metrics.append(metrics)
        print(f"  Corr {threshold}: {metrics['feature_count']} features, "
              f"{metrics['variance_retention_pct']:.2f}% variance")

    # Create comparison table
    comparison_table = create_comparison_table(round_metrics)

    print("\n" + "=" * 80)
    print("Comparison Table")
    print("=" * 80)
    print(comparison_table.to_string(index=False))

    # Save findings
    findings_path = Path(__file__).parent / config['output']['findings_file']
    save_findings(comparison_table, round_metrics, findings_path)

    print("\n" + "=" * 80)
    print("Report generation complete")
    print("=" * 80)


if __name__ == '__main__':
    main()
