"""
Memory-efficient streaming version of cdn_downloader.
Uses incremental Parquet writes to avoid loading entire month into memory.
"""

import calendar
import logging
from pathlib import Path
from typing import List, Optional

import pyarrow as pa
import pyarrow.parquet as pq
from okx_price_provider.cdn_downloader import download_and_parse_day

logger = logging.getLogger(__name__)


def download_month_ticks_streaming(
    msg_type: str,
    year: int,
    month: int,
    output_parquet: Path,
    symbols_filter: Optional[List[str]] = None,
    skip_errors: bool = True,
) -> dict:
    """
    Download month of tick data using streaming/incremental Parquet writes.

    Memory-efficient: Never holds more than 1 day in memory.

    Args:
        msg_type: "allspot" or "allswap"
        year: Year (e.g., 2023)
        month: Month (1-12)
        output_parquet: Path to save Parquet file
        symbols_filter: Optional list of symbols to include
        skip_errors: Continue if some days fail

    Returns:
        Dict with download statistics
    """
    num_days = calendar.monthrange(year, month)[1]

    logger.info(
        f"Downloading {msg_type} {year}-{month:02d} ({num_days} days) - STREAMING MODE"
    )

    output_parquet.parent.mkdir(parents=True, exist_ok=True)

    # Stats tracking
    days_succeeded = 0
    days_failed = 0
    total_trades = 0
    failed_days = []

    # Parquet writer (initialized on first successful day)
    writer = None
    schema = None

    try:
        for day in range(1, num_days + 1):
            try:
                day_df = download_and_parse_day(
                    msg_type=msg_type,
                    year=year,
                    month=month,
                    day=day,
                    symbols_filter=symbols_filter,
                )

                if day_df.empty:
                    logger.warning(f"Day {day} returned no data")
                    days_failed += 1
                    failed_days.append(day)
                    continue

                # Convert to PyArrow Table (zero-copy if possible)
                table = pa.Table.from_pandas(day_df, preserve_index=False)

                # Initialize writer on first successful day
                if writer is None:
                    schema = table.schema
                    writer = pq.ParquetWriter(
                        output_parquet,
                        schema,
                        compression='zstd',
                        compression_level=9,
                        use_dictionary=True,
                        write_statistics=True,
                        data_page_size=1024*1024,  # 1MB pages
                    )
                    logger.info(f"Initialized Parquet writer with schema from day {day}")

                # Append day's data (incremental write)
                writer.write_table(table)

                days_succeeded += 1
                total_trades += len(day_df)

                # Free memory immediately
                del day_df, table

                if day % 5 == 0:
                    logger.info(f"Progress: {day}/{num_days} days, {total_trades:,} trades")

            except Exception as e:
                logger.error(f"Failed to download day {day}: {e}")
                days_failed += 1
                failed_days.append(day)

                if not skip_errors:
                    raise

        # Close writer
        if writer is not None:
            writer.close()
            logger.info(f"Closed Parquet writer after {days_succeeded} days")
        else:
            raise ValueError(f"No data downloaded for {year}-{month:02d}")

        # Get final file size
        file_size_mb = output_parquet.stat().st_size / 1024 / 1024

        logger.info(
            f"âœ… Saved {total_trades:,} trades to {output_parquet} "
            f"({file_size_mb:.1f} MB, {days_succeeded}/{num_days} days)"
        )

        return {
            "file": output_parquet,
            "total_trades": total_trades,
            "days_succeeded": days_succeeded,
            "days_failed": days_failed,
            "failed_days": failed_days,
            "size_mb": file_size_mb,
        }

    except Exception as e:
        # Cleanup on failure
        if writer is not None:
            writer.close()
        if output_parquet.exists():
            output_parquet.unlink()
        raise
