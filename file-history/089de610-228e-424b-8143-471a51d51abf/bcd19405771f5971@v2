# Temporal Leakage Test Plan

**Version**: 1.0.0
**Status**: In Progress
**Created**: 2025-10-07
**Last Updated**: 2025-10-07
**Owner**: Eon Labs ML Feature Engineering

---

## SLOs

**Correctness**: 100% - Zero temporal leakage in all test scenarios
**Availability**: N/A - Testing infrastructure (not production service)
**Observability**: 100% - All test failures with detailed diagnostics
**Maintainability**: 90% - Pytest standard patterns, minimal custom infrastructure

**Error Handling**: raise_and_propagate - All test failures propagate with full context

---

## Current Coverage (v1.0.6)

### Existing Tests: 665 total

**Adversarial Audits** (624 cases, one-time scripts in /tmp):
- Exhaustive timestamp validation: 128 tests
- Mult1 boundary attacks: 160 tests
- Mult2 boundary attacks: 54 tests
- Random validation points: 100 tests
- Dataset boundaries: 20 tests
- Cross-interval consistency: 3 tests
- Extreme edge cases: 159 tests

**Unit Tests** (41 cases, permanent in tests/):
- Availability column: 5 tests
- Feature expander: 12 tests
- Redundancy filter: 15 tests
- Walk-forward validation: 9 tests

### Coverage Gaps

**Gap 1 (CRITICAL)**: One-time vs permanent tests
- Risk: HIGH
- Impact: v1.0.4 boundary bug could regress silently
- Status: BLOCKING

**Gap 2 (HIGH)**: Limited multiplier combinations
- Risk: MEDIUM
- Impact: Bugs in untested combinations
- Tested: (4,12) only
- Missing: (2,6), (3,9), (5,15), (6,18), edge cases

**Gap 3 (HIGH)**: Availability column edge cases
- Risk: MEDIUM
- Impact: Real-world data patterns uncovered
- Missing: backwards time, large gaps, DST, market hours

**Gap 4 (MEDIUM)**: Property-based testing
- Risk: MEDIUM
- Impact: Unknown edge cases undiscovered
- Missing: hypothesis fuzzing, invariant tests

**Gap 5 (MEDIUM)**: Incremental update testing
- Risk: MEDIUM
- Impact: Streaming mode temporal safety unverified
- Missing: State preservation, update() equivalence

---

## Implementation Plan

### Phase 1: Permanence (Week 1) - CRITICAL

**Status**: In Progress
**Priority**: P0
**Blocks**: v1.0.7 release

**Tasks**:
1. Create tests/test_temporal/ directory structure
2. Convert adversarial audits to pytest regression tests
3. Add CI/CD workflow for temporal tests
4. Document test infrastructure

**Deliverables**:
- tests/test_temporal/test_adversarial_regression.py
- tests/test_temporal/conftest.py (fixtures)
- .github/workflows/temporal-tests.yml
- All 624 adversarial tests running in CI

**Acceptance Criteria**:
- All 624 tests passing in CI
- v1.0.4 boundary bug cannot regress
- CI run time <5 minutes
- Test failures include detailed diagnostics

**SLOs**:
- Correctness: 100% - Catches v1.0.4 regression
- Observability: 100% - Clear failure messages
- Maintainability: 90% - Standard pytest patterns

### Phase 2: Property-Based Testing (Week 2) - HIGH

**Status**: Planned
**Priority**: P1
**Blocks**: v1.1.0 release

**Tasks**:
1. Add hypothesis dependency
2. Create property-based temporal tests
3. Create invariant tests
4. Configure hypothesis CI profile

**Deliverables**:
- tests/test_temporal/test_properties.py
- Hypothesis config in pyproject.toml
- 100+ generated test scenarios

**Acceptance Criteria**:
- Hypothesis discovers no new bugs
- Invariants codified and verified
- CI run time <10 minutes total

**SLOs**:
- Correctness: 100% - Finds unknown edge cases
- Observability: 100% - Hypothesis reports failures clearly
- Maintainability: 85% - Hypothesis learning curve

### Phase 3: Availability Stress Testing (Week 3) - HIGH

**Status**: Planned
**Priority**: P1
**Blocks**: Production deployment

**Tasks**:
1. Create availability edge case tests (30+ scenarios)
2. Test backwards time, large gaps, DST
3. Test market hours patterns (stocks vs crypto)
4. Test exchange halt scenarios

**Deliverables**:
- tests/test_temporal/test_availability_stress.py
- 30+ availability edge case tests
- Real-world pattern validation

**Acceptance Criteria**:
- All edge cases handled correctly
- Clear error messages for invalid data
- No silent failures

**SLOs**:
- Correctness: 100% - All edge cases covered
- Observability: 100% - Diagnostic messages for failures
- Maintainability: 90% - Reusable fixtures

### Phase 4: Multiplier Combinations (Week 4) - MEDIUM

**Status**: Planned
**Priority**: P2
**Blocks**: v1.1.0 release

**Tasks**:
1. Test multiplier matrix: (2,6), (3,9), (4,12), (5,15), (6,18)
2. Test edge cases: mult1==mult2, mult1>mult2
3. Test LCM boundary analysis
4. Test very large multipliers

**Deliverables**:
- tests/test_temporal/test_multiplier_combinations.py
- 20+ multiplier combination tests
- Edge case validation

**Acceptance Criteria**:
- All common combinations tested
- Edge cases handled correctly
- Performance acceptable for large multipliers

**SLOs**:
- Correctness: 100% - All combinations validated
- Observability: 100% - Clear parametrize labels
- Maintainability: 95% - Pytest parametrize

### Phase 5: Incremental Update Testing (Week 5) - MEDIUM

**Status**: Planned
**Priority**: P2
**Blocks**: Streaming mode adoption

**Tasks**:
1. Test update() vs batch equivalence
2. Test state preservation across updates
3. Test multi-interval update() temporal safety
4. Test update() after fit_transform()

**Deliverables**:
- tests/test_temporal/test_incremental_update.py
- Streaming mode temporal validation
- State corruption tests

**Acceptance Criteria**:
- update() matches batch processing exactly
- State correctly preserved
- Multi-interval streaming safe

**SLOs**:
- Correctness: 100% - Streaming mode temporal guarantee
- Observability: 100% - State inspection on failure
- Maintainability: 90% - Stateful test patterns

### Phase 6: Large-Scale Testing (Week 6) - LOW

**Status**: Planned
**Priority**: P3
**Blocks**: Large-scale deployments

**Tasks**:
1. Test 10K, 100K, 1M row datasets
2. Test memory usage bounded
3. Test performance characteristics
4. Mark as slow tests (@pytest.mark.slow)

**Deliverables**:
- tests/test_temporal/test_large_scale.py
- Memory profiling tests
- Performance regression tests

**Acceptance Criteria**:
- Temporal safety maintained at scale
- Memory doesn't leak
- Performance acceptable

**SLOs**:
- Correctness: 100% - Scale doesn't break temporal guarantee
- Observability: 100% - Memory/perf metrics logged
- Maintainability: 85% - May require special fixtures

### Phase 7: Real-World Data (Week 7) - MEDIUM

**Status**: Planned
**Priority**: P2
**Blocks**: Enterprise adoption

**Tasks**:
1. Download real exchange data (Binance, etc.)
2. Test COVID crash, FTX collapse scenarios
3. Test stock market hours patterns
4. Cache real data for CI

**Deliverables**:
- tests/test_temporal/test_real_world_data.py
- Real data fixtures (cached)
- Exchange halt scenario tests

**Acceptance Criteria**:
- Real data patterns validated
- Historical events (crashes, halts) tested
- CI has cached data

**SLOs**:
- Correctness: 100% - Real patterns validated
- Observability: 100% - Historical context in failures
- Maintainability: 80% - Data download/cache complexity

---

## Test Infrastructure

### Directory Structure

```
tests/
├── test_temporal/                    # New temporal leakage tests
│   ├── __init__.py
│   ├── conftest.py                   # Fixtures and helpers
│   ├── test_adversarial_regression.py   # Phase 1
│   ├── test_properties.py            # Phase 2
│   ├── test_availability_stress.py   # Phase 3
│   ├── test_multiplier_combinations.py  # Phase 4
│   ├── test_incremental_update.py    # Phase 5
│   ├── test_large_scale.py           # Phase 6
│   └── test_real_world_data.py       # Phase 7
└── fixtures/
    ├── temporal_data.py              # Data generation fixtures
    └── real_data/                    # Cached real exchange data
```

### Dependencies

**Required**:
- pytest >= 8.0 (existing)
- numpy >= 1.26 (existing)
- pandas >= 2.0 (existing)

**New**:
- hypothesis >= 6.0 (Phase 2)
- gapless-crypto-data >= 3.0 (Phase 7, existing)

**CI/CD**:
- GitHub Actions (existing)

### Fixtures

**Core Fixtures** (conftest.py):
```python
@pytest.fixture
def synthetic_ohlcv_data(n_bars, base_interval_hours):
    """Generate synthetic OHLCV with availability column"""

@pytest.fixture
def multi_interval_config(multiplier_1, multiplier_2):
    """Standard multi-interval config"""

@pytest.fixture
def cached_real_data(symbol, exchange, interval):
    """Load cached real exchange data"""
```

**Error Handling**:
- All fixtures raise on error
- No default values or fallbacks
- Clear error messages with context

---

## Success Metrics

### Quantitative Metrics

**Test Coverage**:
- Current: 665 tests
- Phase 1: 665 + 624 permanent = 1289 tests
- Phase 2: +100 hypothesis scenarios = 1389 tests
- Phase 3: +30 availability tests = 1419 tests
- Phase 4: +20 multiplier tests = 1439 tests
- Phase 5: +15 streaming tests = 1454 tests
- Phase 6: +10 scale tests = 1464 tests
- Phase 7: +10 real data tests = 1474 tests
- **Target: 1500+ tests**

**CI Performance**:
- Current: ~12 seconds
- Phase 1: <5 minutes (adversarial regression)
- Full suite: <10 minutes (excluding slow tests)
- Slow tests: <30 minutes (run on schedule)

**Regression Prevention**:
- v1.0.4 boundary bug: BLOCKED
- Target bug escape rate: <1 per year

### Qualitative Metrics

**Production Readiness**:
- Phase 1-3 complete: ✅ Production ready
- Phase 1-7 complete: ✅ Enterprise ready

**Developer Experience**:
- Test failures include: timestamp, feature, diff, expected
- CI failures easy to debug locally
- Fixtures reusable across test phases

---

## Implementation Progress

### Phase 1: Permanence (In Progress)

**Week 1 - Day 1** (2025-10-07):
- Created TEMPORAL_LEAKAGE_TEST_PLAN.md
- Initialized todo tracking
- Status: Starting infrastructure setup

**Completed**:
- [ ] Test infrastructure setup
- [ ] Adversarial regression tests
- [ ] CI/CD workflow
- [ ] Documentation

**Blockers**: None

**Next Actions**:
1. Create tests/test_temporal/ directory
2. Create conftest.py with fixtures
3. Convert adversarial audit to pytest

---

## References

### Source Files

**Adversarial Audits** (to be converted):
- /tmp/adversarial_temporal_audit.py (462 tests)
- /tmp/extreme_adversarial_audit.py (162 tests)
- /tmp/test_boundary_bug_fix.py (4 tests)

**Existing Tests** (reference):
- tests/test_features/test_availability_column.py
- tests/test_validation/test_walk_forward.py

**Implementation**:
- src/atr_adaptive_laguerre/features/atr_adaptive_rsi.py:885-918 (vectorized availability)

### Documentation

- CHANGELOG.md: v1.0.5 boundary bug fix
- /tmp/TEMPORAL_LEAKAGE_AUDIT_REPORT_v1.0.5.md: Audit results

### Related Issues

- v1.0.4: Boundary condition bug (searchsorted side='right')
- v1.0.5: Fixed with side='left'
- v1.0.6: UX improvements

---

## Change Log

**v1.0.0** (2025-10-07):
- Initial plan created
- 7 phases defined
- SLOs established
- Phase 1 started

---

## Notes

**Lessons from v1.0.4 Bug**:
- Boundary conditions require exhaustive testing
- Off-by-one errors subtle (side='right' vs side='left')
- 25% failure rate only at exact boundary alignments
- Synthetic tests didn't catch initially
- Permanent regression tests critical

**Design Principles**:
- raise_and_propagate error handling
- Out-of-box solutions (pytest, hypothesis)
- Machine-readable, version-tracked
- SLO-driven (correctness, observability, maintainability)
- No promotional language
