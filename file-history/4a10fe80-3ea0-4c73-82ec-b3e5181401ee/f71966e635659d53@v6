# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a feature generation module for machine learning on resampled candlestick (OHLCV) data. Feature sets are published to AWS CodeArtifact and consumed by the Eon Labs prediction pipeline. The module follows a class-based architecture where each feature set inherits from `FeatureSet` base class.

## Core Architecture

### FeatureSet Base Class Pattern

All custom feature sets must:
- Be defined in a class named `CustomFeatureSet` that inherits from `FeatureSet`
- Implement three required components:
  1. `extract_feature()` - Core feature extraction logic
  2. `get_source_lookback_length(source_name)` - Define historical data requirements (keep minimal to preserve training data length)
  3. `data_dependencies` property - Declare data source dependencies with resample factors

### Data Source Architecture

- **Primary source**: OHLCV data with `actual_ready_time` column (simulates data availability delay)
- **`actual_ready_time`**: Framework-generated (not in CSVs), simulates data availability delay
- **`resample_factors`**: Multi-timeframe (different intervals via OHLCV aggregation), NOT multi-period (different lookbacks)
- **Data access methods**:
  - `get_data_source(source)` - Get complete source dictionary with `data_df`
  - `get_source_column(column, source)` - Get single column as numpy array
- **Feature assignment**: Use `set_features_batch(features_dict)` to avoid DataFrame fragmentation

### Critical Implementation Rules

1. **Error handling**: If required data source doesn't exist, raise error immediately - never work around it
2. **No data leakage**: Never use future data in feature calculation; respect `actual_ready_time`
3. **Lookback length**: Set to minimum required for feature computation (binary search to optimize if needed)
4. **Feature normalization**: Typically normalize to [-1, 1] range
5. **NaN handling**: Use `np.nan_to_num()` or equivalent to handle missing values

## Docker Container (Optional)

- **Container name**: `ml-dev`
- **Working directory**: `/workspace`
- **Install command**: `docker exec ml-dev pip install -e '.[dev]'`
- **Example validation**: `docker exec ml-dev python -m ml_feature_set.run_feature_set_validation --feature_set_path "..."`

## Project Context

- **Dockerfile fix**: `ENV CONDA_PLUGINS_AUTO_ACCEPT_TOS=true` resolves Anaconda ToS requirement (July 2025)
- **Templates**: Define FeatureSet structure for validation output format
  - Examples: `ohlcv_comprehensive_sizex_v5.py`, `ohlcv_fluid-dynamics_sizex_v1.py`
  - Use IPSS+VIF pipeline to select features → then conform to template format

## Sample Data

- **Binance timestamps**: Always use `datetime.fromtimestamp(ts/1000, tz=timezone.utc)` (naive conversion creates fake DST duplicates)

## PR Policy (ZERO-TRUST WHITELIST)

**Philosophy:** BLOCK EVERYTHING until explicitly approved

**Automated enforcement:** `.github/workflows/enforce-production-only.yml`

**Current whitelist:** EMPTY (nothing allowed)

**Workflow:**
1. Create PR with code changes
2. PR will FAIL (expected - whitelist is empty)
3. Review failed PR to see blocked files
4. If approved, manually add pattern to whitelist in workflow file
5. Commit whitelist update to main first
6. Rebase/update PR - it will now pass

**Example whitelist patterns:**
```bash
'^ml_feature_set/bundled/.*\.py$'     # Python files in bundled/
'^pyproject\.toml$'                    # Project config
'^README\.md$'                         # Root README only
```

**Whitelist location:** Line 45 in `.github/workflows/enforce-production-only.yml`

**Policy:** Default deny - no files merge to main unless pattern is explicitly added to whitelist

## Development Workflow

### Creating a New Feature Set

1. Create file in `ml_feature_set/bundled/` following naming convention: `ohlcv_[type]_sizex_v[N].py`
2. Reference existing templates: `ohlcv_size79_v1`, `ohlcv_support-resistance_sizex_v2`, `ohlcv_support-resistance-feargreed_sizex_v1`
3. Implement `CustomFeatureSet` class with required methods
4. Run validation (see below)

### Validation Process (MANDATORY)

```bash
python -m ml_feature_set.run_feature_set_validation --feature_set_path "path/to/feature_set.py"
```

**If errors occur** (must fix until no errors remain):
1. First check for data leakage (using future data)
2. Then increase `get_source_lookback_length()` value
3. Use binary search to find minimum working lookback (acceptable tolerance: ±20 lookback values)

### Submitting to Touchstone Service

After successful validation:
```bash
AWS_PROFILE=el-prod python3 util/touchstone/touchstone_submission.py create \
  --feature_set_git_path <relative/path/to/file.py> \
  --env prod \
  --set sources.feature_set_git_branch=<current-branch>
```

Requires packages from `util/touchstone/requirements.txt`.

## Building and Publishing

### Build Package

```bash
rm -rf dist build/ *.egg-info
python -m build
```

### Local Installation

```bash
pip install -e ../ml-feature-set  # From another repo
```

### Publish to AWS CodeArtifact

**Important**: Publishing to prod requires also publishing same version to dev.

```bash
rm -rf dist build/ *.egg-info
python3 -m build

export AWS_PROFILE=el-dev  # or el-prod
aws_account_id=$(aws sts get-caller-identity --query "Account" --output text)
aws codeartifact login --tool twine --repository el-prediction-pipeline \
  --domain eonlabs --domain-owner $aws_account_id --region us-west-2

twine upload --verbose --repository codeartifact dist/*
```

### Install Published Package

```bash
export AWS_PROFILE=el-dev
aws_account_id=$(aws sts get-caller-identity --query "Account" --output text)
aws codeartifact login --tool pip --repository el-prediction-pipeline \
  --domain eonlabs --domain-owner $aws_account_id --region us-west-2

pip install ml_feature_set
```

## Development Environment

- Uses VSCode devcontainer (see [common repo docs](https://github.com/Eon-Labs/common/blob/main/environment/dev_container.md))
- Python 3.10 (locked in Dockerfile)
- Core dependencies: numpy (1.24.4 locked), numba, scipy, pandas, talipp
- Technical indicators: TA-Lib (installed via conda-forge in container)

## Testing

Run tests:
```bash
pytest
```

Run validation on specific feature set:
```bash
python -m ml_feature_set.run_feature_set_validation --feature_set_path "ml_feature_set/bundled/your_feature_set.py"
```

## Key Constraints

1. **Language**: Always use English in code and comments
2. **Lookback optimization**: Balance historical data needs vs training data length
3. **Version management**: Current version in `pyproject.toml` is 1.1.19
4. **Python version**: Requires Python >=3.10
5. **No silent failures**: All errors must propagate, no workarounds in feature extraction

## Feature Construction Patterns (Off-the-Shelf)

**Pandas methods:**
- `.ewm()` - Exponentially weighted (MACD, adaptive indicators)
- `.expanding()` - Walk-forward cumulative features
- `.rank(pct=True)` - Percentile normalization (0-1)
- `.pipe()` - Method chaining for pipelines
- `.groupby().transform()` - Group stats → original rows
- `.interpolate(method='time')` - Time-aware missing data
- `.resample().interpolate()` - Upsample with interpolation
- Named aggs: `agg(vol_mean=('volume', 'mean'))`

**sklearn transformers (OOD-robust):**
- `PolynomialFeatures(interaction_only=True)` - Feature crosses (RSI×Volume)
- `QuantileTransformer(output_distribution='normal')` - Outliers → Gaussian (robust)
- `RobustScaler()` - IQR-based scaling (outlier-resistant)

**Robust statistics:**
- `scipy.stats.median_abs_deviation()` - MAD (more robust than std)
- `scipy.stats.mstats.winsorize()` - Cap extremes (vs trimming)
- Downside deviation - Semi-variance for risk metrics

**Normalization principles:**
- `np.arctan()` - Smooth squashing (unbounded → bounded, vs hard clip)
- Multi-period families - Sweep all periods (5,10,15,20,25,30), let model choose vs expert
- Second-order features - Derivatives of moving averages (acceleration = Δ(MA))

**Higher-order derivatives (velocity/acceleration/jerk):**
- `scipy.signal.savgol_filter(deriv=1/2/3)` - Industry standard (Savitzky-Golay smoothing + differentiation)
- `derivative` package (2024, experimental) - Total Variation Regularization for extremely noisy data
- Note: 3rd+ order rarely used in production (noise-sensitive)

**Signal decomposition (production):**
- `pywt.wavedec()` - Wavelet decomposition (trend/detail separation)
- `statsmodels.tsa.seasonal.STL` - Seasonal/trend/residual split

**Information theory (production):**
- `sklearn.feature_selection.mutual_info_regression()` - Mutual information (non-linear dependence)

**Regime detection (feature generation only, not for selection validation):**
- `hmmlearn.GaussianHMM` - Hidden Markov Models (unsupervised regime features)

**Spectral & cross-series analysis:**
- `scipy.signal.coherence()` - Frequency coherence between series
- `scipy.signal.csd()` - Cross-spectral density (phase relationships)
- `scipy.fft` - Frequency domain features

**Financial risk metrics:**
- Maximum drawdown - Steepest peak-to-trough decline
- Sortino ratio - Downside deviation (vs total volatility in Sharpe)
- Calmar ratio - Return / max drawdown

**Advanced aggregations:**
- `statsmodels.regression.rolling.RollingOLS` - Rolling regression (beta over time)
- `scipy.signal.fftconvolve()` - Fast convolution (moving averages)
- `scipy.signal.find_peaks()` - Peak detection

**Realized measures (Andersen-Bollerslev framework):**
- Realized volatility - Sum of squared intraperiod returns (5-min benchmark)
- Bipower variation - Barndorff-Nielsen & Shephard (separates jumps from continuous volatility)
- Jump detection - Realized variance minus bipower variation
- Range-based volatility:
  - Parkinson (5x more efficient than close-to-close)
  - Garman-Klass (7.4x more efficient, assumes no jumps)
  - Yang-Zhang (14x more efficient, handles opening jumps & drift)

**Microstructure proxies (OHLCV-compatible):**
- Amihud ILLIQ - `|return|/dollar_volume` (illiquidity measure)
- High-low Amihud - `(high-low)/volume` (range-based illiquidity)
- VPIN estimation - Volume-synchronized informed trading (classify buy/sell from price changes)
- Kyle's lambda proxy - Market impact/adverse selection
- Order flow imbalance - Tick rule (price > prev = buy, < prev = sell)

**Fractal analysis:**
- Higuchi fractal dimension - Complexity of time series trajectory
- Box-counting dimension - Fractal structure of price patterns

**Math utilities:**
- `np.einsum('ij,ik->jk')` - Efficient covariance matrices
- Broadcasting - Pairwise operations without loops

### Research/Experimental (not in minimal production stack)

**Complexity measures:**
- `ordpy` - Permutation entropy (order-based)
- `antropy` - Sample/approximate/multiscale entropy
- `nolds.dfa()` - Detrended Fluctuation Analysis (Hurst parameter)
- `PyRQA` - Recurrence Quantification Analysis

**Causal discovery:**
- `tigramite` - Transfer entropy, Granger causality

**Advanced decomposition:**
- `PyEMD` - Empirical Mode Decomposition

**Nonlinear dependence:**
- `pyvinecopulib`/`pycop` - Vine copulas, tail dependence

**Relational:**
- `getML` - Automated cross-table aggregations (requires license)

**Fast alternatives:**
- `infomeasure` - Entropy/MI (10x faster than scipy)

### Methods to Avoid (Blacklist)
