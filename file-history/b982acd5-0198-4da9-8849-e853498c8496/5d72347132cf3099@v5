# HMM Regime Detection - 1-Hour Timeframe

**Experiment ID:** `hmm_regime_20251006_1h`
**Objective:** Test if 1-hour timeframe breaks 51.5% ceiling observed on 5-minute data
**Status:** COMPLETED
**Created:** 2025-10-06T00:00:00Z
**Last Updated:** 2025-10-06T13:10:31Z
**Completed:** 2025-10-06T13:10:31Z
**Version:** 1.2.0

---

## Context

**Hypothesis:** The 51.5% ceiling may be timeframe-specific. Longer timeframes (1h) may reduce noise more effectively than 15m and capture macro regime dynamics.

**Previous Results:**
- **5-minute:** `experiments/hmm_regime_20251005_hybrid` - 50.33% ± 1.59%
- **15-minute:** `experiments/hmm_regime_20251006_15m` - In progress

**Methodology Inheritance:**
- Fixed HMM implementation (StandardScaler + KMeans + spherical covariance)
- Optimized parameters (same as 5m: window=2000, stride=10, n_iter=25)
- All execution fixes (unbuffered, progress heartbeat, realistic timeout)

---

## SLOs (Service Level Objectives)

### Availability
- **no_silent_failures:** All errors propagate to caller, no exception swallowing
- **no_retries:** Crash fast on numerical issues, no automatic retry logic
- **error_logging:** All exceptions logged with full traceback before propagation

### Correctness
- **hmm_convergence:** HMM EM algorithm must converge (monitor_.converged == True)
- **transition_matrix_validity:** All row sums == 1.0 (±1e-5 tolerance)
- **state_representation:** All n_components states must be represented in predictions
- **fold_completion:** All 20 CV folds must complete without numerical errors
- **baseline_comparison:** Results compared against 51.5% ± 1.1% baseline

### Observability
- **convergence_diagnostics:** Log iterations, log-likelihood, convergence status per fold
- **state_distribution:** Track state counts and percentages per fold
- **feature_importance:** Log cluster means and separation metrics
- **performance_metrics:** Accuracy, precision, recall per fold
- **summary_artifacts:** JSON results, CSV predictions, confusion matrices

### Maintainability
- **out_of_box_dependencies:** sklearn, hmmlearn, pandas, numpy only
- **no_custom_em:** Use hmmlearn.hmm.GaussianHMM as-is
- **reusable_patterns:** Extract scalable patterns to library if successful
- **version_tracking:** SemVer 1.0.0, changelog in this document

### Excluded from SLOs
- Speed/performance optimization (not in scope)
- Security hardening (not applicable for research code)

---

## Methodology

### Dataset
- **Source:** `ml_feature_set/sample_data/resampled_binance_SOL-1h.csv`
- **Records:** 40,244 samples
- **Date Range:** 2020-08-15 to 2025-xx-xx (5 years)
- **Timeframe:** 1-hour bars
- **Target:** Directional prediction (H=20 bars ahead, ~20 hours / 0.83 days)

### Base Features (26 features)
- OHLCV lags: [1, 5, 10, 20]
- Rolling stats: mean, std, min, max (windows: 10, 20, 50)
- Volume ratios: vs MA(10), MA(20)
- Returns: 1-bar, 5-bar, 20-bar

### HMM Regime Features (5 features)
**Fixed Implementation:**
- StandardScaler normalization
- KMeans initialization (3 clusters)
- Spherical covariance
- min_covar=1e-3 regularization

**Generated Features:**
- `hmm_state_0`, `hmm_state_1`, `hmm_state_2`: One-hot encoded states
- `hmm_state_prob_max`: Max probability across states
- `hmm_state_prob_ratio`: Max prob / second max prob

### HMM Parameters (Same as 5m)
- **Window Size:** 2000 samples (same as 5m)
- **Stride:** 10 (sample every 10th window)
- **N Iterations:** 25 (same as 5m)
- **N States:** 3 (same as 5m)
- **Expected Windows:** ~3,825 windows

### Model Architecture
- **Type:** LogisticRegression (sklearn)
- **Penalty:** l2
- **Solver:** lbfgs
- **Max Iterations:** 1000
- **Class Weight:** balanced

### Cross-Validation
- **Method:** TimeSeriesSplit (sklearn)
- **Folds:** 20
- **Train/Test Split:** Rolling window, no shuffle
- **Purging:** None (chronological split handles leakage)

---

## Execution Plan

1. **Load Data:** Read CSV, validate schema
2. **Generate Base Features:** 26 OHLCV + rolling features
3. **Generate HMM Features:** Apply fixed implementation
4. **Cross-Validation:** 20-fold TimeSeriesSplit
5. **Validation:** Compare vs 51.5% baseline
6. **Artifacts:** Save results, predictions, diagnostics

### Expected Runtime
- HMM training: ~6-8 minutes (~3,825 windows)
- CV training: ~1-2 minutes (20 folds × LogisticRegression)
- **Total:** ~8 minutes

### Execution Command
```bash
timeout 1h uv run --with hmmlearn --with scikit-learn python -u \
  experiments/hmm_regime_20251006_1h/run_experiment.py \
  > experiments/hmm_regime_20251006_1h/experiment_output.log 2>&1 &
```

---

## Success Criteria

### Primary Objective
**Test ceiling at 1-hour timeframe:**
- If accuracy > 53%: Ceiling broken, timeframe effect confirmed
- If accuracy ≈ 50-52%: Ceiling persists across all timeframes

### Secondary Objectives
- **Stability:** All 20 folds complete without numerical errors
- **Convergence:** HMM converges in all rolling windows
- **State Quality:** Regime persistence > 5 bars median across folds

---

## Risk Assessment

### Known Risks
1. **Long Prediction Horizon:** H=20 hours (~0.83 days) may be too far for predictability
   - Mitigation: Accept as valid timeframe-specific limitation
2. **Regime Dynamics:** 1h regimes may capture macro trends vs 5m/15m micro
   - Mitigation: Accept as valid timeframe-specific behavior
3. **Larger Dataset:** 40K samples may expose different patterns
   - Mitigation: Fixed implementation already validated on 315K samples (5m)

---

## References

**Parent Experiment:** `experiments/hmm_regime_20251005_hybrid` (5-minute, 50.33% ± 1.59%)
**Sibling Experiment:** `experiments/hmm_regime_20251006_15m` (15-minute, in progress)
**Master Plan:** `.claude/plans/hmm-hybrid-validation.yaml`

---

## Memory Optimization (v1.1.0)

**Context:** Sibling 15m experiment OOM killed at 93.8% (1,800/1,920 windows)
**Root Cause:** Memory accumulation + inefficient DataFrame assignments
**Reference:** `.claude/plans/hmm-memory-optimization.yaml`

### Applied Fixes (Preemptive)

1. **Explicit Cleanup (MEMORY FIX 1)**
   - `del` statements after each HMM window
   - `gc.collect()` every 100 windows
   - Prevents 15-25 GB projected memory accumulation

2. **Preallocated Arrays (MEMORY FIX 2)**
   - Numpy arrays instead of DataFrame `.loc` assignments
   - Single DataFrame assignment at end (vs 11,475 operations)
   - Eliminates 3-5 GB fragmentation overhead

3. **Sequential Execution (MEMORY FIX 3)**
   - Run only after 15m completes
   - Prevents combined 20-35 GB peak exceeding 36 GB system

4. **Memory Monitoring (MEMORY FIX 4)**
   - `psutil` RSS tracking in progress heartbeat
   - Expected peak: <4 GB (vs 15-25 GB projected before)

### Expected Impact
- **Memory Usage:** 15-25 GB → 2-4 GB (6x-8x reduction)
- **Completion:** Would OOM → successful completion
- **Safety:** Sequential execution enforced

---

## Results Summary

**Mean Accuracy:** 51.77% ± 2.42%
**Range:** 47.51% - 55.80%
**Samples:** 3,820
**Folds:** 20
**Verdict:** ✅ **Ceiling confirmed** - No predictive value

**Comparison Across Timeframes:**
- 5m: 50.33% ± 1.59%
- 15m: 51.87% ± 7.67%
- 1h: 51.77% ± 2.42%
- All within ±2% of 51.5% ceiling

**Memory Performance:**
- Peak: 223 MB stable (vs projected 15-25 GB)
- Reduction: 67x-112x from projection
- Runtime: ~54 minutes
- Execution: Remote (yca server, 62 GB RAM)

**Reference:** `experiments/hmm_regime_20251006_1h/FINDINGS.md`

---

## Changelog

### 1.2.0 (2025-10-06T13:10:31Z) - Experiment Completed
- **COMPLETED:** Experiment finished successfully (3,820/3,820 windows)
- **RESULTS:** Mean accuracy 51.77% ± 2.42% (ceiling confirmed)
- **VALIDATED:** Memory optimization (223 MB stable, 67x-112x reduction)
- **REMOTE:** Executed on yca server (62 GB RAM, Linux Debian)
- **DOCUMENTED:** Created FINDINGS.md with full analysis
- **VERDICT:** No predictive value, recommend exclusion from production

### 1.1.0 (2025-10-06T10:00:00Z) - Memory Optimization
- **PREEMPTIVE:** Applied memory fixes before first execution
- **FIXED:** Explicit cleanup (del + gc.collect every 100 windows)
- **FIXED:** Preallocated arrays (avoid DataFrame .loc overhead)
- **FIXED:** Memory monitoring (psutil RSS tracking)
- **REDUCED:** Peak memory 15-25 GB → 2-4 GB (6x-8x reduction)
- **REFERENCE:** `.claude/plans/hmm-memory-optimization.yaml`

### 1.0.0 (2025-10-06T00:00:00Z)
- **init:** Created 1-hour timeframe experiment
- **kept:** Same HMM parameters as 5m (window=2000, stride=10, n_iter=25)
- **inherited:** All fixes from 5m experiment (unbuffered, progress, optimized)
- **objective:** Test if longest timeframe breaks 51.5% ceiling
