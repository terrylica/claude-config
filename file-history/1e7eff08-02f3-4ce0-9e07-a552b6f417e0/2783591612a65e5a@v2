# Exness Data Preprocess

Professional forex tick data preprocessing with optimal compression (Parquet Zstd-22) and DuckDB OHLC generation. Provides efficient storage (9% smaller than ZIP) with lossless precision and direct queryability.

## Features

- **Optimal Compression**: Parquet with Zstd-22 (9% smaller than ZIP, lossless, directly queryable)
- **DuckDB OHLC Generation**: Pre-computed 1-minute bars with automatic resampling to higher timeframes
- **Embedded Metadata**: DuckDB COMMENT support for data lineage tracking
- **On-Demand Analysis**: Direct tick data access via Parquet for microstructure studies
- **Atomic Operations**: Corruption-proof file operations
- **Simple API**: Both Python API and CLI for flexible integration

## Installation

```bash
# From PyPI (when published)
pip install exness-data-preprocess

# From source
git clone https://github.com/Eon-Labs/exness-data-preprocess.git
cd exness-data-preprocess
pip install -e .

# Using uv (recommended)
uv pip install exness-data-preprocess
```

## Quick Start

### Python API

```python
import exness_data_preprocess as edp

# Process one month of data
result = edp.process_month(year=2024, month=8)
print(f"Processed {result['tick_count']:,} ticks")
print(f"Storage: {result['parquet_size_mb']:.2f} MB ticks + {result['duckdb_size_mb']:.2f} MB OHLC")

# Query 1-hour OHLC bars
df_1h = edp.query_ohlc(year=2024, month=8, timeframe='1h')
print(df_1h.head())

# Analyze tick-level data
df_ticks = edp.analyze_ticks(year=2024, month=8)
spreads = df_ticks['Ask'] - df_ticks['Bid']
print(f"Mean spread: {spreads.mean():.5f}")
```

### Command-Line Interface

```bash
# Process single month
exness-preprocess process --year 2024 --month 8

# Process date range
exness-preprocess process --range 2024-01 2024-12

# Query OHLC data
exness-preprocess query --year 2024 --month 8 --timeframe 1h --output ohlc_1h.csv

# Analyze tick data
exness-preprocess analyze --year 2024 --month 8

# Show storage statistics
exness-preprocess stats
```

## Architecture

### Data Flow

```
Exness Public Repository (monthly ZIPs)
           ↓
    Download + Extract
           ↓
Parquet Conversion (Zstd-22, lossless) → 4.77 MB per month
           ↓
DuckDB OHLC Generation → 0.78 MB per month (1-minute bars)
           ↓
  Query Interface (SQL + Pandas)
```

### Compression Design

After comprehensive benchmarking, we selected **Parquet with Zstd-22** as the optimal compression:

| Method | Size (MB) | vs ZIP | Write Time | Read Time | Queryable? | Lossless? |
|--------|-----------|--------|------------|-----------|------------|-----------|
| **Parquet (zstd-22)** | **4.77** | **0.91x (9% smaller)** | **0.78s** | **0.014s** | **✓** | **✓** |
| Parquet (brotli-11) | 4.34 | 0.83x (17% smaller) | 13.67s | 0.003s | ✓ | ✓ |
| Delta+Parquet | 1.59 | 0.30x (70% smaller) | 0.67s | 0.080s | ✗ | ✗ (36 pips error) |
| ZIP (baseline) | 5.25 | 1.00x | — | — | ✗ | ✓ |

**Why Zstd-22?**
- **Lossless**: Zero precision loss (critical for financial data)
- **Directly Queryable**: DuckDB can query Parquet without loading
- **Fast**: 0.78s write time (17x faster than Brotli-11)
- **Efficient**: 9% smaller than ZIP, 40x faster read than delta encoding

**Why NOT Delta Encoding?**
- Lossy compression (36 pips average error, max 95 pips)
- Requires reconstruction step (5.7x slower read)
- Not queryable directly by DuckDB
- Cumulative error accumulation over time

## Storage Format

### Tick Data (Parquet)
- **Format**: Parquet with Zstd-22 compression
- **Columns**: `Timestamp`, `Bid`, `Ask`
- **Size**: ~4.77 MB per month (EURUSD)
- **Location**: `{base_dir}/parquet/{pair}_ticks_{year}_{month}.parquet`

### OHLC Data (DuckDB)
- **Format**: DuckDB database with embedded metadata
- **Table**: `ohlc_1m` (1-minute bars)
- **Columns**: `Timestamp`, `Open`, `High`, `Low`, `Close`, `spread_avg`, `tick_count`
- **Size**: ~0.78 MB per month (EURUSD)
- **Location**: `{base_dir}/duckdb/{pair}_ohlc_{year}_{month}.duckdb`
- **Metadata**: DuckDB COMMENT fields for data lineage

### Default Directory Structure

**Default Location**: `~/eon/exness-data/` (outside project workspace)

```
~/eon/exness-data/
├── parquet/
│   ├── eurusd_ticks_2024_08.parquet
│   └── eurusd_ticks_2024_09.parquet
├── duckdb/
│   ├── eurusd_ohlc_2024_08.duckdb
│   └── eurusd_ohlc_2024_09.duckdb
└── temp/
    └── (temporary ZIP files)
```

**Why Outside Project?**
- **Clean Separation**: Code and data remain independent
- **Scalability**: Store years of data without bloating repository
- **Easy Backup**: Backup `~/eon/exness-data/` independently from code
- **Best Practice**: Data persistence separate from codebase

## Usage Examples

### Process Multiple Months

```python
import exness_data_preprocess as edp

# Process entire year
results = edp.process_date_range(
    start_year=2024,
    start_month=1,
    end_year=2024,
    end_month=12,
    pair="EURUSD"
)

total_ticks = sum(r['tick_count'] for r in results)
total_storage = sum(r['parquet_size_mb'] + r['duckdb_size_mb'] for r in results)
print(f"Total: {total_ticks:,} ticks in {total_storage:.2f} MB")
```

### Custom Output Directory

```python
from pathlib import Path
import exness_data_preprocess as edp

# Use custom directory
result = edp.process_month(
    year=2024,
    month=8,
    base_dir=Path("/mnt/data/forex")
)
```

### Query Different Timeframes

```python
import exness_data_preprocess as edp

# Query 1-minute bars (direct from DuckDB)
df_1m = edp.query_ohlc(2024, 8, timeframe='1m')

# Query 1-hour bars (resampled on-the-fly)
df_1h = edp.query_ohlc(2024, 8, timeframe='1h')

# Query daily bars
df_1d = edp.query_ohlc(2024, 8, timeframe='1d')

print(f"1m bars: {len(df_1m)}, 1h bars: {len(df_1h)}, 1d bars: {len(df_1d)}")
```

### Microstructure Analysis

```python
import exness_data_preprocess as edp
import numpy as np

# Load tick data
df_ticks = edp.analyze_ticks(2024, 8)

# Spread analysis
spreads = df_ticks['Ask'] - df_ticks['Bid']
print(f"Mean spread: {spreads.mean():.5f}")
print(f"Median spread: {spreads.median():.5f}")
print(f"95th percentile: {spreads.quantile(0.95):.5f}")

# Tick frequency analysis
df_ticks['hour'] = df_ticks['Timestamp'].dt.hour
ticks_per_hour = df_ticks.groupby('hour').size()
print(f"\nBusiest hour: {ticks_per_hour.idxmax()}:00 ({ticks_per_hour.max():,} ticks)")
```

### Direct DuckDB Access

```python
import duckdb

# Connect to DuckDB file
conn = duckdb.connect('~/eon/exness-data/duckdb/eurusd_ohlc_2024_08.duckdb')

# Query with SQL
result = conn.execute("""
    SELECT
        DATE_TRUNC('hour', Timestamp) as hour,
        AVG(spread_avg) as avg_spread,
        SUM(tick_count) as total_ticks
    FROM ohlc_1m
    GROUP BY hour
    ORDER BY hour
""").df()

print(result)
conn.close()
```

## Development

### Setup

```bash
# Clone repository
git clone https://github.com/Eon-Labs/exness-data-preprocess.git
cd exness-data-preprocess

# Install with development dependencies (using uv)
uv sync --dev

# Or with pip
pip install -e ".[dev]"
```

### Testing

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=exness_data_preprocess --cov-report=html

# Run specific test
uv run pytest tests/test_processor.py -v
```

### Code Quality

```bash
# Format code
uv run ruff format .

# Lint
uv run ruff check --fix .

# Type checking
uv run mypy src/
```

### Building

```bash
# Build package
uv build

# Test installation locally
uv tool install --editable .
```

## Data Source

Data is sourced from Exness's public tick data repository:
- **URL**: https://ticks.ex2archive.com/
- **Format**: Monthly ZIP files with CSV tick data
- **Content**: Timestamp, Bid, Ask prices for major forex pairs
- **Quality**: Institutional ECN/STP data with microsecond precision

## Technical Specifications

### Compression Benchmarks (August 2024 EURUSD, 13.5M ticks)

| Codec | Level | Size (MB) | vs ZIP | Write (s) | Read (s) |
|-------|-------|-----------|--------|-----------|----------|
| Zstd | 22 | 4.77 | 0.91x | 0.78 | 0.014 |
| Zstd | 9 | 5.08 | 0.97x | 0.64 | 0.003 |
| Zstd | 1 | 6.28 | 1.20x | 0.57 | 0.003 |
| Brotli | 11 | 4.34 | 0.83x | 13.67 | 0.003 |
| Gzip | 9 | 5.27 | 1.00x | 1.05 | 0.002 |
| Snappy | — | 10.60 | 2.02x | 0.57 | 0.002 |
| LZ4 | — | 10.92 | 2.08x | 0.55 | 0.002 |

**Conclusion**: Zstd-22 provides optimal balance of compression ratio, speed, and queryability for financial time-series data.

## License

MIT License - see LICENSE file for details.

## Authors

- Terry Li <terry@eonlabs.com>
- Eon Labs

## Contributing

Contributions are welcome! Please see CONTRIBUTING.md for guidelines.

## Acknowledgments

- Exness for providing high-quality public tick data
- DuckDB for embedded OLAP capabilities
- Apache Arrow and Parquet for efficient columnar storage
- Facebook's Zstd for fast compression
